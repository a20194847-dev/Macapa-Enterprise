[
  {
    "owner": "openai",
    "name": "ai-and-efficiency",
    "url": "https://github.com/openai/ai-and-efficiency",
    "language": null,
    "stars": 56,
    "forks": 25,
    "updated_at": "2025-11-26T19:12:57Z",
    "description": "Submissions for AI and Efficiency SOTA's",
    "summary": "# Algorithmic Efficiency SOTA Submissions\nWe found that in 2019 it took [44x less compute](https://openai.com/blog/ai-and-efficiency/) to train a neural net to AlexNet-level performance than in 2012.\n(Moore‚Äôs Law would have only yielded an 11x change in cost over this period).\n\nGoing forward, we're going to use this git repository to help publicly track state of the art (SOTA) algorithmic efficiency.\nWe're beginning by tracking training efficiency SOTA's in image recognition and translation at two levels.\n\n#### AlexNet-level performance\n*79.1% top 5 accuracy on ImageNet*\n\n| Publication| Compute(tfs-s/days)| Reduction Factor| Analysis| Date |\n| ----------------------- | ------------- | ------------ | ----------------------- | ------------ |\n| [AlexNet](https://papers.nips.cc/paper/4824-imag",
    "keywords": [
      "algorithmic",
      "efficiency",
      "submissions",
      "compute](https://openai.com/blog/ai-and-efficiency/",
      "alexnet-level",
      "performance",
      "moore‚Äôs",
      "yielded",
      "period",
      "forward",
      "repository",
      "publicly",
      "beginning",
      "tracking",
      "training",
      "recognition",
      "translation",
      "levels",
      "accuracy",
      "imagenet*"
    ]
  },
  {
    "owner": "openai",
    "name": "apps-sdk-ui",
    "url": "https://github.com/openai/apps-sdk-ui",
    "language": "TypeScript",
    "stars": 831,
    "forks": 95,
    "updated_at": "2026-02-07T10:52:31Z",
    "description": null,
    "summary": "# Apps SDK UI\n\nApps SDK UI is a lightweight, accessible design system for building high-quality ChatGPT apps with the [Apps SDK](https://developers.openai.com/apps-sdk). It provides Tailwind-integrated design tokens, a curated React component library, and utilities optimized for consistent experiences inside ChatGPT.\n\n## Features\n\n- **Design tokens** for colors, typography, spacing, sizing, shadows, surfaces, and more.\n- **Tailwind 4 integration** pre-configured with Apps SDK UI's design tokens.\n- **Accessible components**, built on Radix primitives with consistent styling.\n- **Utilities** for dark mode, responsive layouts, and ChatGPT-optimized behaviors.\n- **Minimal boilerplate** ‚Äî import styles, wrap with a provider, start building.\n\n## Prerequisites\n\nApps SDK UI requires **React 18 or ",
    "keywords": [
      "lightweight",
      "accessible",
      "building",
      "high-quality",
      "chatgpt",
      "sdk](https://developers.openai.com/apps-sdk",
      "provides",
      "tailwind-integrated",
      "tokens",
      "curated",
      "component",
      "library",
      "utilities",
      "optimized",
      "consistent",
      "experiences",
      "features",
      "**design",
      "tokens**",
      "colors"
    ]
  },
  {
    "owner": "openai",
    "name": "assign-one-project-github-action",
    "url": "https://github.com/openai/assign-one-project-github-action",
    "language": "Shell",
    "stars": 12,
    "forks": 16,
    "updated_at": "2025-12-23T04:27:58Z",
    "description": "Automatically add an issue or pull request to specific GitHub Project when you create them.",
    "summary": "# GitHub Action for Assign to One Project\n\n[![Docker Cloud Automated build](https://img.shields.io/docker/cloud/automated/srggrs/assign-one-project-github-action)][docker]\n[![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/srggrs/assign-one-project-github-action)][docker]\n[![Docker Pulls](https://img.shields.io/docker/pulls/srggrs/assign-one-project-github-action)][docker]\n[![Docker Stars](https://img.shields.io/docker/stars/srggrs/assign-one-project-github-action)][docker]\n[![GitHub license](https://img.shields.io/github/license/srggrs/assign-one-project-github-action.svg)][license]\n\n[docker]: https://hub.docker.com/r/srggrs/assign-one-project-github-action\n[license]: https://github.com/srggrs/assign-one-project-github-action/blob/master/LICENSE\n\nAutomatically add an ",
    "keywords": [
      "project",
      "![docker",
      "automated",
      "build](https://img.shields.io/docker/cloud/automated/srggrs/assign-one-project-github-action)][docker",
      "status](https://img.shields.io/docker/cloud/build/srggrs/assign-one-project-github-action)][docker",
      "pulls](https://img.shields.io/docker/pulls/srggrs/assign-one-project-github-action)][docker",
      "stars](https://img.shields.io/docker/stars/srggrs/assign-one-project-github-action)][docker",
      "![github",
      "license](https://img.shields.io/github/license/srggrs/assign-one-project-github-action.svg)][license",
      "docker",
      "https://hub.docker.com/r/srggrs/assign-one-project-github-action",
      "license",
      "https://github.com/srggrs/assign-one-project-github-action/blob/master/license",
      "automatically",
      "request",
      "specific",
      "github",
      "project](https://help.github.com/articles/about-project-boards/",
      "__create__",
      "__label__"
    ]
  },
  {
    "owner": "openai",
    "name": "atari-demo",
    "url": "https://github.com/openai/atari-demo",
    "language": "Python",
    "stars": 33,
    "forks": 18,
    "updated_at": "2025-11-26T19:12:16Z",
    "description": "Code for the blog post \"Learning Montezuma‚Äôs Revenge from a Single Demonstration\"",
    "summary": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# atari-demo\nRecord demonstrations for atari.\n\nUse as `pythonw record_demo.py --game='MontezumaRevenge'`\n\nRequires pygame (`pip install pygame`)\n",
    "keywords": [
      "**status:**",
      "archive",
      "provided",
      "updates",
      "expected",
      "atari-demo",
      "demonstrations",
      "`pythonw",
      "record_demo.py",
      "--game='montezumarevenge'`",
      "requires",
      "install",
      "pygame`"
    ]
  },
  {
    "owner": "openai",
    "name": "atari-py",
    "url": "https://github.com/openai/atari-py",
    "language": "C++",
    "stars": 388,
    "forks": 184,
    "updated_at": "2026-01-08T11:30:10Z",
    "description": "A packaged and slightly-modified version of https://github.com/bbitmaster/ale_python_interface",
    "summary": "**Status:** Deprecated (don't expect bug fixes or other updates)\n\nNotice: `atari-py` is fully deprecated and no future updates, bug fixes or releases will be made.\nPlease use the official [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment) Python package (`ale-py`) instead;\nit is fully backwards compatible with all `atari-py` code.\n\n\n# atari_py\n\n[![Build Status](https://travis-ci.org/openai/atari-py.svg?branch=master)](https://travis-ci.org/openai/atari-py)\n\nA packaged and slightly-modified version of [https://github.com/bbitmaster/ale_python_interface](https://github.com/bbitmaster/ale_python_interface).\n\n## Supported Systems\n\natari-py supports Linux and Mac OS X with Python 3.5, 3.6, and 3.7.  Binaries for those platforms are published to [PyPI](http",
    "keywords": [
      "**status:**",
      "deprecated",
      "updates",
      "notice",
      "`atari-py`",
      "releases",
      "official",
      "arcade",
      "learning",
      "environment](https://github.com/mgbellemare/arcade-learning-environment",
      "package",
      "`ale-py`",
      "instead",
      "backwards",
      "compatible",
      "atari_py",
      "![build",
      "status](https://travis-ci.org/openai/atari-py.svg?branch=master)](https://travis-ci.org/openai/atari-py",
      "packaged",
      "slightly-modified"
    ]
  },
  {
    "owner": "openai",
    "name": "atari-reset",
    "url": "https://github.com/openai/atari-reset",
    "language": "Python",
    "stars": 207,
    "forks": 44,
    "updated_at": "2025-12-01T13:19:56Z",
    "description": "Code for the blog post \"Learning Montezuma‚Äôs Revenge from a Single Demonstration\"",
    "summary": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Learn RL policies on Atari by resetting from a demonstration\n\nCodebase for learning to play Atari from demonstrations. Contrary to other work on learning from demonstrations we learn to maximize the score using pure RL, rather than trying to imitate the demo.\n\nAll learning is done through RL on the regular Atari environments, but we automatically build a curriculum for our agent by starting the rollouts from points in a demonstration provided by a human expert: We start by having each RL episode begin near the end of the demonstration. Once the agent is able to beat or at least tie the score of the demonstrator on the remaining part of the game, in at least 20\\% of the rollouts, we slowly move the starting point back in t",
    "keywords": [
      "**status:**",
      "archive",
      "provided",
      "updates",
      "expected",
      "policies",
      "resetting",
      "demonstration",
      "codebase",
      "learning",
      "demonstrations",
      "contrary",
      "maximize",
      "imitate",
      "through",
      "regular",
      "environments",
      "automatically",
      "curriculum",
      "starting"
    ]
  },
  {
    "owner": "openai",
    "name": "automated-interpretability",
    "url": "https://github.com/openai/automated-interpretability",
    "language": "Python",
    "stars": 1069,
    "forks": 124,
    "updated_at": "2026-02-06T12:37:09Z",
    "description": null,
    "summary": "# Automated interpretability\n\n## Code and tools\n\nThis repository contains code and tools associated with the [Language models can explain neurons in\nlanguage models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html) paper, specifically:\n\n* Code for automatically generating, simulating, and scoring explanations of neuron behavior using\nthe methodology described in the paper. See the\n[neuron-explainer README](neuron-explainer/README.md) for more information.\n\nNote: if you run into errors of the form \"Error: Could not find any credentials that grant access to storage account: 'openaipublic' and container: 'neuron-explainer'\".\" you might be able to fix this by signing up for an azure account and specifying the credentials as described in the error message. \n\n* A too",
    "keywords": [
      "automated",
      "interpretability",
      "repository",
      "contains",
      "associated",
      "language",
      "explain",
      "neurons",
      "models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html",
      "specifically",
      "automatically",
      "generating",
      "simulating",
      "scoring",
      "explanations",
      "behavior",
      "methodology",
      "described",
      "neuron-explainer",
      "readme](neuron-explainer/readme.md"
    ]
  },
  {
    "owner": "openai",
    "name": "aws-fluent-plugin-kinesis",
    "url": "https://github.com/openai/aws-fluent-plugin-kinesis",
    "language": "Ruby",
    "stars": 12,
    "forks": 24,
    "updated_at": "2025-12-23T04:25:43Z",
    "description": "Fluentd output plugin that sends events to Amazon Kinesis Streams and Amazon Kinesis Firehose.",
    "summary": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Fluent plugin for Amazon Kinesis\n\n[![Build Status](https://travis-ci.org/awslabs/aws-fluent-plugin-kinesis.svg?branch=master)](https://travis-ci.org/awslabs/aws-fluent-plugin-kinesis)\n\n[Fluentd][fluentd] output plugin\nthat sends events to [Amazon Kinesis Streams][streams] (via both API and [Kinesis Producer Library (KPL)][producer]) and [Amazon Kinesis Firehose][firehose] (via API). This gem includes three output plugins respectively:\n\n- `kinesis_streams`\n- `kinesis_producer`\n- `kinesis_firehose`\n\nAlso, there is a [documentation on Fluentd official site][fluentd-doc-kinesis].\n\n## Warning: `kinesis` is no longer supported\nAs of v1.0.0, `kinesis` plugin is no longer supported. Still you can use the plugin, but if you see th",
    "keywords": [
      "**status:**",
      "archive",
      "provided",
      "updates",
      "expected",
      "kinesis",
      "![build",
      "status](https://travis-ci.org/awslabs/aws-fluent-plugin-kinesis.svg?branch=master)](https://travis-ci.org/awslabs/aws-fluent-plugin-kinesis",
      "fluentd][fluentd",
      "amazon",
      "streams][streams",
      "producer",
      "library",
      "kpl)][producer",
      "firehose][firehose",
      "includes",
      "plugins",
      "respectively",
      "`kinesis_streams`",
      "`kinesis_producer`"
    ]
  },
  {
    "owner": "openai",
    "name": "azure-cli",
    "url": "https://github.com/openai/azure-cli",
    "language": null,
    "stars": 11,
    "forks": 7,
    "updated_at": "2026-02-04T03:34:52Z",
    "description": "Azure Command-Line Interface",
    "summary": "# Microsoft Azure CLI\n\n[![Python](https://img.shields.io/pypi/pyversions/azure-cli.svg?maxAge=2592000)](https://pypi.python.org/pypi/azure-cli)\n[![Build Status](https://dev.azure.com/azure-sdk/public/_apis/build/status/cli/Azure.azure-cli?branchName=dev)](https://dev.azure.com/azure-sdk/public/_build/latest?definitionId=246&branchName=dev)\n[![Slack](https://img.shields.io/badge/Slack-azurecli.slack.com-blue.svg)](https://azurecli.slack.com)\n\nA great cloud needs great tools; we're excited to introduce *Azure CLI*, our next generation multi-platform command line experience for Azure.\n\nTake a test run now from [Azure Cloud Shell](https://portal.azure.com/#cloudshell)!\n\n## Installation\n\nPlease refer to the [install guide](https://docs.microsoft.com/cli/azure/install-azure-cli) for detailed ins",
    "keywords": [
      "microsoft",
      "![python](https://img.shields.io/pypi/pyversions/azure-cli.svg?maxage=2592000)](https://pypi.python.org/pypi/azure-cli",
      "![build",
      "status](https://dev.azure.com/azure-sdk/public/_apis/build/status/cli/azure.azure-cli?branchname=dev)](https://dev.azure.com/azure-sdk/public/_build/latest?definitionid=246&branchname=dev",
      "![slack](https://img.shields.io/badge/slack-azurecli.slack.com-blue.svg)](https://azurecli.slack.com",
      "excited",
      "introduce",
      "generation",
      "multi-platform",
      "command",
      "experience",
      "shell](https://portal.azure.com/#cloudshell)!",
      "installation",
      "install",
      "guide](https://docs.microsoft.com/cli/azure/install-azure-cli",
      "detailed",
      "instructions",
      "resolutions",
      "available",
      "troubleshooting](https://github.com/azure/azure-cli/blob/dev/doc/install_troubleshooting.md"
    ]
  },
  {
    "owner": "openai",
    "name": "baselines",
    "url": "https://github.com/openai/baselines",
    "language": "Python",
    "stars": 16650,
    "forks": 4955,
    "updated_at": "2026-02-09T08:08:25Z",
    "description": "OpenAI Baselines: high-quality implementations of reinforcement learning algorithms",
    "summary": "**Status:** Maintenance (expect bug fixes and minor updates)\n\n<img src=\"data/logo.jpg\" width=25% align=\"right\" /> [![Build status](https://travis-ci.org/openai/baselines.svg?branch=master)](https://travis-ci.org/openai/baselines)\n\n# Baselines\n\nOpenAI Baselines is a set of high-quality implementations of reinforcement learning algorithms.\n\nThese algorithms will make it easier for the research community to replicate, refine, and identify new ideas, and will create good baselines to build research on top of. Our DQN implementation and its variants are roughly on par with the scores in published papers. We expect they will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. \n\n## Prerequisites \nBaselines requires python3 (>=3.",
    "keywords": [
      "**status:**",
      "maintenance",
      "expect",
      "updates",
      "src=\"data/logo.jpg",
      "width=25%",
      "align=\"right",
      "![build",
      "status](https://travis-ci.org/openai/baselines.svg?branch=master)](https://travis-ci.org/openai/baselines",
      "baselines",
      "high-quality",
      "implementations",
      "reinforcement",
      "learning",
      "algorithms",
      "research",
      "community",
      "replicate",
      "refine",
      "identify"
    ]
  },
  {
    "owner": "openai",
    "name": "baselines-results",
    "url": "https://github.com/openai/baselines-results",
    "language": "Jupyter Notebook",
    "stars": 119,
    "forks": 56,
    "updated_at": "2025-11-26T19:12:13Z",
    "description": null,
    "summary": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# DQN results\n\nThis repository contains a [jupyter notebook with the results from running `openai/baselines`](https://github.com/openai/baselines-results/blob/master/dqn_results.ipynb)\n",
    "keywords": [
      "**status:**",
      "archive",
      "provided",
      "updates",
      "expected",
      "results",
      "repository",
      "contains",
      "jupyter",
      "notebook",
      "running",
      "`openai/baselines`](https://github.com/openai/baselines-results/blob/master/dqn_results.ipynb"
    ]
  },
  {
    "owner": "openai",
    "name": "blocksparse",
    "url": "https://github.com/openai/blocksparse",
    "language": "Cuda",
    "stars": 1063,
    "forks": 198,
    "updated_at": "2026-01-30T08:31:10Z",
    "description": "Efficient GPU kernels for block-sparse matrix multiplication and convolution",
    "summary": "**Status:** Active (under active development, breaking changes may occur)\n\n# Blocksparse\n\nThe `blocksparse` package contains TensorFlow Ops and corresponding GPU kernels for block-sparse matrix multiplication.  Also included are related ops like edge bias, sparse weight norm and layer norm.\n\nTo learn more, see [the launch post on the OpenAI blog](https://blog.openai.com/block-sparse-gpu-kernels/).\n\n## Prerequisites\n\nFirst, you need at least one Nvidia GPU. For best performance, we recommend using a Pascal or Maxwell generation GPU -- this is the full list of features by GPU type:\n\n| GPU Family | BSMatMul-ASM | BSMatMul-CudaC | BSConv |\n|------------|------------------------|----------------|--------|\n| Kepler | - | X | - |\n| Maxwell | X (fastest) | X | X |\n| Pascal | X (fastest) | X | X |\n",
    "keywords": [
      "**status:**",
      "development",
      "breaking",
      "changes",
      "blocksparse",
      "`blocksparse`",
      "package",
      "contains",
      "tensorflow",
      "corresponding",
      "kernels",
      "block-sparse",
      "multiplication",
      "included",
      "related",
      "blog](https://blog.openai.com/block-sparse-gpu-kernels/",
      "prerequisites",
      "performance",
      "recommend",
      "maxwell"
    ]
  },
  {
    "owner": "openai",
    "name": "box2d-py",
    "url": "https://github.com/openai/box2d-py",
    "language": "C++",
    "stars": 41,
    "forks": 27,
    "updated_at": "2025-11-26T19:12:32Z",
    "description": null,
    "summary": "**Status:** Maintenance (expect bug fixes and minor updates)\n\nThis is a repackaged version of [pybox2d](https://github.com/pybox2d/pybox2d). For all information, see that repo. For licensing information, see `LICENSE`.\n",
    "keywords": [
      "**status:**",
      "maintenance",
      "expect",
      "updates",
      "repackaged",
      "version",
      "pybox2d](https://github.com/pybox2d/pybox2d",
      "information",
      "licensing",
      "`license`"
    ]
  },
  {
    "owner": "openai",
    "name": "bugbounty-gpt",
    "url": "https://github.com/openai/bugbounty-gpt",
    "language": "Python",
    "stars": 50,
    "forks": 14,
    "updated_at": "2025-11-26T19:14:13Z",
    "description": "A helpful gpt-based triage tool for BugCrowd bugbounty programs.",
    "summary": "# BugCrowd GPT Classifier\n\nThis project is a system designed to manage, classify, and process BugCrowd submissions using OpenAI, and a Postgres Database for data storage. It provides an automatic handling mechanism for submissions and can be run directly or within a Docker container.\n\n## Table of Contents\n\n- [BugCrowd GPT Clasifier](#bugcrowd-gpt-clasifier)\n  - [Table of Contents](#table-of-contents)\n  - [Prerequisites](#prerequisites)\n  - [Getting Started](#getting-started)\n    - [Configuration](#configuration)\n      - [`config.yaml` File](#configyaml-file)\n        - [API Settings](#api-settings)\n        - [User Settings](#user-settings)\n        - [Categories](#categories)\n        - [OpenAI Prompt](#openai-prompt)\n      - [Environment Variables](#environment-variables)\n      - [Docker Com",
    "keywords": [
      "bugcrowd",
      "classifier",
      "project",
      "designed",
      "manage",
      "classify",
      "process",
      "submissions",
      "openai",
      "postgres",
      "database",
      "storage",
      "provides",
      "automatic",
      "handling",
      "mechanism",
      "directly",
      "container",
      "contents",
      "clasifier](#bugcrowd-gpt-clasifier"
    ]
  },
  {
    "owner": "openai",
    "name": "build-hours",
    "url": "https://github.com/openai/build-hours",
    "language": "Jupyter Notebook",
    "stars": 691,
    "forks": 143,
    "updated_at": "2026-02-09T00:59:25Z",
    "description": "Build hours code to share.",
    "summary": "# Build Hours\n\n[OpenAI Build Hours](https://webinar.openai.com/buildhours) is a live virtual series led by our technical team to help you build with the latest product releases. These one-hour interactive sessions include hands-on demos with a code repo, best practices, and live Q&A. \n\nRegister for all upcoming sessions and watch on demand recordings on the [Build Hours homepage](https://webinar.openai.com/buildhours). \n\n<img width=\"1156\" height=\"577\" alt=\"Screenshot 2025-07-18 at 12 07 14‚ÄØPM\" src=\"https://github.com/user-attachments/assets/3d58f9ba-c44c-402a-8fd2-b7759270c0c6\" />\n\n",
    "keywords": [
      "openai",
      "hours](https://webinar.openai.com/buildhours",
      "virtual",
      "technical",
      "product",
      "releases",
      "one-hour",
      "interactive",
      "sessions",
      "include",
      "hands-on",
      "practices",
      "register",
      "upcoming",
      "recordings",
      "homepage](https://webinar.openai.com/buildhours",
      "width=\"1156",
      "height=\"577",
      "alt=\"screenshot",
      "2025-07-18"
    ]
  },
  {
    "owner": "openai",
    "name": "ceph-chef",
    "url": "https://github.com/openai/ceph-chef",
    "language": "Ruby",
    "stars": 11,
    "forks": 16,
    "updated_at": "2025-11-26T19:12:23Z",
    "description": "Chef cookbooks for managing a Ceph cluster",
    "summary": "# Ceph-Chef Cookbook\n\n[![Join the chat at https://gitter.im/ceph/ceph-chef](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/ceph/ceph-chef?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![License](https://img.shields.io/badge/license-Apache_2-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0)\n\n## DESCRIPTION\n\nInstalls and configures Ceph, a distributed network storage and filesystem designed to provide excellent performance, reliability, and scalability. Supports *Hammer* and higher releases (nothing below Hammer is supported in this repo).\n\n>Once *Hammer* support has stopped in Ceph then it will be removed from this cookbook as an option.\n\nThe current version is focused on installing and configuring Ceph for Ubuntu, CentOS and RHEL.\n\nFor docu",
    "keywords": [
      "ceph-chef",
      "cookbook",
      "![join",
      "https://gitter.im/ceph/ceph-chef](https://badges.gitter.im/join%20chat.svg)](https://gitter.im/ceph/ceph-chef?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge",
      "![license](https://img.shields.io/badge/license-apache_2-blue.svg)](https://www.apache.org/licenses/license-2.0",
      "description",
      "installs",
      "configures",
      "distributed",
      "network",
      "storage",
      "filesystem",
      "designed",
      "provide",
      "excellent",
      "performance",
      "reliability",
      "scalability",
      "supports",
      "*hammer*"
    ]
  },
  {
    "owner": "openai",
    "name": "chatgpt-retrieval-plugin",
    "url": "https://github.com/openai/chatgpt-retrieval-plugin",
    "language": "Python",
    "stars": 21224,
    "forks": 3661,
    "updated_at": "2026-02-09T14:33:30Z",
    "description": "The ChatGPT Retrieval Plugin lets you easily find personal or work documents by asking questions in natural language.",
    "summary": "# ChatGPT Retrieval Plugin\n\nBuild Custom GPTs with a Retrieval Plugin backend to give ChatGPT access to personal documents.\n![Example Custom GPT Screenshot](/assets/example.png)\n\n## Introduction\n\nThe ChatGPT Retrieval Plugin repository provides a flexible solution for semantic search and retrieval of personal or organizational documents using natural language queries. It is a standalone retrieval backend, and can be used with [ChatGPT custom GPTs](https://chat.openai.com/gpts/discovery), [function calling](https://platform.openai.com/docs/guides/function-calling) with the [chat completions](https://platform.openai.com/docs/guides/text-generation) or [assistants APIs](https://platform.openai.com/docs/assistants/overview), or with the [ChatGPT plugins model (deprecated)](https://chat.openai.",
    "keywords": [
      "chatgpt",
      "retrieval",
      "backend",
      "personal",
      "documents",
      "![example",
      "screenshot](/assets/example.png",
      "introduction",
      "repository",
      "provides",
      "flexible",
      "solution",
      "semantic",
      "organizational",
      "natural",
      "language",
      "queries",
      "standalone",
      "gpts](https://chat.openai.com/gpts/discovery",
      "function"
    ]
  },
  {
    "owner": "openai",
    "name": "chatkit-js",
    "url": "https://github.com/openai/chatkit-js",
    "language": "TypeScript",
    "stars": 1734,
    "forks": 158,
    "updated_at": "2026-02-08T07:27:30Z",
    "description": null,
    "summary": "ChatKit is a batteries-included framework for building high-quality, AI-powered chat experiences. It‚Äôs designed for developers who want to add advanced conversational intelligence to their apps fast‚Äîwith minimal setup and no reinventing the wheel. ChatKit delivers a complete, production-ready chat interface out of the box.\n\n**Key features include:**\n\n- **Deep UI customization** so that ChatKit feels like a first-class part of your app\n- **Built-in response streaming** for interactive, natural conversations\n- **Tool and workflow integration** for visualizing agentic actions and chain-of-thought reasoning\n- **Rich interactive widgets** rendered directly inside the chat\n- **Attachment handling** with support for file and image uploads\n- **Thread and message management** for organizing complex",
    "keywords": [
      "chatkit",
      "batteries-included",
      "framework",
      "building",
      "high-quality",
      "ai-powered",
      "experiences",
      "designed",
      "developers",
      "advanced",
      "conversational",
      "intelligence",
      "fast‚Äîwith",
      "minimal",
      "reinventing",
      "delivers",
      "complete",
      "production-ready",
      "interface",
      "features"
    ]
  },
  {
    "owner": "openai",
    "name": "chatkit-python",
    "url": "https://github.com/openai/chatkit-python",
    "language": "Python",
    "stars": 357,
    "forks": 121,
    "updated_at": "2026-02-09T06:34:13Z",
    "description": null,
    "summary": "## License\nThis project is licensed under the [Apache License 2.0](LICENSE).\n",
    "keywords": [
      "license",
      "project",
      "licensed",
      "apache",
      "2.0](license"
    ]
  },
  {
    "owner": "openai",
    "name": "chef-cookbook-hostname",
    "url": "https://github.com/openai/chef-cookbook-hostname",
    "language": null,
    "stars": 3,
    "forks": 2,
    "updated_at": "2025-11-26T19:14:14Z",
    "description": "Chef cookbook to set node's hostname and FQDN.",
    "summary": "# hostname cookbook\n\n## Description\n\nSets hostname and FQDN of the node. The latest code is hosted at\nhttps://github.com/3ofcoins/chef-cookbook-hostname\n\n### Important\n\nSetting hostname on FQDN is not (and won't be) supported. Unfortunately, using dots in the hostname can cause\n[inconsistent results for any system that consumes DNS](http://serverfault.com/questions/229331/can-i-have-dots-in-a-hostname)\nand [is not allowed by RFC952](http://tools.ietf.org/html/rfc952). If a user\nneeds additional info in their shell prompt, they can change PS1 in etc/profile\nto include the FQDN together with any information they find useful (such as\nthe customer, the environment, etc).\n\n## Attributes\n\n- `node['set_fqdn']` - FQDN to set.\n\nThe asterisk character will be replaced with `node.name`. This way,\nyou",
    "keywords": [
      "hostname",
      "cookbook",
      "description",
      "https://github.com/3ofcoins/chef-cookbook-hostname",
      "important",
      "setting",
      "supported",
      "unfortunately",
      "inconsistent",
      "results",
      "consumes",
      "dns](http://serverfault.com/questions/229331/can-i-have-dots-in-a-hostname",
      "allowed",
      "rfc952](http://tools.ietf.org/html/rfc952",
      "additional",
      "prompt",
      "etc/profile",
      "include",
      "together",
      "information"
    ]
  },
  {
    "owner": "openai",
    "name": "chef-logdna_agent",
    "url": "https://github.com/openai/chef-logdna_agent",
    "language": "Ruby",
    "stars": 8,
    "forks": 16,
    "updated_at": "2025-11-26T19:12:22Z",
    "description": "Installs the LogDNA host agent. Currently only supports apt systems.",
    "summary": "logdna_agent Cookbook\n=====================\nInstalls the LogDNA host agent. Currently only supports apt systems.\n\n\nAttributes\n----------\n\n#### logdna_agent::default\n<table>\n  <tr>\n    <th>Key</th>\n    <th>Type</th>\n    <th>Description</th>\n    <th>Default</th>\n  </tr>\n  <tr>\n    <td><tt>['logdna_agent']['api_key']</tt></td>\n    <td>String</td>\n    <td>Your API key as provided by LogDNA.</td>\n    <td><tt>''</tt></td>\n  </tr>\n  <tr>\n    <td><tt>['logdna_agent']['log_directories']</tt></td>\n    <td>Array[String]</td>\n    <td>Directories to follow (recursively) in addition to <var>/var/log</var>.</td>\n    <td><tt>[]</tt></td>\n  </tr>\n  <tr>\n    <td><tt>['logdna_agent']['tags']</tt></td>\n    <td>String</td>\n    <td>(Optional) Comma-separated list of tags to apply to the host.</td>\n    <td><tt>'",
    "keywords": [
      "logdna_agent",
      "cookbook",
      "=====================",
      "installs",
      "currently",
      "supports",
      "systems",
      "attributes",
      "----------",
      "logdna_agent::default",
      "<table>",
      "<th>key</th>",
      "<th>type</th>",
      "<th>description</th>",
      "<th>default</th>",
      "<td><tt>['logdna_agent']['api_key']</tt></td>",
      "<td>string</td>",
      "<td>your",
      "provided",
      "logdna.</td>"
    ]
  },
  {
    "owner": "openai",
    "name": "chz",
    "url": "https://github.com/openai/chz",
    "language": "Python",
    "stars": 221,
    "forks": 12,
    "updated_at": "2026-02-07T20:16:04Z",
    "description": null,
    "summary": "# ü™§ chz\n\n*(pronounced \"‡§ö‡•Ä‡•õ\")*\n\n`chz` helps you manage configuration, particularly from the command line.\n\n`chz` is available on [PyPI](https://pypi.org/project/chz/).\n\nTo click the links below, please visit [Github](https://github.com/openai/chz).\n\nOverview:\n- [Quickstart](docs/01_quickstart.md)\n- [Declarative object model](docs/02_object_model.md)\n- [Immutability](docs/02_object_model.md#immutability)\n- [Validation](docs/03_validation.md)\n- [Type checking](docs/03_validation.md#type-checking)\n- [Command line parsing](docs/04_command_line.md)\n- [Discoverability](docs/04_command_line.md#discoverability---help-and-errors)\n- [Partial application](docs/05_blueprint.md)\n- [Presets or shared configuration](docs/05_blueprint.md#presets-or-shared-configuration)\n- [Serialisation and deserialisation",
    "keywords": [
      "*(pronounced",
      "‡§ö‡•Ä‡•õ\")*",
      "configuration",
      "particularly",
      "command",
      "available",
      "pypi](https://pypi.org/project/chz/",
      "github](https://github.com/openai/chz",
      "overview",
      "quickstart](docs/01_quickstart.md",
      "declarative",
      "model](docs/02_object_model.md",
      "immutability](docs/02_object_model.md#immutability",
      "validation](docs/03_validation.md",
      "checking](docs/03_validation.md#type-checking",
      "parsing](docs/04_command_line.md",
      "discoverability](docs/04_command_line.md#discoverability---help-and-errors",
      "partial",
      "application](docs/05_blueprint.md",
      "presets"
    ]
  },
  {
    "owner": "openai",
    "name": "circuit_sparsity",
    "url": "https://github.com/openai/circuit_sparsity",
    "language": "Python",
    "stars": 502,
    "forks": 57,
    "updated_at": "2026-02-09T12:47:14Z",
    "description": "Open-source release accompanying Gao et al. 2025",
    "summary": "# Circuit Sparsity Visualizer and Models\n\nTools for inspecting sparse circuit models from [Gao et al. 2025](https://openai.com/index/understanding-neural-networks-through-sparse-circuits/). Provides code \nfor running inference as well as a Streamlit dashboard that allows you to interact\nwith task-specific circuits found by pruning. Note: this README was AI-generated and lightly edited.\n\n## Installation\n\n```bash\npip install -e .\n```\n\n## Launching the Visualizer\n\nStart the Streamlit app from the project root:\n\n```bash\nstreamlit run circuit_sparsity/viz.py\n```\n\nThe app loads data from the openaipublic webpage and caches locally. When the\nvisualizer loads you can choose a model, dataset, pruning sweep, and node budget `k`\nusing the controls in the left column. The plots are rendered with Plotl",
    "keywords": [
      "circuit",
      "sparsity",
      "visualizer",
      "inspecting",
      "2025](https://openai.com/index/understanding-neural-networks-through-sparse-circuits/",
      "provides",
      "running",
      "inference",
      "streamlit",
      "dashboard",
      "interact",
      "task-specific",
      "circuits",
      "pruning",
      "ai-generated",
      "lightly",
      "edited",
      "installation",
      "```bash",
      "install"
    ]
  },
  {
    "owner": "openai",
    "name": "CLIP",
    "url": "https://github.com/openai/CLIP",
    "language": "Jupyter Notebook",
    "stars": 32529,
    "forks": 3903,
    "updated_at": "2026-02-09T14:29:12Z",
    "description": "CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image",
    "summary": "# CLIP\n\n[[Blog]](https://openai.com/blog/clip/) [[Paper]](https://arxiv.org/abs/2103.00020) [[Model Card]](model-card.md) [[Colab]](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb)\n\nCLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet ‚Äúzero-shot‚Äù without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision.\n\n\n\n## Approach\n\n![CLIP](CLIP.png)\n\n\n\n## Usage\n\nFirst",
    "keywords": [
      "blog]](https://openai.com/blog/clip/",
      "paper]](https://arxiv.org/abs/2103.00020",
      "model",
      "card]](model-card.md",
      "colab]](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/interacting_with_clip.ipynb",
      "contrastive",
      "language-image",
      "pre-training",
      "network",
      "trained",
      "variety",
      "image",
      "instructed",
      "natural",
      "language",
      "predict",
      "relevant",
      "snippet",
      "without",
      "directly"
    ]
  },
  {
    "owner": "openai",
    "name": "CLIP-featurevis",
    "url": "https://github.com/openai/CLIP-featurevis",
    "language": "Python",
    "stars": 309,
    "forks": 69,
    "updated_at": "2026-01-23T14:21:12Z",
    "description": "code for reproducing some of the diagrams in the paper \"Multimodal Neurons in Artificial Neural Networks\"",
    "summary": null,
    "keywords": []
  },
  {
    "owner": "openai",
    "name": "code-align-evals-data",
    "url": "https://github.com/openai/code-align-evals-data",
    "language": "Python",
    "stars": 28,
    "forks": 12,
    "updated_at": "2025-11-26T19:13:18Z",
    "description": null,
    "summary": "# code-align-evals-data\n\n# Repo structure\n\n## Datasets\n\n **human_eval**: Contains original 158 human-written tasks for evaluating capabilities performance. Each file contains a docstring describing a function, a human-written solution to that task, and human-written code to test whether an implementation is correct\n\n**bad_solutions**: Buggy solutions to the first 30 human_eval tasks. These can be put in the context of an eval task and we can measure the decrease in performance\n\n**alignment**: Eval tasks testing alignment, of four different types:\n\n- **bad_contexts**: Human eval tasks with buggy vs non-buggy solutions in the contex\n\n- **find_bug**: The same human_eval tasks, with an example implementation in the docstring, that contains a single bug. The model is asked to print which line c",
    "keywords": [
      "code-align-evals-data",
      "structure",
      "datasets",
      "**human_eval**",
      "contains",
      "original",
      "human-written",
      "evaluating",
      "capabilities",
      "performance",
      "docstring",
      "describing",
      "function",
      "solution",
      "whether",
      "implementation",
      "correct",
      "**bad_solutions**",
      "solutions",
      "human_eval"
    ]
  },
  {
    "owner": "openai",
    "name": "codex",
    "url": "https://github.com/openai/codex",
    "language": "Rust",
    "stars": 59699,
    "forks": 7834,
    "updated_at": "2026-02-09T17:59:56Z",
    "description": "Lightweight coding agent that runs in your terminal",
    "summary": "<p align=\"center\"><code>npm i -g @openai/codex</code><br />or <code>brew install --cask codex</code></p>\n<p align=\"center\"><strong>Codex CLI</strong> is a coding agent from OpenAI that runs locally on your computer.\n<p align=\"center\">\n  <img src=\"https://github.com/openai/codex/blob/main/.github/codex-cli-splash.png\" alt=\"Codex CLI splash\" width=\"80%\" />\n</p>\n</br>\nIf you want Codex in your code editor (VS Code, Cursor, Windsurf), <a href=\"https://developers.openai.com/codex/ide\">install in your IDE.</a>\n</br>If you are looking for the <em>cloud-based agent</em> from OpenAI, <strong>Codex Web</strong>, go to <a href=\"https://chatgpt.com/codex\">chatgpt.com/codex</a>.</p>\n\n---\n\n## Quickstart\n\n### Installing and running Codex CLI\n\nInstall globally with your preferred package manager:\n\n```shel",
    "keywords": [
      "align=\"center\"><code>npm",
      "@openai/codex</code><br",
      "<code>brew",
      "install",
      "codex</code></p>",
      "align=\"center\"><strong>codex",
      "cli</strong>",
      "locally",
      "computer",
      "align=\"center\">",
      "src=\"https://github.com/openai/codex/blob/main/.github/codex-cli-splash.png",
      "alt=\"codex",
      "splash",
      "width=\"80%",
      "cursor",
      "windsurf",
      "href=\"https://developers.openai.com/codex/ide\">install",
      "ide.</a>",
      "</br>if",
      "looking"
    ]
  },
  {
    "owner": "openai",
    "name": "codex-action",
    "url": "https://github.com/openai/codex-action",
    "language": "TypeScript",
    "stars": 799,
    "forks": 86,
    "updated_at": "2026-02-09T05:48:30Z",
    "description": null,
    "summary": "# Codex GitHub Action\n\nRun [Codex](https://github.com/openai/codex#codex-exec) from a GitHub Actions workflow while keeping tight control over the privileges available to Codex. This action handles installing the Codex CLI and configuring it with a secure proxy to the [Responses API](https://platform.openai.com/docs/api-reference/responses).\n\nUsers must provide an API key for their chosen provider (for example, [`OPENAI_API_KEY`](https://platform.openai.com/api-keys) or `AZURE_OPENAI_API_KEY` [if using Azure for OpenAI models](#azure)) as a [GitHub Actions secret](https://docs.github.com/en/actions/how-tos/write-workflows/choose-what-workflows-do/use-secrets) to use this action.\n\n## Example: Create Your Own Pull Request Bot\n\nWhile Codex cloud offers a [powerful code review tool](https://de",
    "keywords": [
      "codex](https://github.com/openai/codex#codex-exec",
      "actions",
      "workflow",
      "keeping",
      "control",
      "privileges",
      "available",
      "handles",
      "installing",
      "configuring",
      "responses",
      "api](https://platform.openai.com/docs/api-reference/responses",
      "provide",
      "provider",
      "example",
      "`openai_api_key`](https://platform.openai.com/api-keys",
      "`azure_openai_api_key`",
      "models](#azure",
      "github",
      "secret](https://docs.github.com/en/actions/how-tos/write-workflows/choose-what-workflows-do/use-secrets"
    ]
  },
  {
    "owner": "openai",
    "name": "codex-universal",
    "url": "https://github.com/openai/codex-universal",
    "language": "Dockerfile",
    "stars": 790,
    "forks": 218,
    "updated_at": "2026-02-09T01:33:25Z",
    "description": "Base docker image used in Codex environments",
    "summary": "# codex-universal\n\n`codex-universal` is a reference implementation of the base Docker image available in [OpenAI Codex](http://platform.openai.com/docs/codex).\n\nThis repository is intended to help developers cutomize environments in Codex, by providing a similar image that can be pulled and run locally. This is not an identical environment but should help for debugging and development.\n\nFor more details on environment setup, see [OpenAI Codex](http://platform.openai.com/docs/codex).\n\n## Usage\n\nThe Docker image is available at:\n\n```\ndocker pull ghcr.io/openai/codex-universal:latest\n```\n\nThis repository builds the image for both linux/amd64 and linux/arm64. However we only run the linux/amd64 version.\nYour installed Docker may support linux/amd64 emulation by passing the `--platform linux/am",
    "keywords": [
      "codex-universal",
      "`codex-universal`",
      "reference",
      "implementation",
      "available",
      "openai",
      "codex](http://platform.openai.com/docs/codex",
      "repository",
      "intended",
      "developers",
      "cutomize",
      "environments",
      "providing",
      "similar",
      "locally",
      "identical",
      "environment",
      "debugging",
      "development",
      "details"
    ]
  },
  {
    "owner": "openai",
    "name": "coinrun",
    "url": "https://github.com/openai/coinrun",
    "language": "C++",
    "stars": 407,
    "forks": 87,
    "updated_at": "2026-01-25T14:03:36Z",
    "description": "Code for the paper \"Quantifying Transfer in Reinforcement Learning\"",
    "summary": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Quantifying Generalization in Reinforcement Learning\n\n#### [[Blog Post]](https://blog.openai.com/quantifying-generalization-in-reinforcement-learning/) [[Paper]](https://drive.google.com/file/d/1U1-uufB_ZzQ1HG67BhW9bB8mTJ6JtS19/view)\n\nThis is code for the environments used in the paper [Quantifying Generalization in Reinforcement Learning](https://drive.google.com/file/d/1U1-uufB_ZzQ1HG67BhW9bB8mTJ6JtS19/view) along with an example training script.\n\nAuthors: Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, John Schulman\n\n![CoinRun](coinrun.png?raw=true \"CoinRun\")\n\n## Install\n\nYou should install the package in development mode so you can easily change the files.  You may also want to create a virtualenv before installing",
    "keywords": [
      "**status:**",
      "archive",
      "provided",
      "updates",
      "expected",
      "quantifying",
      "generalization",
      "reinforcement",
      "learning",
      "post]](https://blog.openai.com/quantifying-generalization-in-reinforcement-learning/",
      "paper]](https://drive.google.com/file/d/1u1-uufb_zzq1hg67bhw9bb8mtj6jts19/view",
      "environments",
      "learning](https://drive.google.com/file/d/1u1-uufb_zzq1hg67bhw9bb8mtj6jts19/view",
      "example",
      "training",
      "script",
      "authors",
      "klimov",
      "taehoon",
      "schulman"
    ]
  },
  {
    "owner": "ray-project",
    "name": "2022_04_13_ray_serve_meetup_demo",
    "url": "https://github.com/ray-project/2022_04_13_ray_serve_meetup_demo",
    "language": "Python",
    "stars": 3,
    "forks": 1,
    "updated_at": "2026-01-12T18:52:43Z",
    "description": "Code samples for Ray Serve Meetup on 04/13/2022",
    "summary": "# 2022_04_13_ray_serve_meetup_demo\n\nCode samples for Ray Serve Meetup on 04/13/2022\n\n- Download image caption pre-trained model by running `make prep`\n- `python deployment_graph.py` should execute the dag locally\n- `serve run deployment_graph.serve_entrypoint` to deploy to Serve cluster. and use http://localhost:8000/docs call.\n- `serve build deployment_graph.serve_entrypoint > config.yaml`\n- `ray start --head`, `serve start`, and `serve deploy config.yaml`\n",
    "keywords": [
      "2022_04_13_ray_serve_meetup_demo",
      "samples",
      "04/13/2022",
      "download",
      "caption",
      "pre-trained",
      "running",
      "`python",
      "deployment_graph.py`",
      "execute",
      "locally",
      "deployment_graph.serve_entrypoint`",
      "cluster",
      "http://localhost:8000/docs",
      "deployment_graph.serve_entrypoint",
      "config.yaml`",
      "--head`",
      "start`"
    ]
  },
  {
    "owner": "ray-project",
    "name": "air-reference-arch",
    "url": "https://github.com/ray-project/air-reference-arch",
    "language": "Jupyter Notebook",
    "stars": 6,
    "forks": 1,
    "updated_at": "2026-01-12T19:19:25Z",
    "description": null,
    "summary": "# Reference architecture with best of breed OSS ML tools running on top of Ray AIR. \n# Feast & Ray \n\nThis reference architecture contains an end to end example illustrating the following components.\n- Ray AIR for scalable AI Runtime. (Data preprocessing, Train,Tune and batch)\n- Feast for feature store\n- Ray Serve for scalable, composable and framework agnostic ML model serving compute\n\n\n## Overview\n\nThis tutorial demonstrates the use of best of breed ML tools as part of a real-time credit scoring application. It uses the feast on AWS example as a starting point.\n* The primary training dataset is a loan table. This table contains historic loan data with accompanying features. The dataset also contains a target variable, namely whether a user has defaulted on their loan.\n* We will be using t",
    "keywords": [
      "reference",
      "architecture",
      "running",
      "contains",
      "example",
      "illustrating",
      "following",
      "components",
      "scalable",
      "runtime",
      "preprocessing",
      "train,tune",
      "feature",
      "composable",
      "framework",
      "agnostic",
      "serving",
      "compute",
      "overview",
      "tutorial"
    ]
  },
  {
    "owner": "ray-project",
    "name": "air-sample-data",
    "url": "https://github.com/ray-project/air-sample-data",
    "language": null,
    "stars": 0,
    "forks": 0,
    "updated_at": "2026-01-12T19:22:18Z",
    "description": "A repository hosting data for air examples and notebooks",
    "summary": "# air-sample-data\nA repository hosting data for air examples and notebooks\n\n## Attribution\n\n* `breast_cancer.csv` - sourced from https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n* `iris.csv` - sourced from https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "keywords": [
      "air-sample-data",
      "repository",
      "hosting",
      "examples",
      "notebooks",
      "attribution",
      "`breast_cancer.csv`",
      "sourced",
      "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html",
      "`iris.csv`",
      "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html"
    ]
  },
  {
    "owner": "ray-project",
    "name": "anyscale-berkeley-ai-hackathon",
    "url": "https://github.com/ray-project/anyscale-berkeley-ai-hackathon",
    "language": "Jupyter Notebook",
    "stars": 11,
    "forks": 0,
    "updated_at": "2024-03-13T05:39:57Z",
    "description": "Ray and Anyscale for UC Berkeley AI Hackathon!",
    "summary": "# Anyscale - UC Berkeley AI Hackathon!\n\n¬© 2023, Anyscale Inc. All Rights Reserved\n\n<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/events/2023-berkeley-hackathon/hackathon.png\" width=\"90%\" loading=\"lazy\">\n\n<a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\"><img src=\"https://img.shields.io/badge/Ray-Join%20Slack-blue\" alt=\"join-ray-slack\"></a>\n<a href=\"https://discuss.ray.io/\"><img src=\"https://img.shields.io/badge/Discuss-Ask%20Questions-blue\" alt=\"discuss\"></a>\n<a href=\"https://twitter.com/raydistributed\"><img src=\"https://img.shields.io/twitter/follow/raydistributed?label=Follow\" alt=\"twitter\"></a>\n\n## Overview\n\nLLMs have gained immense popularity in recent months. An entirely new ecosystem of pre-trained models and tools has emerged that streamline the process of build",
    "keywords": [
      "anyscale",
      "berkeley",
      "hackathon!",
      "reserved",
      "src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/events/2023-berkeley-hackathon/hackathon.png",
      "width=\"90%",
      "loading=\"lazy\">",
      "href=\"https://forms.gle/9tsddyugxys8sa9e8\"><img",
      "src=\"https://img.shields.io/badge/ray-join%20slack-blue",
      "alt=\"join-ray-slack\"></a>",
      "href=\"https://discuss.ray.io/\"><img",
      "src=\"https://img.shields.io/badge/discuss-ask%20questions-blue",
      "alt=\"discuss\"></a>",
      "href=\"https://twitter.com/raydistributed\"><img",
      "src=\"https://img.shields.io/twitter/follow/raydistributed?label=follow",
      "alt=\"twitter\"></a>",
      "overview",
      "immense",
      "popularity",
      "months"
    ]
  },
  {
    "owner": "ray-project",
    "name": "anyscale-workshop-nyc-2023",
    "url": "https://github.com/ray-project/anyscale-workshop-nyc-2023",
    "language": "Jupyter Notebook",
    "stars": 7,
    "forks": 1,
    "updated_at": "2026-01-30T12:02:17Z",
    "description": "Scalable NLP model fine-tuning and batch inference with Ray and Anyscale",
    "summary": "# Scaling NLP Workloads with Ray and Anyscale\n\n¬© 2023, Anyscale Inc. All Rights Reserved\n\n<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Anyscale-generic/Anyscale-logo-horizontal.png\" width=\"50%\" loading=\"lazy\">\n\n<a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\"><img src=\"https://img.shields.io/badge/Ray-Join%20Slack-blue\" alt=\"join-ray-slack\"></a>\n<a href=\"https://discuss.ray.io/\"><img src=\"https://img.shields.io/badge/Discuss-Ask%20Questions-blue\" alt=\"discuss\"></a>\n<a href=\"https://twitter.com/raydistributed\"><img src=\"https://img.shields.io/twitter/follow/raydistributed?label=Follow\" alt=\"twitter\"></a>\n\nWelcome to the Scaling NLP Workloads with Ray and Anyscale tutorial!\n\n## Tutorial content\n\nThis tutorial is intended to teach you how you can use Ray AI Runtime and ",
    "keywords": [
      "scaling",
      "workloads",
      "anyscale",
      "reserved",
      "src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/anyscale-generic/anyscale-logo-horizontal.png",
      "width=\"50%",
      "loading=\"lazy\">",
      "href=\"https://forms.gle/9tsddyugxys8sa9e8\"><img",
      "src=\"https://img.shields.io/badge/ray-join%20slack-blue",
      "alt=\"join-ray-slack\"></a>",
      "href=\"https://discuss.ray.io/\"><img",
      "src=\"https://img.shields.io/badge/discuss-ask%20questions-blue",
      "alt=\"discuss\"></a>",
      "href=\"https://twitter.com/raydistributed\"><img",
      "src=\"https://img.shields.io/twitter/follow/raydistributed?label=follow",
      "alt=\"twitter\"></a>",
      "welcome",
      "tutorial!",
      "tutorial",
      "content"
    ]
  },
  {
    "owner": "ray-project",
    "name": "arrow",
    "url": "https://github.com/ray-project/arrow",
    "language": "C++",
    "stars": 6,
    "forks": 0,
    "updated_at": "2018-05-21T10:31:34Z",
    "description": "Mirror of Apache Arrow",
    "summary": "<!---\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\n## Apa",
    "keywords": [
      "licensed",
      "software",
      "foundation",
      "contributor",
      "license",
      "agreements",
      "distributed",
      "additional",
      "information",
      "regarding",
      "copyright",
      "ownership",
      "licenses",
      "version",
      "compliance",
      "http://www.apache.org/licenses/license-2.0",
      "required",
      "applicable",
      "writing",
      "without"
    ]
  },
  {
    "owner": "ray-project",
    "name": "arrow-build",
    "url": "https://github.com/ray-project/arrow-build",
    "language": null,
    "stars": 2,
    "forks": 4,
    "updated_at": "2026-01-12T07:47:48Z",
    "description": "Queue for building arrow",
    "summary": "# arrow-build\nQueue for building arrow\n",
    "keywords": [
      "arrow-build",
      "building"
    ]
  },
  {
    "owner": "ray-project",
    "name": "arrow-plasma-object-store",
    "url": "https://github.com/ray-project/arrow-plasma-object-store",
    "language": "C++",
    "stars": 2,
    "forks": 3,
    "updated_at": "2025-12-17T16:46:32Z",
    "description": "Plasma Object Store code for proposed import to Apache Arrow",
    "summary": "## Plasma Object Store\n\nThis code is released under the Apache License 2.0, and was originally\ndeveloped as part of the Ray project with the UC Berkeley RISELab:\n\n* https://github.com/ray-project/ray/tree/master/src/plasma\n* https://github.com/ray-project/ray/tree/master/python/ray/plasma\n\nThe code was authored by:\n\n* Philipp Moritz\n* Robert Nishihara\n* Richard Shin\n* Stephanie Wang\n* Alexey Tumanov\n* Ion Stoica\n* Mehrdad Nicknami\n* Ujval Misra\n\nIt is being proposed for import into the Apache Arrow project\n(http://arrow.apache.org/).",
    "keywords": [
      "released",
      "license",
      "originally",
      "developed",
      "project",
      "berkeley",
      "riselab",
      "https://github.com/ray-project/ray/tree/master/src/plasma",
      "https://github.com/ray-project/ray/tree/master/python/ray/plasma",
      "authored",
      "philipp",
      "nishihara",
      "richard",
      "stephanie",
      "tumanov",
      "mehrdad",
      "nicknami",
      "proposed",
      "http://arrow.apache.org/"
    ]
  },
  {
    "owner": "ray-project",
    "name": "asv",
    "url": "https://github.com/ray-project/asv",
    "language": "Python",
    "stars": 0,
    "forks": 3,
    "updated_at": "2025-12-17T16:56:11Z",
    "description": "Airspeed Velocity: A simple Python benchmarking tool with web-based reporting",
    "summary": null,
    "keywords": []
  },
  {
    "owner": "ray-project",
    "name": "benchmarks",
    "url": "https://github.com/ray-project/benchmarks",
    "language": null,
    "stars": 0,
    "forks": 0,
    "updated_at": "2025-12-17T16:43:07Z",
    "description": null,
    "summary": "# benchmarks\n",
    "keywords": [
      "benchmarks"
    ]
  },
  {
    "owner": "ray-project",
    "name": "bredis",
    "url": "https://github.com/ray-project/bredis",
    "language": "C",
    "stars": 0,
    "forks": 0,
    "updated_at": "2025-12-17T16:43:48Z",
    "description": "Experiments with sharding redis",
    "summary": null,
    "keywords": []
  },
  {
    "owner": "ray-project",
    "name": "catapult",
    "url": "https://github.com/ray-project/catapult",
    "language": "HTML",
    "stars": 1,
    "forks": 0,
    "updated_at": "2025-12-17T16:47:57Z",
    "description": "Catapult",
    "summary": "\n<!-- Copyright 2015 The Chromium Authors. All rights reserved.\n     Use of this source code is governed by a BSD-style license that can be\n     found in the LICENSE file.\n-->\nCatapult\n========\n\nCatapult is the home for several performance tools that span from gathering,\ndisplaying and analyzing performance data. This includes:\n\n * [Trace-viewer](/tracing/README.md)\n * [Telemetry](/telemetry/README.md)\n * [Performance Dashboard](/dashboard/README.md)\n * [Systrace](/systrace/README.md)\n * [Web Page Replay](/web_page_replay_go/README.md)\n\nThese tools were created by Chromium developers for performance analysis,\ntesting, and monitoring of Chrome, but they can also be used for analyzing and\nmonitoring websites, and eventually Android apps.\n\nContributing\n============\nPlease see [our contributor",
    "keywords": [
      "copyright",
      "chromium",
      "authors",
      "reserved",
      "governed",
      "bsd-style",
      "license",
      "catapult",
      "========",
      "several",
      "performance",
      "gathering",
      "displaying",
      "analyzing",
      "includes",
      "trace-viewer](/tracing/readme.md",
      "telemetry](/telemetry/readme.md",
      "dashboard](/dashboard/readme.md",
      "systrace](/systrace/readme.md",
      "replay](/web_page_replay_go/readme.md"
    ]
  },
  {
    "owner": "ray-project",
    "name": "checkstyle_java",
    "url": "https://github.com/ray-project/checkstyle_java",
    "language": "Python",
    "stars": 1,
    "forks": 2,
    "updated_at": "2026-01-12T07:47:00Z",
    "description": null,
    "summary": "# checkstyle_java\nJava checkstyle for bazel build\n",
    "keywords": [
      "checkstyle_java",
      "checkstyle"
    ]
  },
  {
    "owner": "ray-project",
    "name": "common",
    "url": "https://github.com/ray-project/common",
    "language": "C",
    "stars": 1,
    "forks": 6,
    "updated_at": "2026-01-12T07:48:15Z",
    "description": "Code that is shared between Ray projects",
    "summary": null,
    "keywords": []
  },
  {
    "owner": "ray-project",
    "name": "community",
    "url": "https://github.com/ray-project/community",
    "language": null,
    "stars": 9,
    "forks": 2,
    "updated_at": "2026-01-12T19:18:50Z",
    "description": "Artifacts intended to support the Ray Developer Community: SIGs, RFC overviews, and governance. We're very glad you're here! ‚ú®",
    "summary": "# üëã Welcome to the `ray-project` Developer Community!\n\nWe're excited to have you here, and to have you as an extended member of the Ray engineering team. Questions, code, comments, concerns: all are welcome, and valued! Please make sure to take a look at our `CODE OF CONDUCT` and recommendations for `CONTRIBUTIONS`, before referring to the content below. \n\n_We can't wait to see what you build!_\n\n### üì¶  This Repository\n\nThis repository stores documents and artifacts produced by the [`ray-project`](github.com/ray-project) developer community.\n\n* [`rfcs`](https://github.com/ray-project/community/tree/main/rfcs) - A collection of design documents for the `ray-project` and its libraries.\n* [`sigs`](https://github.com/ray-project/community/tree/main/sigs) - Documentation and logistics for our sp",
    "keywords": [
      "welcome",
      "`ray-project`",
      "developer",
      "community!",
      "excited",
      "extended",
      "engineering",
      "questions",
      "comments",
      "concerns",
      "valued!",
      "conduct`",
      "recommendations",
      "`contributions`",
      "referring",
      "content",
      "build!_",
      "repository",
      "documents",
      "artifacts"
    ]
  },
  {
    "owner": "ray-project",
    "name": "contrib-workflow-dag",
    "url": "https://github.com/ray-project/contrib-workflow-dag",
    "language": "Python",
    "stars": 11,
    "forks": 0,
    "updated_at": "2026-01-12T19:02:54Z",
    "description": null,
    "summary": "# contrib-workflow-dag\n\n## Overview\n\nThis repo provides a **Graph** layer implementation and semantic enrichment for creating and running Ray workflows.\n\nGraph (i.e. DAG) layer provides a higher level abstraction on top of workflow steps,\naiming to make workflow construction more convenient and intuitive. It also exposes semantic information such as the\ntype of execution, level of parallelism, and state of application.\n\n## Graph vs. Workflow Step\n\nWorkflows provide the building blocks for repeatable execution of \"steps\" in Ray. However, as we chain multiple steps and evolve the workflow graph, it can\nquickly become a challenge to maintain and modify this graph. DAG APIs enable the end user to abstract the underlying step graph by providing simple APIs for constructing workflows.\n\nFollowing",
    "keywords": [
      "contrib-workflow-dag",
      "overview",
      "provides",
      "**graph**",
      "implementation",
      "semantic",
      "enrichment",
      "creating",
      "running",
      "workflows",
      "abstraction",
      "workflow",
      "construction",
      "convenient",
      "intuitive",
      "exposes",
      "information",
      "execution",
      "parallelism",
      "application"
    ]
  },
  {
    "owner": "ray-project",
    "name": "credis",
    "url": "https://github.com/ray-project/credis",
    "language": "C++",
    "stars": 10,
    "forks": 7,
    "updated_at": "2025-12-17T17:08:49Z",
    "description": null,
    "summary": "# Chain Replicated Redis\n\n## Building\n\n```\ngit submodule init\ngit submodule update\n\n# Install tcmalloc according to\n# https://github.com/gperftools/gperftools/blob/master/INSTALL\n\npushd redis && env USE_TCMALLOC=yes make -j && popd\npushd glog && cmake . && make -j install && popd\npushd leveldb && make -j && popd\n\nmkdir build; cd build\ncmake ..\nmake -j\n```\n\n## Trying it out\n\nFirst we start the master and two chain members:\n\n```\ncd build/src\n# Start the master\n../../redis/src/redis-server --loadmodule libmaster.so --port 6369\n# Start the first chain members\n../../redis/src/redis-server --loadmodule libmember.so --port 6370\n../../redis/src/redis-server --loadmodule libmember.so --port 6371\n```\n\nNow we register the chain members with the master:\n\n```\nredis-cli -p 6369 MASTER.ADD 127.0.0.1 6370",
    "keywords": [
      "replicated",
      "building",
      "submodule",
      "install",
      "tcmalloc",
      "according",
      "https://github.com/gperftools/gperftools/blob/master/install",
      "use_tcmalloc=yes",
      "leveldb",
      "members",
      "build/src",
      "/../redis/src/redis-server",
      "--loadmodule",
      "libmaster.so",
      "libmember.so",
      "register",
      "master",
      "redis-cli",
      "master.add",
      "127.0.0.1"
    ]
  },
  {
    "owner": "ray-project",
    "name": "crossbow",
    "url": "https://github.com/ray-project/crossbow",
    "language": null,
    "stars": 0,
    "forks": 0,
    "updated_at": "2026-01-12T07:42:27Z",
    "description": "Arrow build queue",
    "summary": null,
    "keywords": []
  },
  {
    "owner": "ray-project",
    "name": "deltacat",
    "url": "https://github.com/ray-project/deltacat",
    "language": "Python",
    "stars": 267,
    "forks": 45,
    "updated_at": "2026-01-28T07:21:19Z",
    "description": "A portable Multimodal Lakehouse powered by Ray that brings exabyte-level scalability and fast, ACID-compliant, change-data-capture to your ML and analytics workloads.",
    "summary": "# DeltaCAT\n\nDeltaCAT is a Pythonic Data Catalog powered by Ray.\n\nIts data storage model allows you to define and manage fast, scalable,\nACID-compliant data catalogs through git-like stage/commit APIs, and has been\nused to successfully host exabyte-scale enterprise data lakes.\n\nDeltaCAT uses the Ray distributed compute framework together with Apache Arrow\nfor common table management tasks, including petabyte-scale\nchange-data-capture, data consistency checks, and table repair.\n\n## Getting Started\n\n### Install\n\n```\npip install deltacat\n```\n\n### Running Tests\n\n```\npip3 install virtualenv\nvirtualenv test_env\nsource test_env/bin/activate\npip3 install -r requirements.txt\n\npytest\n```\n",
    "keywords": [
      "deltacat",
      "pythonic",
      "catalog",
      "powered",
      "storage",
      "scalable",
      "acid-compliant",
      "catalogs",
      "through",
      "git-like",
      "stage/commit",
      "successfully",
      "exabyte-scale",
      "enterprise",
      "distributed",
      "compute",
      "framework",
      "together",
      "management",
      "including"
    ]
  },
  {
    "owner": "ray-project",
    "name": "dind-buildkite-plugin",
    "url": "https://github.com/ray-project/dind-buildkite-plugin",
    "language": "Shell",
    "stars": 0,
    "forks": 0,
    "updated_at": "2025-07-24T10:27:21Z",
    "description": "A Buildkite Plugin to enable Docker-in-Docker",
    "summary": "## Example\n\n```yml\nsteps:\n  - command: docker ps\n    plugins:\n      - ray-project/dind#v1.0.2:\n          network-name: dind-network\n          certs-volume-name: ray-docker-certs-client\n          additional-volume-mount: ray-volume:/ray\n```\n",
    "keywords": [
      "example",
      "command",
      "plugins",
      "ray-project/dind#v1.0.2",
      "network-name",
      "dind-network",
      "certs-volume-name",
      "ray-docker-certs-client",
      "additional-volume-mount",
      "ray-volume:/ray"
    ]
  },
  {
    "owner": "ray-project",
    "name": "distml",
    "url": "https://github.com/ray-project/distml",
    "language": "Python",
    "stars": 35,
    "forks": 1,
    "updated_at": "2026-01-13T08:23:27Z",
    "description": "Distributed ML Optimizer",
    "summary": "# Introduction\n\n*DistML* is a [Ray](https://github.com/ray-project/ray) extension library to support large-scale distributed ML training \non heterogeneous multi-node multi-GPU clusters. This library is under active development and we are adding more advanced \ntraining strategies and auto-parallelization features. \n\nDistML currently supports:\n* Distributed training strategies\n    * Data parallelism\n        * AllReduce strategy\n        * Sharded parameter server strategy\n        * BytePS strategy\n    Pipeline parallleism\n        * Micro-batch pipeline parallelism\n    \n* DL Frameworks:\n    * PyTorch\n    * JAX\n\n# Installation\n\n### Install Dependencies\nDepending on your CUDA version, install cupy following https://docs.cupy.dev/en/stable/install.html.\n\n### Install from source for dev\n```python\n",
    "keywords": [
      "introduction",
      "*distml*",
      "ray](https://github.com/ray-project/ray",
      "extension",
      "library",
      "support",
      "large-scale",
      "distributed",
      "training",
      "heterogeneous",
      "multi-node",
      "multi-gpu",
      "clusters",
      "development",
      "advanced",
      "strategies",
      "auto-parallelization",
      "features",
      "currently",
      "supports"
    ]
  },
  {
    "owner": "ray-project",
    "name": "distributed-zkml",
    "url": "https://github.com/ray-project/distributed-zkml",
    "language": "Rust",
    "stars": 3,
    "forks": 0,
    "updated_at": "2026-01-25T18:37:17Z",
    "description": "Distributed Proofs with ZMKL and Ray",
    "summary": "# distributed-zkml\n\nExtension of [zkml](https://github.com/uiuc-kang-lab/zkml) for distributed proving using Ray, layer-wise partitioning, and Merkle trees.\n\n> **‚ö†Ô∏è Status Note:** This is an experimental research project. Also consider [zk-torch](https://github.com/uiuc-kang-lab/zk-torch).\n\n## Completed Milestones\n\n1. ~~**Make Merkle root public**: Add root to public values so next chunk can verify it~~ Done\n2. ~~**Complete proof generation**: Connect chunk execution to actual proof generation ([#8](https://github.com/ray-project/distributed-zkml/issues/8))~~ Done\n3. ~~**Ray-Rust integration**: Connect Python Ray workers to Rust proof generation ([#9](https://github.com/ray-project/distributed-zkml/issues/9))~~ Done\n4. ~~**GPU acceleration**: ICICLE GPU backend for MSM operations ([#10](ht",
    "keywords": [
      "distributed-zkml",
      "extension",
      "zkml](https://github.com/uiuc-kang-lab/zkml",
      "distributed",
      "proving",
      "layer-wise",
      "partitioning",
      "note:**",
      "experimental",
      "research",
      "project",
      "consider",
      "zk-torch](https://github.com/uiuc-kang-lab/zk-torch",
      "completed",
      "milestones",
      "~~**make",
      "public**",
      "~~**complete",
      "generation**",
      "connect"
    ]
  },
  {
    "owner": "ray-project",
    "name": "docu-mentor",
    "url": "https://github.com/ray-project/docu-mentor",
    "language": "Python",
    "stars": 10,
    "forks": 3,
    "updated_at": "2026-01-12T20:57:04Z",
    "description": null,
    "summary": "# Docu-Mentor\n\n<img width=\"212\" alt=\"docu_mentor\" src=\"https://github.com/maxpumperla/docu-mentor/assets/3462566/de9f387a-4c97-4ade-a811-3b6282950f2c\">\n\nAutomatically get suggestions to improve the writing in your PRs from this\nGitHub app powered by [Anyscale Endpoints](https://app.endpoints.anyscale.com/).\n\n## Installation\n\nSimply install the app on your project on GitHub: [Docu-Mentor App](https://github.com/apps/docu-mentor)\n\n## Usage\n\nThen in any PR in your project, create a new comment that says:\n\n```bash\n@docu-mentor run\n```\n\nand I will start my analysis. I only look at what you changed\nin this PR. If you only want me to look at specific files or folders,\nyou can specify them like this:\n\n```bash\n@docu-mentor run doc/ README.md\n```\n\nIn this example, I'll have a look at all files conta",
    "keywords": [
      "docu-mentor",
      "width=\"212",
      "alt=\"docu_mentor",
      "src=\"https://github.com/maxpumperla/docu-mentor/assets/3462566/de9f387a-4c97-4ade-a811-3b6282950f2c\">",
      "automatically",
      "suggestions",
      "improve",
      "writing",
      "powered",
      "anyscale",
      "endpoints](https://app.endpoints.anyscale.com/",
      "installation",
      "install",
      "project",
      "github",
      "app](https://github.com/apps/docu-mentor",
      "comment",
      "```bash",
      "@docu-mentor",
      "analysis"
    ]
  },
  {
    "owner": "ray-project",
    "name": "enablement-content",
    "url": "https://github.com/ray-project/enablement-content",
    "language": "HTML",
    "stars": 0,
    "forks": 0,
    "updated_at": "2026-02-04T20:29:51Z",
    "description": null,
    "summary": "# Ray Enablement Content: Jupyter Book Publishing\n\nThis project provides a robust workflow for publishing Jupyter notebooks as a clean, embeddable Jupyter Book website. It automatically splits large notebooks into smaller sections, generates a navigation index, and applies a minimalist, content-focused style suitable for embedding or sharing.\n\n## Features\n\n- **Automatic notebook splitting**: Each notebook is split into parts at every second-level markdown header (`##`).\n- **Navigation index**: An `index.md` is generated with links to all notebook parts, serving as the landing page.\n- **Minimalist UI**: All navigation, sidebars, footers, and theme switchers are hidden by default. Light mode is always enforced.\n- **No code execution**: Notebooks are never executed during build, and all outpu",
    "keywords": [
      "enablement",
      "content",
      "jupyter",
      "publishing",
      "project",
      "provides",
      "workflow",
      "notebooks",
      "embeddable",
      "website",
      "automatically",
      "smaller",
      "sections",
      "generates",
      "navigation",
      "applies",
      "minimalist",
      "content-focused",
      "suitable",
      "embedding"
    ]
  },
  {
    "owner": "ray-project",
    "name": "enhancements",
    "url": "https://github.com/ray-project/enhancements",
    "language": null,
    "stars": 63,
    "forks": 30,
    "updated_at": "2026-01-13T18:15:33Z",
    "description": "Tracking Ray Enhancement Proposals",
    "summary": "# Ray Enhancement Proposals\nThis repo tracks Ray Enhancement Proposals (REP). The REP process is the main way to propose, discuss, and decide on features and other major changes to the Ray project. We'll start with a simple decision-making process (and evolve it over time):.\n- First, a draft PR is created against the repo with a draft REP. A senior Ray committer should be designated as the shepherd in the Stewardship section and assigned to the PR.\n- The shepherd will review the PR and get it into a polished state for further review by Ray committers.\n- Once the PR is reviewable, we will hold a vote on the ``ray-committers`` mailing list. In most cases this should reach consensus; if the result is not unanimous, Eric Liang (@ericl) and Philipp Moritz (@pcmoritz) will be the final deciders ",
    "keywords": [
      "enhancement",
      "proposals",
      "process",
      "propose",
      "discuss",
      "features",
      "changes",
      "project",
      "decision-making",
      "time",
      "created",
      "against",
      "committer",
      "designated",
      "shepherd",
      "stewardship",
      "section",
      "assigned",
      "polished",
      "further"
    ]
  },
  {
    "owner": "ray-project",
    "name": "flatbuffers",
    "url": "https://github.com/ray-project/flatbuffers",
    "language": "C++",
    "stars": 1,
    "forks": 0,
    "updated_at": "2018-04-12T00:30:36Z",
    "description": "Memory Efficient Serialization Library",
    "summary": null,
    "keywords": []
  },
  {
    "owner": "ray-project",
    "name": "images",
    "url": "https://github.com/ray-project/images",
    "language": null,
    "stars": 0,
    "forks": 1,
    "updated_at": "2025-07-24T10:27:19Z",
    "description": "Host images",
    "summary": null,
    "keywords": []
  },
  {
    "owner": "ray-project",
    "name": "issues-to-airtable",
    "url": "https://github.com/ray-project/issues-to-airtable",
    "language": "JavaScript",
    "stars": 1,
    "forks": 3,
    "updated_at": "2026-01-12T20:56:04Z",
    "description": null,
    "summary": "# Sync Github Issues to Airtable Database\n\nA bot that periodically (every 1hr) syncs Github Issues to Airtable databases.\n\n![image](https://user-images.githubusercontent.com/21118851/102130591-de48be80-3e05-11eb-8fd8-ff6816538916.png)\n",
    "keywords": [
      "airtable",
      "database",
      "periodically",
      "databases",
      "![image](https://user-images.githubusercontent.com/21118851/102130591-de48be80-3e05-11eb-8fd8-ff6816538916.png"
    ]
  },
  {
    "owner": "ray-project",
    "name": "kuberay",
    "url": "https://github.com/ray-project/kuberay",
    "language": "Go",
    "stars": 2315,
    "forks": 700,
    "updated_at": "2026-02-09T17:14:44Z",
    "description": "A toolkit to run Ray applications on Kubernetes",
    "summary": "<!-- markdownlint-disable MD013 -->\n# KubeRay\n\n[![Build Status](https://github.com/ray-project/kuberay/workflows/Go-build-and-test/badge.svg)](https://github.com/ray-project/kuberay/actions)\n[![Release](https://img.shields.io/github/v/release/ray-project/kuberay)](https://github.com/ray-project/kuberay/releases)\n[![Go Report Card](https://goreportcard.com/badge/github.com/ray-project/kuberay)](https://goreportcard.com/report/github.com/ray-project/kuberay)\n\nKubeRay is a powerful, open-source Kubernetes operator that simplifies the deployment and management of [Ray](https://github.com/ray-project/ray) applications on Kubernetes. It offers several key components:\n\n**KubeRay core**: This is the official, fully-maintained component of KubeRay that provides three custom resource definitions, Ra",
    "keywords": [
      "markdownlint-disable",
      "kuberay",
      "![build",
      "status](https://github.com/ray-project/kuberay/workflows/go-build-and-test/badge.svg)](https://github.com/ray-project/kuberay/actions",
      "![release](https://img.shields.io/github/v/release/ray-project/kuberay)](https://github.com/ray-project/kuberay/releases",
      "card](https://goreportcard.com/badge/github.com/ray-project/kuberay)](https://goreportcard.com/report/github.com/ray-project/kuberay",
      "powerful",
      "open-source",
      "kubernetes",
      "operator",
      "simplifies",
      "deployment",
      "management",
      "ray](https://github.com/ray-project/ray",
      "applications",
      "several",
      "components",
      "**kuberay",
      "core**",
      "official"
    ]
  },
  {
    "owner": "ray-project",
    "name": "kuberay-helm",
    "url": "https://github.com/ray-project/kuberay-helm",
    "language": "Mustache",
    "stars": 59,
    "forks": 31,
    "updated_at": "2026-01-23T22:29:07Z",
    "description": "Helm charts for the KubeRay project",
    "summary": "# KubeRay Helm Charts\n\nThis repository hosts Helm charts for the [KubeRay](https://github.com/ray-project/kuberay) project since KubeRay v0.4.0.\n\n# Usage\n[Helm](https://helm.sh/) must be installed to use the charts. Please refer to Helm's [documentation](https://helm.sh/docs/) to get started.\n\nOnce Helm is set up properly, add the repo as follows:\n\n```sh\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm install kuberay kuberay/kuberay-operator\n```\n\nYou can then run `helm search repo kuberay` to see the charts.",
    "keywords": [
      "kuberay",
      "repository",
      "kuberay](https://github.com/ray-project/kuberay",
      "project",
      "v0.4.0",
      "helm](https://helm.sh/",
      "installed",
      "charts",
      "documentation](https://helm.sh/docs/",
      "started",
      "properly",
      "follows",
      "https://ray-project.github.io/kuberay-helm/",
      "install",
      "kuberay/kuberay-operator",
      "kuberay`"
    ]
  },
  {
    "owner": "tensorflow",
    "name": ".allstar",
    "url": "https://github.com/tensorflow/.allstar",
    "language": null,
    "stars": 3,
    "forks": 2,
    "updated_at": "2025-02-21T15:55:37Z",
    "description": null,
    "summary": "Default Allstar configuration that points to [google/allstar-config](https://github.com/google/allstar-config).\n",
    "keywords": [
      "default",
      "allstar",
      "configuration",
      "google/allstar-config](https://github.com/google/allstar-config"
    ]
  },
  {
    "owner": "tensorflow",
    "name": ".github",
    "url": "https://github.com/tensorflow/.github",
    "language": null,
    "stars": 3,
    "forks": 0,
    "updated_at": "2025-02-27T09:34:34Z",
    "description": null,
    "summary": null,
    "keywords": []
  },
  {
    "owner": "tensorflow",
    "name": "adanet",
    "url": "https://github.com/tensorflow/adanet",
    "language": "Jupyter Notebook",
    "stars": 3458,
    "forks": 528,
    "updated_at": "2026-02-08T14:44:17Z",
    "description": "Fast and flexible AutoML with learning guarantees.",
    "summary": "# AdaNet\n\n<div align=\"center\">\n  <img src=\"https://tensorflow.github.io/adanet/images/adanet_tangram_logo.png\" alt=\"adanet_tangram_logo\"><br><br>\n</div>\n\n[![Documentation Status](https://readthedocs.org/projects/adanet/badge)](https://adanet.readthedocs.io)\n[![PyPI version](https://badge.fury.io/py/adanet.svg)](https://badge.fury.io/py/adanet)\n[![Travis](https://travis-ci.org/tensorflow/adanet.svg?branch=master)](https://travis-ci.org/tensorflow/adanet)\n[![codecov](https://codecov.io/gh/tensorflow/adanet/branch/master/graph/badge.svg)](https://codecov.io/gh/tensorflow/adanet)\n[![Gitter](https://badges.gitter.im/tensorflow/adanet.svg)](https://gitter.im/tensorflow/adanet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![Downloads](https://pepy.tech/badge/adanet)](https://pepy.tech",
    "keywords": [
      "align=\"center\">",
      "src=\"https://tensorflow.github.io/adanet/images/adanet_tangram_logo.png",
      "alt=\"adanet_tangram_logo\"><br><br>",
      "![documentation",
      "status](https://readthedocs.org/projects/adanet/badge)](https://adanet.readthedocs.io",
      "![pypi",
      "version](https://badge.fury.io/py/adanet.svg)](https://badge.fury.io/py/adanet",
      "![travis](https://travis-ci.org/tensorflow/adanet.svg?branch=master)](https://travis-ci.org/tensorflow/adanet",
      "![codecov](https://codecov.io/gh/tensorflow/adanet/branch/master/graph/badge.svg)](https://codecov.io/gh/tensorflow/adanet",
      "![gitter](https://badges.gitter.im/tensorflow/adanet.svg)](https://gitter.im/tensorflow/adanet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge",
      "![downloads](https://pepy.tech/badge/adanet)](https://pepy.tech/project/adanet",
      "![license](https://img.shields.io/badge/license-apache%202.0-blue.svg)](https://github.com/tensorflow/adanet/blob/master/license",
      "**adanet**",
      "lightweight",
      "tensorflow-based",
      "framework",
      "automatically",
      "learning",
      "high-quality",
      "minimal"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "addons",
    "url": "https://github.com/tensorflow/addons",
    "language": "Python",
    "stars": 1710,
    "forks": 619,
    "updated_at": "2026-01-31T14:13:17Z",
    "description": "Useful extra functionality for TensorFlow 2.x maintained by SIG-addons",
    "summary": "\n<h2 align=\"center\">\n :warning: :warning: :warning:\n</h2>\n\n<h4 align=\"center\">\nTensorFlow Addons (TFA) has ended development and introduction of new features.\n\nTFA has entered a minimal maintenance and release mode until a planned end of life \nin May 2024. Please modify downstream libraries to take dependencies from other \nrepositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP)\n\nFor more information see: [https://github.com/tensorflow/addons/issues/2807](https://github.com/tensorflow/addons/issues/2807)\n</h4>\n\n-----------------\n\n<div align=\"center\">\n  <img src=\"https://github.com/tensorflow/community/blob/master/sigs/logos/SIGAddons.png\" width=\"60%\"><br><br>\n</div>\n\n-----------------\n\n[![PyPI Status Badge](https://badge.fury.io/py/tensorflow-addons.svg)](https://pypi",
    "keywords": [
      "align=\"center\">",
      "warning",
      "tensorflow",
      "development",
      "introduction",
      "features",
      "entered",
      "minimal",
      "maintenance",
      "release",
      "planned",
      "downstream",
      "libraries",
      "dependencies",
      "repositories",
      "community",
      "keras-cv",
      "keras-nlp",
      "information",
      "https://github.com/tensorflow/addons/issues/2807](https://github.com/tensorflow/addons/issues/2807"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "agents",
    "url": "https://github.com/tensorflow/agents",
    "language": "Python",
    "stars": 2988,
    "forks": 744,
    "updated_at": "2026-02-06T02:01:55Z",
    "description": "TF-Agents: A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning.",
    "summary": "# TF-Agents: A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning.\n\n[![PyPI tf-agents](https://badge.fury.io/py/tf-agents.svg)](https://badge.fury.io/py/tf-agents)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/tf-agents)\n\n[TF-Agents](https://github.com/tensorflow/agents) makes implementing, deploying,\nand testing new Bandits and RL algorithms easier. It provides well tested and\nmodular components that can be modified and extended. It enables fast code\niteration, with good test integration and benchmarking.\n\nTo get started, we recommend checking out one of our Colab tutorials. If you\nneed an intro to RL (or a quick recap),\n[start here](docs/tutorials/0_intro_rl.ipynb). Otherwise, check out our\n[DQN tutorial](docs/tutoria",
    "keywords": [
      "tf-agents",
      "reliable",
      "scalable",
      "tensorflow",
      "library",
      "contextual",
      "bandits",
      "reinforcement",
      "learning",
      "![pypi",
      "tf-agents](https://badge.fury.io/py/tf-agents.svg)](https://badge.fury.io/py/tf-agents",
      "version](https://img.shields.io/pypi/pyversions/tf-agents",
      "tf-agents](https://github.com/tensorflow/agents",
      "implementing",
      "deploying",
      "testing",
      "algorithms",
      "easier",
      "provides",
      "modular"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "autograph",
    "url": "https://github.com/tensorflow/autograph",
    "language": "Python",
    "stars": 51,
    "forks": 17,
    "updated_at": "2024-11-12T21:04:21Z",
    "description": null,
    "summary": "# AutoGraph\n\nThis repository contains tests and example code for [TensorFlow AutoGraph](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph). For more information, see:\n\n * [tf.function and AutoGraph guide](https://www.tensorflow.org/beta/guide/autograph)\n * [AutoGraph reference documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/index.md)\n",
    "keywords": [
      "autograph",
      "repository",
      "contains",
      "example",
      "tensorflow",
      "autograph](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph",
      "information",
      "tf.function",
      "guide](https://www.tensorflow.org/beta/guide/autograph",
      "reference",
      "documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/index.md"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "benchmarks",
    "url": "https://github.com/tensorflow/benchmarks",
    "language": "Python",
    "stars": 1146,
    "forks": 630,
    "updated_at": "2026-01-28T15:08:50Z",
    "description": " A benchmark framework for Tensorflow",
    "summary": "# TensorFlow benchmarks\nThis repository contains various TensorFlow benchmarks. Currently, it consists of two projects:\n\n\n1. [PerfZero](https://github.com/tensorflow/benchmarks/tree/master/perfzero): A benchmark framework for TensorFlow.\n\n2. [scripts/tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) (no longer maintained): The TensorFlow CNN benchmarks contain TensorFlow 1 benchmarks for several convolutional neural networks.\n\nIf you want to run TensorFlow models and measure their performance, also consider the [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official)\n\n",
    "keywords": [
      "tensorflow",
      "benchmarks",
      "repository",
      "contains",
      "various",
      "currently",
      "consists",
      "projects",
      "perfzero](https://github.com/tensorflow/benchmarks/tree/master/perfzero",
      "benchmark",
      "framework",
      "scripts/tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks",
      "maintained",
      "contain",
      "several",
      "convolutional",
      "networks",
      "measure",
      "performance",
      "consider"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "build",
    "url": "https://github.com/tensorflow/build",
    "language": "Shell",
    "stars": 300,
    "forks": 138,
    "updated_at": "2026-01-20T00:55:13Z",
    "description": "Build-related tools for TensorFlow",
    "summary": "<div align=\"center\">\n  <img src=\"https://github.com/tensorflow/community/blob/master/sigs/logos/SIGBuild.png\" width=\"60%\"><br><br>\n</div>\n\n-----------------\n\n[![Gitter chat](https://img.shields.io/badge/chat-on%20gitter-46bc99.svg)](https://gitter.im/tensorflow/sig-build)\n[![SIG Build Forum](https://img.shields.io/badge/discuss-on%20tensorflow.org-orange)](https://groups.google.com/a/tensorflow.org/g/build)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/tensorflow/build/badge)](https://api.securityscorecards.dev/projects/github.com/tensorflow/build)\n\n**TensorFlow SIG Build** is a community group dedicated to the TensorFlow build\nprocess. This repository is a showcase of resources, guides, tools, and builds\ncontributed by the community, for the community.\n\n## G",
    "keywords": [
      "align=\"center\">",
      "src=\"https://github.com/tensorflow/community/blob/master/sigs/logos/sigbuild.png",
      "width=\"60%\"><br><br>",
      "-----------------",
      "![gitter",
      "chat](https://img.shields.io/badge/chat-on%20gitter-46bc99.svg)](https://gitter.im/tensorflow/sig-build",
      "forum](https://img.shields.io/badge/discuss-on%20tensorflow.org-orange)](https://groups.google.com/a/tensorflow.org/g/build",
      "![openssf",
      "scorecard](https://api.securityscorecards.dev/projects/github.com/tensorflow/build/badge)](https://api.securityscorecards.dev/projects/github.com/tensorflow/build",
      "**tensorflow",
      "build**",
      "community",
      "dedicated",
      "tensorflow",
      "process",
      "repository",
      "showcase",
      "resources",
      "guides",
      "contributed"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "cloud",
    "url": "https://github.com/tensorflow/cloud",
    "language": "Python",
    "stars": 382,
    "forks": 92,
    "updated_at": "2026-01-14T17:31:22Z",
    "description": "The TensorFlow Cloud repository provides APIs that will allow to easily go from debugging and training your Keras and TensorFlow code in a local environment to distributed training in the cloud.",
    "summary": "# TensorFlow Cloud\n\nThe TensorFlow Cloud repository provides APIs that will allow to easily go from\ndebugging, training, tuning your Keras and TensorFlow code in a local\nenvironment to distributed training/tuning on Cloud.\n\n## Introduction\n\n-   [TensorFlow Cloud `run` API](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/core/README.md)\n\n-   [TensorFlow Cloud Tuner](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/tuner/README.md)\n\n## TensorFlow Cloud `run` API for GCP training/tuning\n\n### Installation\n\n#### Requirements\n\n-   Python >= 3.6\n-   [A Google Cloud project](https://cloud.google.com/ai-platform/docs/getting-started-keras#set_up_your_project)\n-   An\n    [authenticated GCP account](https://cloud.google.com/ai-platform/docs/getti",
    "keywords": [
      "tensorflow",
      "repository",
      "provides",
      "debugging",
      "training",
      "environment",
      "distributed",
      "training/tuning",
      "introduction",
      "api](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/core/readme.md",
      "tuner](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/tuner/readme.md",
      "installation",
      "requirements",
      "project](https://cloud.google.com/ai-platform/docs/getting-started-keras#set_up_your_project",
      "authenticated",
      "account](https://cloud.google.com/ai-platform/docs/getting-started-keras#authenticate_your_gcp_account",
      "google",
      "platform](https://cloud.google.com/ai-platform/",
      "enabled",
      "account"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "codelabs",
    "url": "https://github.com/tensorflow/codelabs",
    "language": "Jupyter Notebook",
    "stars": 42,
    "forks": 24,
    "updated_at": "2025-12-11T21:11:02Z",
    "description": null,
    "summary": "# TensorFlow Codelabs\n\nThis repository contains sample code for several TensorFlow codelabs. Check out the Google machine learning pathways to learn more.\n\nhttps://developers.google.com/learn/pathways?category=aiandmachinelearning\n",
    "keywords": [
      "tensorflow",
      "codelabs",
      "repository",
      "contains",
      "several",
      "machine",
      "learning",
      "pathways",
      "https://developers.google.com/learn/pathways?category=aiandmachinelearning"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "community",
    "url": "https://github.com/tensorflow/community",
    "language": "C++",
    "stars": 1280,
    "forks": 582,
    "updated_at": "2026-02-06T22:24:58Z",
    "description": "Stores documents used by the TensorFlow developer community",
    "summary": "# Welcome to the TensorFlow Developer Community\n\n## This Repository\n\nThe `community` repository stores documents used by the developer community.\n\n* `rfcs` - design documents used by the design review process\n* `sigs` - documentation for each TensorFlow Special Interest group (SIG)\n* `governance` - operating processes for the TensorFlow project\n\n## Contact\n\nFor questions about this repository, please file an issue or reach out\nto Thea Lamkin: thealamkin@google.com.\n\n## Further Community Resources\n\nFor a complete overview of the TensorFlow community resources,\nplease visit [tensorflow.org/community](https://tensorflow.org/community). \n",
    "keywords": [
      "welcome",
      "tensorflow",
      "developer",
      "community",
      "repository",
      "`community`",
      "documents",
      "process",
      "documentation",
      "special",
      "interest",
      "`governance`",
      "operating",
      "processes",
      "project",
      "contact",
      "questions",
      "lamkin",
      "thealamkin@google.com",
      "further"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "compression",
    "url": "https://github.com/tensorflow/compression",
    "language": "Python",
    "stars": 909,
    "forks": 261,
    "updated_at": "2026-01-22T15:56:58Z",
    "description": "Data compression in TensorFlow",
    "summary": "# TensorFlow Compression\n\nTensorFlow Compression (TFC) contains data compression tools for TensorFlow.\n\nYou can use this library to build your own ML models with end-to-end optimized\ndata compression built in. It's useful to find storage-efficient representations\nof your data (images, features, examples, etc.) while only sacrificing a small\nfraction of model performance. Take a look at the [lossy data compression\ntutorial](https://www.tensorflow.org/tutorials/generative/data_compression) or\nthe [model compression\ntutorial](https://www.tensorflow.org/tutorials/optimization/compression) to get\nstarted.\n\nFor a more in-depth introduction from a classical data compression perspective,\nconsider our [paper on nonlinear transform\ncoding](https://arxiv.org/abs/2007.03034), or watch @jonarchists's [",
    "keywords": [
      "tensorflow",
      "compression",
      "contains",
      "library",
      "end-to-end",
      "optimized",
      "storage-efficient",
      "representations",
      "images",
      "features",
      "examples",
      "sacrificing",
      "fraction",
      "performance",
      "tutorial](https://www.tensorflow.org/tutorials/generative/data_compression",
      "tutorial](https://www.tensorflow.org/tutorials/optimization/compression",
      "started",
      "in-depth",
      "introduction",
      "classical"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "custom-op",
    "url": "https://github.com/tensorflow/custom-op",
    "language": "Smarty",
    "stars": 385,
    "forks": 119,
    "updated_at": "2025-11-25T20:36:55Z",
    "description": "Guide for building custom op for TensorFlow",
    "summary": "# TensorFlow Custom Op\nThis is a guide for users who want to write custom c++ op for TensorFlow and distribute the op as a pip package. This repository serves as both a working example of the op building and packaging process, as well as a template/starting point for writing your own ops. The way this repository is set up allow you to build your custom ops from TensorFlow's pip package instead of building TensorFlow from scratch. This guarantee that the shared library you build will be binary compatible with TensorFlow's pip packages.\n\nThis guide currently supports Ubuntu and Windows custom ops, and it includes examples for both cpu and gpu ops.\n\nStarting from Aug 1, 2019, nightly previews `tf-nightly` and `tf-nightly-gpu`, as well as\nofficial releases `tensorflow` and `tensorflow-gpu` pas",
    "keywords": [
      "tensorflow",
      "distribute",
      "package",
      "repository",
      "working",
      "example",
      "building",
      "packaging",
      "process",
      "template/starting",
      "writing",
      "tensorflow's",
      "instead",
      "scratch",
      "guarantee",
      "library",
      "compatible",
      "packages",
      "currently",
      "supports"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "data-validation",
    "url": "https://github.com/tensorflow/data-validation",
    "language": "Python",
    "stars": 779,
    "forks": 181,
    "updated_at": "2026-02-01T21:23:41Z",
    "description": "Library for exploring and validating machine learning data",
    "summary": "<!-- See: www.tensorflow.org/tfx/data_validation/ -->\n\n# TensorFlow Data Validation\n\n[![Python](https://img.shields.io/badge/python%7C3.9%7C3.10%7C3.11-blue)](https://github.com/tensorflow/data-validation)\n[![PyPI](https://badge.fury.io/py/tensorflow-data-validation.svg)](https://badge.fury.io/py/tensorflow-data-validation)\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv)\n\n*TensorFlow Data Validation* (TFDV) is a library for exploring and validating\nmachine learning data. It is designed to be highly scalable\nand to work well with TensorFlow and [TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx).\n\nTF Data Validation includes:\n\n*    Scalable calculation of summary statistics of training and t",
    "keywords": [
      "www.tensorflow.org/tfx/data_validation/",
      "tensorflow",
      "validation",
      "![python](https://img.shields.io/badge/python%7c3.9%7c3.10%7c3.11-blue)](https://github.com/tensorflow/data-validation",
      "![pypi](https://badge.fury.io/py/tensorflow-data-validation.svg)](https://badge.fury.io/py/tensorflow-data-validation",
      "![documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv",
      "*tensorflow",
      "validation*",
      "library",
      "exploring",
      "validating",
      "machine",
      "learning",
      "designed",
      "scalable",
      "extended",
      "tfx)](https://www.tensorflow.org/tfx",
      "includes",
      "calculation",
      "summary"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "datasets",
    "url": "https://github.com/tensorflow/datasets",
    "language": "Python",
    "stars": 4541,
    "forks": 1598,
    "updated_at": "2026-02-09T11:37:55Z",
    "description": "TFDS is a collection of datasets ready to use with TensorFlow, Jax, ...",
    "summary": "# TensorFlow Datasets\n\nTensorFlow Datasets provides many public datasets as `tf.data.Datasets`.\n\n[![Unittests](https://github.com/tensorflow/datasets/actions/workflows/pytest.yml/badge.svg)](https://github.com/tensorflow/datasets/actions/workflows/pytest.yml)\n[![PyPI version](https://badge.fury.io/py/tensorflow-datasets.svg)](https://badge.fury.io/py/tensorflow-datasets)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![Tutorial](https://img.shields.io/badge/doc-tutorial-blue.svg)](https://www.tensorflow.org/datasets/overview)\n[![API](https://img.shields.io/badge/doc-api-blue.svg)](https://www.tensorflow.org/datasets/api_docs/python/tfds)\n[![Catalog](https://img.shields.io/badge/doc-datasets-blue.svg)](https://www.tensorflow.org/dat",
    "keywords": [
      "tensorflow",
      "datasets",
      "provides",
      "`tf.data.datasets`",
      "![unittests](https://github.com/tensorflow/datasets/actions/workflows/pytest.yml/badge.svg)](https://github.com/tensorflow/datasets/actions/workflows/pytest.yml",
      "![pypi",
      "version](https://badge.fury.io/py/tensorflow-datasets.svg)](https://badge.fury.io/py/tensorflow-datasets",
      "![python",
      "3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/",
      "![tutorial](https://img.shields.io/badge/doc-tutorial-blue.svg)](https://www.tensorflow.org/datasets/overview",
      "![api](https://img.shields.io/badge/doc-api-blue.svg)](https://www.tensorflow.org/datasets/api_docs/python/tfds",
      "![catalog](https://img.shields.io/badge/doc-datasets-blue.svg)](https://www.tensorflow.org/datasets/catalog/overview#all_datasets",
      "documentation",
      "install",
      "strongly",
      "encourage",
      "**getting",
      "started",
      "guide**](https://www.tensorflow.org/datasets/overview",
      "interactively"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "decision-forests",
    "url": "https://github.com/tensorflow/decision-forests",
    "language": "Python",
    "stars": 694,
    "forks": 116,
    "updated_at": "2026-02-06T23:49:26Z",
    "description": "A collection of state-of-the-art algorithms for the training, serving and interpretation of Decision Forest models in Keras.",
    "summary": "<p align=\"center\">\n<img src=\"documentation/image/logo.png\"  />\n</p>\n\n> **Note:** We recommend users to migrate to **Yggdrasil Decision Forests**\n> (**YDF**). YDF trains the same models as TF-DF, but is faster and has more\n> functionality. See the\n> [migration guide](https://ydf.readthedocs.io/en/latest/tutorial/migrating_to_ydf/)\n> for more information.\n\n**TensorFlow Decision Forests** (**TF-DF**) is a library to train, run and\ninterpret [decision forest](https://ydf.readthedocs.io/en/latest/intro_df.html)\nmodels (e.g., Random Forests, Gradient Boosted Trees) in TensorFlow. TF-DF\nsupports classification, regression and ranking.\n\n**TF-DF** is powered by\n[Yggdrasil Decision Forest](https://github.com/google/yggdrasil-decision-forests)\n(**YDF**, a library to train and use decision forests in ",
    "keywords": [
      "align=\"center\">",
      "src=\"documentation/image/logo.png",
      "**note:**",
      "recommend",
      "migrate",
      "**yggdrasil",
      "decision",
      "forests**",
      "**ydf**",
      "functionality",
      "migration",
      "guide](https://ydf.readthedocs.io/en/latest/tutorial/migrating_to_ydf/",
      "information",
      "**tensorflow",
      "**tf-df**",
      "library",
      "interpret",
      "forest](https://ydf.readthedocs.io/en/latest/intro_df.html",
      "forests",
      "gradient"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "deepmath",
    "url": "https://github.com/tensorflow/deepmath",
    "language": "C++",
    "stars": 789,
    "forks": 137,
    "updated_at": "2026-01-05T06:42:33Z",
    "description": "Experiments towards neural network theorem proving",
    "summary": "# Deepmath\n\nThe Deepmath project seeks to improve automated theorem proving using deep\nlearning and other machine learning techniques.  Deepmath is a collaboration\nbetween [Google Research](https://research.google.com) and several universities.\n\n## DISCLAIMER:\n\nThe source code in this repository is not an official Google product, but\nis a research collaboration with external research teams.\n\n## Installation\n\nDeepmath depends on TensorFlow, which is included as a submodule.  Use, or\nsee, the Dockerfile for build instructions for `deephol`, our neural prover. It\nrequires connecting to a proof assistant server. See\nhttps://github.com/brain-research/hol-light for a server implementation.\n",
    "keywords": [
      "deepmath",
      "project",
      "improve",
      "automated",
      "theorem",
      "proving",
      "learning",
      "machine",
      "techniques",
      "collaboration",
      "between",
      "google",
      "research](https://research.google.com",
      "several",
      "universities",
      "disclaimer",
      "repository",
      "official",
      "product",
      "research"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "docs",
    "url": "https://github.com/tensorflow/docs",
    "language": "Jupyter Notebook",
    "stars": 6292,
    "forks": 5366,
    "updated_at": "2026-02-09T14:36:20Z",
    "description": "TensorFlow documentation",
    "summary": "# TensorFlow Documentation\n\n<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_horizontal.png\"><br><br>\n</div>\n\nThese are the source files for the guide and tutorials on\n[tensorflow.org](https://www.tensorflow.org/overview).\n\nTo contribute to the TensorFlow documentation, please read\n[CONTRIBUTING.md](CONTRIBUTING.md), the\n[TensorFlow docs contributor guide](https://www.tensorflow.org/community/contribute/docs),\nand the [style guide](https://www.tensorflow.org/community/contribute/docs_style).\n\nTo file a docs issue, use the issue tracker in the\n[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md) repo.\n\nAnd join the TensorFlow documentation contributors on the\n[TensorFlow Forum](https://discuss.tensorflow.or",
    "keywords": [
      "tensorflow",
      "documentation",
      "align=\"center\">",
      "src=\"https://www.tensorflow.org/images/tf_logo_horizontal.png\"><br><br>",
      "tutorials",
      "tensorflow.org](https://www.tensorflow.org/overview",
      "contribute",
      "contributing.md](contributing.md",
      "contributor",
      "guide](https://www.tensorflow.org/community/contribute/docs",
      "guide](https://www.tensorflow.org/community/contribute/docs_style",
      "tracker",
      "tensorflow/tensorflow](https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md",
      "contributors",
      "forum](https://discuss.tensorflow.org/",
      "community",
      "translations",
      "translations](https://www.tensorflow.org/community/contribute/docs#community_translations",
      "located",
      "tensorflow/docs-l10n](https://github.com/tensorflow/docs-l10n"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "docs-l10n",
    "url": "https://github.com/tensorflow/docs-l10n",
    "language": "Jupyter Notebook",
    "stars": 760,
    "forks": 629,
    "updated_at": "2026-01-20T06:57:34Z",
    "description": "Translations of TensorFlow documentation",
    "summary": "# TensorFlow Docs Translations\n\nThis project contains translations of the technical content and Jupyter\nnotebooks published on [tensorflow.org](https://www.tensorflow.org/guide).\n\nPlease file issues under the *documentation* component of the\n[TensorFlow issue tracker](https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md).\nQuestions about TensorFlow usage are better addressed on the\n[TensorFlow Forum](https://discuss.tensorflow.org/).\n\n## Contributing\n\nContributors are encouraged to use our GitLocalize project to submit pull\nrequests and reviews: https://gitlocalize.com/tensorflow/docs-l10n\n\nGeneral docs instructions are available in the\n[TensorFlow docs contributor guide](https://www.tensorflow.org/community/contribute/docs).\n\nPlease sign a\n[Contributor Li",
    "keywords": [
      "tensorflow",
      "translations",
      "project",
      "contains",
      "technical",
      "content",
      "jupyter",
      "notebooks",
      "published",
      "tensorflow.org](https://www.tensorflow.org/guide",
      "*documentation*",
      "component",
      "tracker](https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md",
      "questions",
      "addressed",
      "forum](https://discuss.tensorflow.org/",
      "contributing",
      "contributors",
      "encouraged",
      "gitlocalize"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "dtensor-gcp-examples",
    "url": "https://github.com/tensorflow/dtensor-gcp-examples",
    "language": "Python",
    "stars": 18,
    "forks": 4,
    "updated_at": "2025-12-10T17:32:08Z",
    "description": "Using DTensor on Google Cloud",
    "summary": "# DTensor GCP Examples\n\nThis project contains examples of using multi-client DTensor on GCP with a\ncluster of GPUs or TPUs.\n\n\n## Prerequisites\n\n1. gcloud environment on the local console:\n  ```\n  gcloud auth login ...\n  gcloud config set project  ...\n  ```\n\n2. A GCS bucket that the GCE service account can write into. The bucket is used\n  to demo checkpointing. Set the prefix paths name with\n  ```\n  export GCS_BUCKET=<bucket_name>\n  ```\n  or edit bootstrap.sh.\n\n\n# For maintainers\n\nSince this requires tf-nightly, periodically update requirements.txt in\neach directory to known 'good' versions for these examples.\n",
    "keywords": [
      "dtensor",
      "examples",
      "project",
      "contains",
      "multi-client",
      "cluster",
      "prerequisites",
      "environment",
      "console",
      "service",
      "account",
      "checkpointing",
      "gcs_bucket=<bucket_name>",
      "bootstrap.sh",
      "maintainers",
      "requires",
      "tf-nightly",
      "periodically",
      "requirements.txt",
      "directory"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "ecosystem",
    "url": "https://github.com/tensorflow/ecosystem",
    "language": "Scala",
    "stars": 1374,
    "forks": 392,
    "updated_at": "2026-01-10T16:33:22Z",
    "description": "Integration of TensorFlow with other open-source frameworks",
    "summary": "# TensorFlow Ecosystem\n\nThis repository contains examples for integrating TensorFlow with other\nopen-source frameworks. The examples are minimal and intended for use as\ntemplates. Users can tailor the templates for their own use-cases.\n\nIf you have any additions or improvements, please create an issue or pull\nrequest.\n\n## Contents\n\n- [docker](docker) - Docker configuration for running TensorFlow on\n  cluster managers.\n- [kubeflow](https://github.com/kubeflow/kubeflow) - A Kubernetes native platform for ML\n\t* A K8s custom resource for running distributed [TensorFlow jobs](https://github.com/kubeflow/kubeflow/blob/master/user_guide.md#submitting-a-tensorflow-training-job) \n\t* Jupyter images for different versions of TensorFlow\n\t* [TFServing](https://github.com/kubeflow/kubeflow/blob/master/u",
    "keywords": [
      "tensorflow",
      "ecosystem",
      "repository",
      "contains",
      "examples",
      "integrating",
      "open-source",
      "frameworks",
      "minimal",
      "intended",
      "templates",
      "use-cases",
      "additions",
      "improvements",
      "request",
      "contents",
      "docker](docker",
      "configuration",
      "running",
      "cluster"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "embedding-projector-standalone",
    "url": "https://github.com/tensorflow/embedding-projector-standalone",
    "language": "HTML",
    "stars": 306,
    "forks": 190,
    "updated_at": "2025-11-30T08:27:44Z",
    "description": null,
    "summary": null,
    "keywords": []
  },
  {
    "owner": "tensorflow",
    "name": "estimator",
    "url": "https://github.com/tensorflow/estimator",
    "language": "Python",
    "stars": 302,
    "forks": 195,
    "updated_at": "2025-11-06T03:06:54Z",
    "description": "TensorFlow Estimator",
    "summary": "-----------------\n| **`Documentation`** |\n|-----------------|\n| [![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/python/tf/estimator) |\n\nTensorFlow Estimator is a high-level TensorFlow API that greatly simplifies machine learning programming.\nEstimators encapsulate training, evaluation, prediction, and exporting for your model.\n\n## Getting Started\n\nSee our Estimator\n[getting started guide](https://www.tensorflow.org/guide/estimator) for an\nintroduction to the Estimator APIs.\n\n## Installation\n\n`tf.estimator` is installed when you install the TensorFlow pip package. See\n[Installing TensorFlow](https://www.tensorflow.org/install) for instructions.\n\n## Developing\n\nIf you want to build TensorFlow Estimator locally, you will need to\n[ins",
    "keywords": [
      "-----------------",
      "**`documentation`**",
      "|-----------------|",
      "![documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/python/tf/estimator",
      "tensorflow",
      "estimator",
      "high-level",
      "greatly",
      "simplifies",
      "machine",
      "learning",
      "programming",
      "estimators",
      "encapsulate",
      "training",
      "evaluation",
      "prediction",
      "exporting",
      "getting",
      "started"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "examples",
    "url": "https://github.com/tensorflow/examples",
    "language": "Jupyter Notebook",
    "stars": 8250,
    "forks": 7369,
    "updated_at": "2026-02-08T13:46:15Z",
    "description": "TensorFlow examples",
    "summary": "# TensorFlow Examples\n\n<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_social.png\" /><br /><br />\n</div>\n\n<h2>Most important links!</h2>\n\n* [Community examples](./community)\n* [Course materials](./courses/udacity_deep_learning) for the [Deep Learning](https://www.udacity.com/course/deep-learning--ud730) class on Udacity\n\nIf you are looking to learn TensorFlow, don't miss the\n[core TensorFlow documentation](http://github.com/tensorflow/docs)\nwhich is largely runnable code.\nThose notebooks can be opened in Colab from\n[tensorflow.org](https://tensorflow.org).\n\n<h2>What is this repo?</h2>\n\nThis is the TensorFlow example repo.  It has several classes of material:\n\n* Showcase examples and documentation for our fantastic [TensorFlow Community](https://tensorflow.org/com",
    "keywords": [
      "tensorflow",
      "examples",
      "align=\"center\">",
      "src=\"https://www.tensorflow.org/images/tf_logo_social.png",
      "<h2>most",
      "important",
      "links!</h2>",
      "community",
      "examples](./community",
      "course",
      "materials](./courses/udacity_deep_learning",
      "learning](https://www.udacity.com/course/deep-learning--ud730",
      "udacity",
      "looking",
      "documentation](http://github.com/tensorflow/docs",
      "largely",
      "runnable",
      "notebooks",
      "tensorflow.org](https://tensorflow.org",
      "<h2>what"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "fairness-indicators",
    "url": "https://github.com/tensorflow/fairness-indicators",
    "language": "Python",
    "stars": 358,
    "forks": 87,
    "updated_at": "2026-02-05T01:52:48Z",
    "description": "Tensorflow's Fairness Evaluation and Visualization Toolkit",
    "summary": "# Fairness Indicators\n\n![Fairness_Indicators](https://raw.githubusercontent.com/tensorflow/fairness-indicators/master/fairness_indicators/images/fairnessIndicators.png)\n\nFairness Indicators is designed to support teams in evaluating, improving, and comparing models for fairness concerns in partnership with the broader Tensorflow toolkit.\n\nThe tool is currently actively used internally by many of our products. We would love to partner with you to understand where Fairness Indicators is most useful, and where added functionality would be valuable. Please reach out at tfx@tensorflow.org. You can provide feedback and feature requests [here](https://github.com/tensorflow/fairness-indicators/issues/new/choose).\n\n## Key links\n* [Introductory Video](https://www.youtube.com/watch?v=pHT-ImFXPQo)\n* [",
    "keywords": [
      "fairness",
      "indicators",
      "![fairness_indicators](https://raw.githubusercontent.com/tensorflow/fairness-indicators/master/fairness_indicators/images/fairnessindicators.png",
      "designed",
      "support",
      "evaluating",
      "improving",
      "comparing",
      "concerns",
      "partnership",
      "broader",
      "tensorflow",
      "toolkit",
      "currently",
      "actively",
      "internally",
      "products",
      "partner",
      "understand",
      "useful"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "flutter-tflite",
    "url": "https://github.com/tensorflow/flutter-tflite",
    "language": "Dart",
    "stars": 640,
    "forks": 203,
    "updated_at": "2026-02-06T06:02:54Z",
    "description": null,
    "summary": " <p align=\"center\">\n    <br>\n    <img src=\"https://github.com/am15h/tflite_flutter_plugin/raw/update_readme/docs/tflite_flutter_cover.png\"/>\n    </br>\n</p>\n<p align=\"center\">\n\n   <a href=\"https://flutter.dev\">\n     <img src=\"https://img.shields.io/badge/Platform-Flutter-02569B?logo=flutter\"\n       alt=\"Platform\" />\n   </a>\n   <a href=\"https://pub.dartlang.org/packages/tflite_flutter\">\n     <img src=\"https://img.shields.io/pub/v/tflite_flutter.svg\"\n       alt=\"Pub Package\" />\n   </a>\n    <a href=\"https://pub.dev/documentation/tflite_flutter/latest/tflite_flutter/tflite_flutter-library.html\">\n        <img alt=\"Docs\" src=\"https://readthedocs.org/projects/hubdb/badge/?version=latest\">\n    </a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://img.shields.io/badge/Licen",
    "keywords": [
      "align=\"center\">",
      "src=\"https://github.com/am15h/tflite_flutter_plugin/raw/update_readme/docs/tflite_flutter_cover.png\"/>",
      "href=\"https://flutter.dev\">",
      "src=\"https://img.shields.io/badge/platform-flutter-02569b?logo=flutter",
      "alt=\"platform",
      "href=\"https://pub.dartlang.org/packages/tflite_flutter\">",
      "src=\"https://img.shields.io/pub/v/tflite_flutter.svg",
      "alt=\"pub",
      "package",
      "href=\"https://pub.dev/documentation/tflite_flutter/latest/tflite_flutter/tflite_flutter-library.html\">",
      "alt=\"docs",
      "src=\"https://readthedocs.org/projects/hubdb/badge/?version=latest\">",
      "href=\"https://opensource.org/licenses/apache-2.0\"><img",
      "src=\"https://img.shields.io/badge/license-apache%202.0-blue.svg\"></a>",
      "announcement",
      "update",
      "tensorflow",
      "managed",
      "tflite_flutter_plugin](https://github.com/am15h/tflite_flutter_plugin",
      "project"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "fold",
    "url": "https://github.com/tensorflow/fold",
    "language": "Python",
    "stars": 1823,
    "forks": 264,
    "updated_at": "2025-12-03T01:54:07Z",
    "description": "Deep learning with dynamic computation graphs in TensorFlow",
    "summary": "# TensorFlow Fold\n\nTensorFlow Fold is a library for\ncreating [TensorFlow](https://www.tensorflow.org) models that consume structured\ndata, where the structure of the computation graph depends on the structure of\nthe input data. For example, [this model](tensorflow_fold/g3doc/sentiment.ipynb)\nimplements [TreeLSTMs](https://arxiv.org/abs/1503.00075) for sentiment analysis\non parse trees of arbitrary shape/size/depth.\n\nFold implements [*dynamic batching*](https://arxiv.org/abs/1702.02181).\nBatches of arbitrarily shaped computation graphs are transformed to produce a\nstatic computation graph. This graph has the same structure regardless of what\ninput it receives, and can be executed efficiently by TensorFlow.\n\n* [Download and Setup](tensorflow_fold/g3doc/setup.md)\n* [Quick Start Notebook](tens",
    "keywords": [
      "tensorflow",
      "library",
      "creating",
      "tensorflow](https://www.tensorflow.org",
      "consume",
      "structured",
      "structure",
      "computation",
      "depends",
      "example",
      "model](tensorflow_fold/g3doc/sentiment.ipynb",
      "implements",
      "treelstms](https://arxiv.org/abs/1503.00075",
      "sentiment",
      "analysis",
      "arbitrary",
      "shape/size/depth",
      "*dynamic",
      "batching*](https://arxiv.org/abs/1702.02181",
      "batches"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "gan",
    "url": "https://github.com/tensorflow/gan",
    "language": "Jupyter Notebook",
    "stars": 967,
    "forks": 245,
    "updated_at": "2026-01-21T07:39:11Z",
    "description": "Tooling for GANs in TensorFlow",
    "summary": "# TensorFlow-GAN (TF-GAN)\n\nTF-GAN is a lightweight library for training and evaluating\n[Generative Adversarial Networks (GANs)](https://arxiv.org/abs/1406.2661).\n\n\n*   Can be installed with `pip` using `pip install tensorflow-gan`, and used\n    with `import tensorflow_gan as tfgan`\n*   [Well-tested examples](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/examples/)\n*   [Interactive introduction to TF-GAN](https://github.com/tensorflow/gan/blob/master/tensorflow_gan/examples/colab_notebooks/tfgan_tutorial.ipynb) in\n\n## Structure of the TF-GAN Library\n\nTF-GAN is composed of several parts, which are designed to exist independently:\n\n*   [Core](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/python/train.py):\n    the main infrastructure needed to train a GAN. Set up ",
    "keywords": [
      "tensorflow-gan",
      "tf-gan",
      "lightweight",
      "library",
      "training",
      "evaluating",
      "generative",
      "adversarial",
      "networks",
      "gans)](https://arxiv.org/abs/1406.2661",
      "installed",
      "install",
      "tensorflow-gan`",
      "`import",
      "tensorflow_gan",
      "well-tested",
      "examples](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/examples/",
      "interactive",
      "introduction",
      "tf-gan](https://github.com/tensorflow/gan/blob/master/tensorflow_gan/examples/colab_notebooks/tfgan_tutorial.ipynb"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "gnn",
    "url": "https://github.com/tensorflow/gnn",
    "language": "Python",
    "stars": 1510,
    "forks": 198,
    "updated_at": "2026-02-06T10:20:40Z",
    "description": "TensorFlow GNN is a library to build Graph Neural Networks on the TensorFlow platform.",
    "summary": "# TensorFlow GNN\n\n## Summary\n\nTensorFlow GNN is a library to build\n[Graph Neural Networks](tensorflow_gnn/docs/guide/intro.md) on the TensorFlow platform.\nIt provides...\n\n  * a [`tfgnn.GraphTensor`](tensorflow_gnn/docs/guide/graph_tensor.md) type to represent\n    graphs with a [heterogeneous schema](tensorflow_gnn/docs/guide/schema.md), that is,\n    multiple types of nodes and edges;\n  * tools for [data preparation](tensorflow_gnn/docs/guide/data_prep.md),\n    notably a [graph sampler](tensorflow_gnn/docs/guide/beam_sampler.md)\n    to convert a huge database into a stream of reasonably-sized subgraphs for\n    training and inference;\n  * a collection of [ready-to-use models](tensorflow_gnn/models/README.md)\n    and Keras layers to do your own [GNN modeling](tensorflow_gnn/docs/guide/gnn_mod",
    "keywords": [
      "tensorflow",
      "summary",
      "library",
      "networks](tensorflow_gnn/docs/guide/intro.md",
      "platform",
      "provides",
      "`tfgnn.graphtensor`](tensorflow_gnn/docs/guide/graph_tensor.md",
      "represent",
      "heterogeneous",
      "schema](tensorflow_gnn/docs/guide/schema.md",
      "multiple",
      "preparation](tensorflow_gnn/docs/guide/data_prep.md",
      "notably",
      "sampler](tensorflow_gnn/docs/guide/beam_sampler.md",
      "convert",
      "database",
      "reasonably-sized",
      "subgraphs",
      "training",
      "inference"
    ]
  },
  {
    "owner": "tensorflow",
    "name": "graphics",
    "url": "https://github.com/tensorflow/graphics",
    "language": "Python",
    "stars": 2783,
    "forks": 369,
    "updated_at": "2026-02-07T01:22:18Z",
    "description": "TensorFlow Graphics: Differentiable Graphics Layers for TensorFlow",
    "summary": "# TensorFlow Graphics\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Build](https://github.com/tensorflow/graphics/workflows/Build/badge.svg?branch=master)](https://github.com/tensorflow/graphics/actions)\n[![Code coverage](https://img.shields.io/coveralls/github/tensorflow/graphics.svg)](https://coveralls.io/github/tensorflow/graphics)\n[![PyPI project status](https://img.shields.io/pypi/status/tensorflow-graphics.svg)](https://pypi.org/project/tensorflow-graphics/)\n[![Supported Python version](https://img.shields.io/pypi/pyversions/tensorflow-graphics.svg)](https://pypi.org/project/tensorflow-graphics/)\n[![PyPI release version](https://img.shields.io/pypi/v/tensorflow-graphics.svg)](https://pypi.org/project/tensorflo",
    "keywords": [
      "tensorflow",
      "graphics",
      "![license](https://img.shields.io/badge/license-apache%202.0-blue.svg)](https://opensource.org/licenses/apache-2.0",
      "![build](https://github.com/tensorflow/graphics/workflows/build/badge.svg?branch=master)](https://github.com/tensorflow/graphics/actions",
      "![code",
      "coverage](https://img.shields.io/coveralls/github/tensorflow/graphics.svg)](https://coveralls.io/github/tensorflow/graphics",
      "![pypi",
      "project",
      "status](https://img.shields.io/pypi/status/tensorflow-graphics.svg)](https://pypi.org/project/tensorflow-graphics/",
      "![supported",
      "version](https://img.shields.io/pypi/pyversions/tensorflow-graphics.svg)](https://pypi.org/project/tensorflow-graphics/",
      "release",
      "version](https://img.shields.io/pypi/v/tensorflow-graphics.svg)](https://pypi.org/project/tensorflow-graphics/",
      "![downloads](https://pepy.tech/badge/tensorflow-graphics)](https://pepy.tech/project/tensorflow-graphics",
      "differentiable",
      "inserted",
      "network",
      "architectures",
      "spatial",
      "transformers"
    ]
  },
  {
    "owner": "pytorch",
    "name": "accimage",
    "url": "https://github.com/pytorch/accimage",
    "language": "C",
    "stars": 318,
    "forks": 38,
    "updated_at": "2026-01-10T14:57:00Z",
    "description": "high performance image loading and augmenting routines mimicking PIL.Image interface",
    "summary": "# accimage\n\n[![Build status](https://github.com/pytorch/accimage/actions/workflows/accimage.yml/badge.svg)](https://github.com/pytorch/accimage/actions/workflows/accimage.yml)\n[![Anaconda badge](https://anaconda.org/conda-forge/accimage/badges/version.svg)](https://anaconda.org/conda-forge/accimage)\n\n\nAn accelerated Image loader and preprocessor leveraging [Intel\nIPP](https://software.intel.com/en-us/intel-ipp).\n\naccimage mimics the PIL API and can be used as a backend for\n[`torchvision`](https://github.com/pytorch/vision).\n\nOperations implemented:\n\n- `Image.resize((width, height))`\n- `Image.crop((left, upper, right, lower))`\n- `Image.transpose(PIL.Image.FLIP_LEFT_RIGHT)`\n\nEnable the torchvision accimage backend with:\n\n```python\ntorchvision.set_image_backend('accimage')\n```\n\n## Installatio",
    "keywords": [
      "accimage",
      "![build",
      "status](https://github.com/pytorch/accimage/actions/workflows/accimage.yml/badge.svg)](https://github.com/pytorch/accimage/actions/workflows/accimage.yml",
      "![anaconda",
      "badge](https://anaconda.org/conda-forge/accimage/badges/version.svg)](https://anaconda.org/conda-forge/accimage",
      "accelerated",
      "preprocessor",
      "leveraging",
      "ipp](https://software.intel.com/en-us/intel-ipp",
      "backend",
      "`torchvision`](https://github.com/pytorch/vision",
      "operations",
      "implemented",
      "`image.resize((width",
      "height))`",
      "`image.crop((left",
      "lower))`",
      "`image.transpose(pil.image.flip_left_right)`",
      "torchvision",
      "```python"
    ]
  },
  {
    "owner": "pytorch",
    "name": "add-annotations-github-action",
    "url": "https://github.com/pytorch/add-annotations-github-action",
    "language": "JavaScript",
    "stars": 14,
    "forks": 10,
    "updated_at": "2025-02-10T01:53:02Z",
    "description": "A GitHub action to run clang-tidy and annotate failures",
    "summary": "# add-annotations-github-action\n\nA [GitHub Action][] to add [checks][] to pull requests by running lines of\noutput from a linter (such as [Flake8][] or [clang-tidy][]) through a regex.\n\n## Example\n\n```yml\non: pull_request\njobs:\n  fake8:\n    runs-on: ubuntu-latest\n    steps:\n      - run: |\n          touch fake8.txt\n          echo 'README.md:1:3: R100 make a better title' >> fake8.txt\n          echo 'README.md:2:1: R200 give a better description' >> fake8.txt\n      - uses: pytorch/add-annotations-github-action@master\n        with:\n          check_name: fake8\n          linter_output_path: fake8.txt\n          commit_sha: ${{ github.event.pull_request.head.sha }}\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n```\n\n## Usage\n\nSee [action.yml][] for most of the usage info. Also n",
    "keywords": [
      "add-annotations-github-action",
      "github",
      "action",
      "checks",
      "requests",
      "running",
      "flake8",
      "clang-tidy",
      "through",
      "example",
      "pull_request",
      "runs-on",
      "ubuntu-latest",
      "fake8.txt",
      "readme.md:1:3",
      "readme.md:2:1",
      "description",
      "pytorch/add-annotations-github-action@master",
      "check_name",
      "linter_output_path"
    ]
  },
  {
    "owner": "pytorch",
    "name": "alerting-infra",
    "url": "https://github.com/pytorch/alerting-infra",
    "language": "TypeScript",
    "stars": 2,
    "forks": 3,
    "updated_at": "2026-01-27T22:59:04Z",
    "description": "Alerting system for the pytorch org",
    "summary": "# PyTorch Test Infrastructure Alerting System\n\nA production-ready alert normalization pipeline that processes CloudWatch and Grafana alerts, normalizes them into a canonical format, and automatically manages GitHub issues for incident response.\n\n**Key Features:**\n- üîÑ **Alert Normalization**: Converts CloudWatch and Grafana alerts to canonical schema\n- üéØ **Intelligent Routing**: Team-based alert assignment with priority handling\n- üîç **Alert Grouping**: Groups recurring alerts by fingerprint with fresh GitHub issues per occurrence\n- üìã **Issue Lifecycle**: Automated GitHub issue creation, updates, and closure\n- üõ°Ô∏è **Resilience**: Circuit breakers, rate limiting, and graceful degradation\n- ‚ö° **Serverless**: Fully serverless AWS architecture with auto-scaling\n\n\n## üìë Table of Contents\n\n- [üèóÔ∏è Arc",
    "keywords": [
      "pytorch",
      "infrastructure",
      "alerting",
      "production-ready",
      "normalization",
      "pipeline",
      "processes",
      "cloudwatch",
      "grafana",
      "alerts",
      "normalizes",
      "canonical",
      "format",
      "automatically",
      "manages",
      "incident",
      "response",
      "features:**",
      "**alert",
      "normalization**"
    ]
  },
  {
    "owner": "pytorch",
    "name": "android-demo-app",
    "url": "https://github.com/pytorch/android-demo-app",
    "language": "Java",
    "stars": 1554,
    "forks": 624,
    "updated_at": "2026-01-26T08:53:36Z",
    "description": "PyTorch android examples of usage in applications",
    "summary": "# PyTorch Android Examples\n\nA list of Android demo apps built on the powerful [PyTorch Mobile](https://pytorch.org/mobile) platform.\n\n> ### Note\n> PyTorch Mobile is no longer actively supported. Please check out [ExecuTorch](https://pytorch.org/executorch-overview), PyTorch‚Äôs all-new on-device inference library. You can learn more about ExecuTorch‚Äôs Android demo apps [here](https://github.com/pytorch/executorch/tree/main/examples/demo-apps/android).\n\n### HelloWorld\n\n[HelloWorld](https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp) is a simple image classification application that demonstrates how to use the PyTorch Android API with the latest PyTorch 1.8, MobileNet v3, and [MemoryFormat.CHANNELS_LAST](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)",
    "keywords": [
      "pytorch",
      "android",
      "examples",
      "powerful",
      "mobile](https://pytorch.org/mobile",
      "platform",
      "actively",
      "supported",
      "executorch](https://pytorch.org/executorch-overview",
      "pytorch‚Äôs",
      "all-new",
      "on-device",
      "inference",
      "library",
      "executorch‚Äôs",
      "here](https://github.com/pytorch/executorch/tree/main/examples/demo-apps/android",
      "helloworld",
      "helloworld](https://github.com/pytorch/android-demo-app/tree/master/helloworldapp",
      "classification",
      "application"
    ]
  },
  {
    "owner": "pytorch",
    "name": "ao",
    "url": "https://github.com/pytorch/ao",
    "language": "Python",
    "stars": 2669,
    "forks": 425,
    "updated_at": "2026-02-09T17:00:09Z",
    "description": "PyTorch native quantization and sparsity for training and inference",
    "summary": "<div align=\"center\">\n\n# TorchAO\n\n</div>\n\n### PyTorch-Native Training-to-Serving Model Optimization\n- Pre-train Llama-3.1-70B **1.5x faster** with float8 training\n- Recover **67% of quantized accuracy degradation** on Gemma3-4B with QAT\n- Quantize Llama-3-8B to int4 for **1.89x faster** inference with **58% less memory**\n\n<div align=\"center\">\n\n[![](https://img.shields.io/badge/CodeML_%40_ICML-2025-blue)](https://openreview.net/attachment?id=HpqH0JakHf&name=pdf)\n[![](https://dcbadge.vercel.app/api/server/gpumode?style=flat&label=TorchAO%20in%20GPU%20Mode)](https://discord.com/channels/1189498204333543425/1205223658021458100)\n[![](https://img.shields.io/github/contributors-anon/pytorch/ao?color=yellow&style=flat-square)](https://github.com/pytorch/ao/graphs/contributors)\n[![](https://img.shie",
    "keywords": [
      "align=\"center\">",
      "torchao",
      "pytorch-native",
      "training-to-serving",
      "optimization",
      "pre-train",
      "llama-3.1-70b",
      "faster**",
      "training",
      "recover",
      "quantized",
      "accuracy",
      "degradation**",
      "gemma3-4b",
      "quantize",
      "llama-3-8b",
      "**1.89x",
      "inference",
      "memory**",
      "![](https://img.shields.io/badge/codeml_%40_icml-2025-blue)](https://openreview.net/attachment?id=hpqh0jakhf&name=pdf"
    ]
  },
  {
    "owner": "pytorch",
    "name": "audio",
    "url": "https://github.com/pytorch/audio",
    "language": "Python",
    "stars": 2824,
    "forks": 761,
    "updated_at": "2026-02-09T06:47:15Z",
    "description": "Data manipulation and transformation for audio signal processing, powered by PyTorch",
    "summary": "torchaudio: an audio library for PyTorch\n========================================\n\n[![Documentation](https://img.shields.io/badge/dynamic/json.svg?label=docs&url=https%3A%2F%2Fpypi.org%2Fpypi%2Ftorchaudio%2Fjson&query=%24.info.version&colorB=brightgreen&prefix=v)](https://pytorch.org/audio/main/)\n[![Anaconda Badge](https://anaconda.org/pytorch/torchaudio/badges/downloads.svg)](https://anaconda.org/pytorch/torchaudio)\n[![Anaconda-Server Badge](https://anaconda.org/pytorch/torchaudio/badges/platforms.svg)](https://anaconda.org/pytorch/torchaudio)\n\n![TorchAudio Logo](docs/source/_static/img/logo.png)\n\n> [!NOTE]\n> **We have transitioned TorchAudio into a\n>  maintenance phase. This process removed some user-facing\n>  features. These features were deprecated from TorchAudio 2.8 and removed in 2.",
    "keywords": [
      "torchaudio",
      "library",
      "pytorch",
      "========================================",
      "![documentation](https://img.shields.io/badge/dynamic/json.svg?label=docs&url=https%3a%2f%2fpypi.org%2fpypi%2ftorchaudio%2fjson&query=%24.info.version&colorb=brightgreen&prefix=v)](https://pytorch.org/audio/main/",
      "![anaconda",
      "badge](https://anaconda.org/pytorch/torchaudio/badges/downloads.svg)](https://anaconda.org/pytorch/torchaudio",
      "![anaconda-server",
      "badge](https://anaconda.org/pytorch/torchaudio/badges/platforms.svg)](https://anaconda.org/pytorch/torchaudio",
      "![torchaudio",
      "logo](docs/source/_static/img/logo.png",
      "!note",
      "transitioned",
      "maintenance",
      "process",
      "removed",
      "user-facing",
      "features",
      "deprecated",
      "redundancies"
    ]
  },
  {
    "owner": "pytorch",
    "name": "benchmark",
    "url": "https://github.com/pytorch/benchmark",
    "language": "Python",
    "stars": 1012,
    "forks": 330,
    "updated_at": "2026-02-06T06:01:52Z",
    "description": "TorchBench is a collection of open source benchmarks used to evaluate PyTorch performance.",
    "summary": "# PyTorch Benchmarks\nThis is a collection of open source benchmarks used to evaluate PyTorch performance.\n\n`torchbenchmark/models` contains copies of popular or exemplary workloads which have been modified to:\n*(a)* expose a standardized API for benchmark drivers, *(b)* optionally, enable backends such as torchinductor/torchscript,\n *(c)* contain a miniature version of train/test data and a dependency install script.\n\n## Installation\nThe benchmark suite should be self contained in terms of dependencies,\nexcept for the torch products which are intended to be installed separately so\ndifferent torch versions can be benchmarked.\n\n### Using Pre-built Packages\nWe support Python 3.8+, and 3.11 is recommended. Conda is optional but suggested. To start with Python 3.11 in conda:\n```\n# Using your cu",
    "keywords": [
      "pytorch",
      "benchmarks",
      "collection",
      "evaluate",
      "performance",
      "`torchbenchmark/models`",
      "contains",
      "popular",
      "exemplary",
      "workloads",
      "modified",
      "standardized",
      "benchmark",
      "drivers",
      "optionally",
      "backends",
      "torchinductor/torchscript",
      "contain",
      "miniature",
      "version"
    ]
  },
  {
    "owner": "pytorch",
    "name": "builder",
    "url": "https://github.com/pytorch/builder",
    "language": "Python",
    "stars": 356,
    "forks": 227,
    "updated_at": "2025-12-03T01:54:55Z",
    "description": "Continuous builder and binary build scripts for pytorch",
    "summary": "# pytorch builder\n\n**WARNING**: Builder repository is migrated to pytorch/pytorch and pytorch/test-infra repositories. Please note: If you intend to add or modify PyTorch build or test scripts please do it directly in pytorch/pytorch repository. Consult following issue for details: https://github.com/pytorch/builder/issues/2054 \n\nScripts to build pytorch binaries and do end-to-end integration tests.\n\nFolders:\n\n- **conda** : files to build conda packages of pytorch, torchvision and other dependencies and repos\n- **manywheel** : scripts to build linux wheels\n- **wheel** : scripts to build OSX wheels\n- **windows** : scripts to build Windows wheels\n- **cron** : scripts to drive all of the above scripts across multiple configurations together\n- **analytics** : scripts to pull wheel download cou",
    "keywords": [
      "pytorch",
      "builder",
      "**warning**",
      "repository",
      "migrated",
      "pytorch/pytorch",
      "pytorch/test-infra",
      "repositories",
      "scripts",
      "directly",
      "consult",
      "following",
      "details",
      "https://github.com/pytorch/builder/issues/2054",
      "binaries",
      "end-to-end",
      "integration",
      "folders",
      "**conda**",
      "packages"
    ]
  },
  {
    "owner": "pytorch",
    "name": "caffe2.github.io",
    "url": "https://github.com/pytorch/caffe2.github.io",
    "language": "HTML",
    "stars": 3,
    "forks": 6,
    "updated_at": "2025-02-10T01:54:48Z",
    "description": "The caffe2 website.",
    "summary": "## User Documentation for Caffe2\n\nThis directory will contain the user and feature documentation for Caffe2. The documentation will be hosted on GitHub pages.\n\n### Contributing\n\nSee [CONTRIBUTING.md](./CONTRIBUTING.md) for details on how to add or modify content.\n\n### Run the Site Locally\n\nThe requirements for running a GitHub pages site locally is described in [GitHub help](https://help.github.com/articles/setting-up-your-github-pages-site-locally-with-jekyll/#requirements). The steps below summarize these steps.\n\n> If you have run the site before, you can start with step 1 and then move on to step 5.\n\n1. Ensure that you are in the same directory where this `README.md` and the `Gemfile` file exists (e.g., it could be in `caff2/docs` on `master`, in the root of a `gh-pages` branch, etc). T",
    "keywords": [
      "documentation",
      "directory",
      "contain",
      "feature",
      "caffe2",
      "contributing",
      "contributing.md](./contributing.md",
      "details",
      "content",
      "locally",
      "requirements",
      "running",
      "described",
      "github",
      "help](https://help.github.com/articles/setting-up-your-github-pages-site-locally-with-jekyll/#requirements",
      "summarize",
      "before",
      "`readme.md`",
      "`gemfile`",
      "`caff2/docs`"
    ]
  },
  {
    "owner": "pytorch",
    "name": "ci-hud",
    "url": "https://github.com/pytorch/ci-hud",
    "language": "JavaScript",
    "stars": 13,
    "forks": 24,
    "updated_at": "2025-02-10T01:53:58Z",
    "description": "HUD for CI activity on `pytorch/pytorch`, provides a top level view for jobs to easily discern regressions",
    "summary": "# [PyTorch CI HUD](https://hud.pytorch.org)\n\nVisit https://hud.pytorch.org and see https://github.com/pytorch/pytorch/wiki/Using-hud.pytorch.org for usage details.\n\n## Development\n\nThis project was bootstrapped with [Create React App](https://create-react-app.dev). To build locally, install dependencies and run in develop mode:\n\n```bash\ngit clone https://github.com/pytorch/pytorch-ci-hud.git ci-hud\ncd ci-hud\nyarn install\nnpm start  # start a development server on localhost:3000\n```\n\nThe code is routed from [`App.js`](src/App.js) to places like:\n\n- [`PrDisplay.js`](src/PrDisplay.js): handles the per commit/PR pages with test results\n- [`GitHubStatusDisplay.js`](src/GitHubStatusDisplay.js): shows status for a set of commits on master or a release branch. This also depends on this [lambda fun",
    "keywords": [
      "pytorch",
      "hud](https://hud.pytorch.org",
      "https://hud.pytorch.org",
      "https://github.com/pytorch/pytorch/wiki/using-hud.pytorch.org",
      "details",
      "development",
      "project",
      "bootstrapped",
      "create",
      "app](https://create-react-app.dev",
      "locally",
      "install",
      "dependencies",
      "develop",
      "```bash",
      "https://github.com/pytorch/pytorch-ci-hud.git",
      "localhost:3000",
      "`app.js`](src/app.js",
      "`prdisplay.js`](src/prdisplay.js",
      "handles"
    ]
  },
  {
    "owner": "pytorch",
    "name": "ci-infra",
    "url": "https://github.com/pytorch/ci-infra",
    "language": "HCL",
    "stars": 13,
    "forks": 6,
    "updated_at": "2026-02-09T14:08:02Z",
    "description": null,
    "summary": "# PyTorch ci-infra\n\nThe PyTorch ci-infra repo contains the terraform configuration for the PyTorch\nLambda based EC2 instance autoscaler.\n\n## PyTorch CI Users\n\nThe PyTorch CI infrastructure is for the use of the github.com/pytorch/pytorch\nproject. It powers GitHub Actions self-hosted runners from the PyTorch\nFoundation's Cloud Provider Accounts.\n",
    "keywords": [
      "pytorch",
      "ci-infra",
      "contains",
      "terraform",
      "configuration",
      "instance",
      "autoscaler",
      "infrastructure",
      "github.com/pytorch/pytorch",
      "project",
      "actions",
      "self-hosted",
      "runners",
      "foundation's",
      "provider",
      "accounts"
    ]
  },
  {
    "owner": "pytorch",
    "name": "contrib",
    "url": "https://github.com/pytorch/contrib",
    "language": "Python",
    "stars": 392,
    "forks": 45,
    "updated_at": "2025-06-23T23:24:22Z",
    "description": "Implementations of ideas from recent papers",
    "summary": null,
    "keywords": []
  },
  {
    "owner": "pytorch",
    "name": "cppdocs",
    "url": "https://github.com/pytorch/cppdocs",
    "language": "HTML",
    "stars": 245,
    "forks": 38,
    "updated_at": "2026-02-09T06:39:44Z",
    "description": "PyTorch C++ API Documentation",
    "summary": "# pytorch/cppdocs\n\nThis repository is automatically generated to contain the website source for the\nPyTorch C++ API documentation at https://pytorch.org/cppdocs.\n\n__Please do not send pull requests against this repository__ to edit tutorial or\ndocumentation sources as it is automatically generated. Instead, edit the\nsources in https://github.com/pytorch/pytorch/tree/master/docs/cpp/source to\nchange tutorials and notes, or the source code in the PyTorch documentation to\nupdate source-level docs. Otherwise, changes to this repository will be\noverwritten by the next automatic sync.\n",
    "keywords": [
      "pytorch/cppdocs",
      "repository",
      "automatically",
      "generated",
      "contain",
      "website",
      "pytorch",
      "documentation",
      "https://pytorch.org/cppdocs",
      "__please",
      "requests",
      "against",
      "repository__",
      "tutorial",
      "sources",
      "instead",
      "https://github.com/pytorch/pytorch/tree/master/docs/cpp/source",
      "tutorials",
      "source-level",
      "otherwise"
    ]
  },
  {
    "owner": "pytorch",
    "name": "cpuinfo",
    "url": "https://github.com/pytorch/cpuinfo",
    "language": "C",
    "stars": 1153,
    "forks": 387,
    "updated_at": "2026-02-06T23:36:34Z",
    "description": "CPU INFOrmation library (x86/x86-64/ARM/ARM64, Linux/Windows/Android/macOS/iOS)",
    "summary": "# CPU INFOrmation library\n\n[![BSD (2 clause) License](https://img.shields.io/badge/License-BSD%202--Clause%20%22Simplified%22%20License-blue.svg)](https://github.com/pytorch/cpuinfo/blob/master/LICENSE)\n[![Linux/Mac build status](https://img.shields.io/travis/pytorch/cpuinfo.svg)](https://travis-ci.org/pytorch/cpuinfo)\n[![Windows build status](https://ci.appveyor.com/api/projects/status/g5khy9nr0xm458t7/branch/master?svg=true)](https://ci.appveyor.com/project/MaratDukhan/cpuinfo/branch/master)\n\ncpuinfo is a library to detect essential for performance optimization information about host CPU.\n\n## Features\n\n- **Cross-platform** availability:\n  - Linux, Windows, macOS, Android, iOS and FreeBSD operating systems\n  - x86, x86-64, ARM, and ARM64 architectures\n- Modern **C/C++ interface**\n  - Thre",
    "keywords": [
      "information",
      "library",
      "clause",
      "license](https://img.shields.io/badge/license-bsd%202--clause%20%22simplified%22%20license-blue.svg)](https://github.com/pytorch/cpuinfo/blob/master/license",
      "![linux/mac",
      "status](https://img.shields.io/travis/pytorch/cpuinfo.svg)](https://travis-ci.org/pytorch/cpuinfo",
      "![windows",
      "status](https://ci.appveyor.com/api/projects/status/g5khy9nr0xm458t7/branch/master?svg=true)](https://ci.appveyor.com/project/maratdukhan/cpuinfo/branch/master",
      "cpuinfo",
      "essential",
      "performance",
      "optimization",
      "features",
      "**cross-platform**",
      "availability",
      "windows",
      "android",
      "freebsd",
      "operating",
      "systems"
    ]
  },
  {
    "owner": "pytorch",
    "name": "docs",
    "url": "https://github.com/pytorch/docs",
    "language": "HTML",
    "stars": 10,
    "forks": 9,
    "updated_at": "2026-02-09T01:28:54Z",
    "description": "This repository is automatically generated to contain the website source for the PyTorch documentation at https//pytorch.org/docs.",
    "summary": "This repository is automatically generated to contain the website source for the PyTorch documentation at https://pytorch.org/docs.\n\nPlease do not send pull requests against this repository to edit documentation sources as it is automatically generated. Instead, edit the source code in the PyTorch documentation to update source-level docs. Otherwise, changes to this repository will be overwritten by the next automatic sync.\n\n## License\nDocs is BSD licensed, as found in the [LICENSE](LICENSE.md) file.\n",
    "keywords": [
      "repository",
      "automatically",
      "generated",
      "contain",
      "website",
      "pytorch",
      "documentation",
      "https://pytorch.org/docs",
      "requests",
      "against",
      "sources",
      "instead",
      "source-level",
      "otherwise",
      "changes",
      "overwritten",
      "automatic",
      "license",
      "licensed",
      "license](license.md"
    ]
  },
  {
    "owner": "pytorch",
    "name": "dr-ci",
    "url": "https://github.com/pytorch/dr-ci",
    "language": "Haskell",
    "stars": 2,
    "forks": 7,
    "updated_at": "2023-01-28T08:38:23Z",
    "description": "Diagnose and remediate CI jobs",
    "summary": "[![CI](https://github.com/pytorch/dr-ci/workflows/CI/badge.svg)](https://github.com/pytorch/dr-ci/actions?query=branch%3Amaster)\n\n# A log analyzer for CircleCI\n\n\n## Intro\n\nAn organization would like to determine what the most common causes of intermittent build\nfailures/flaky tests are in the a repository so that effort can be prioritized\nto fix them.\n\n## Outputs\n\nThe Dr. CI project entails two distinct user-facing outputs:\n\n* [Automatically-posted GitHub PR comments](application-logic/pull-request-comments)\n* The Dr. CI website\n\nThe latter has several distinct utilities:\n* Annotation interface for determinisitic `master` failures\n* Flakiness review tool\n* Stats dashboards\n\n# Codebase\n\nSee [docs/CODEBASE-OVERVIEW.md](docs/CODEBASE-OVERVIEW.md).\n\n# Repository assumptions\n\nDr. CI assumes a l",
    "keywords": [
      "![ci](https://github.com/pytorch/dr-ci/workflows/ci/badge.svg)](https://github.com/pytorch/dr-ci/actions?query=branch%3amaster",
      "analyzer",
      "circleci",
      "organization",
      "determine",
      "intermittent",
      "failures/flaky",
      "repository",
      "prioritized",
      "outputs",
      "project",
      "entails",
      "distinct",
      "user-facing",
      "automatically-posted",
      "comments](application-logic/pull-request-comments",
      "website",
      "several",
      "utilities",
      "annotation"
    ]
  },
  {
    "owner": "pytorch",
    "name": "elastic",
    "url": "https://github.com/pytorch/elastic",
    "language": "Python",
    "stars": 728,
    "forks": 100,
    "updated_at": "2025-12-25T20:23:20Z",
    "description": "PyTorch elastic training",
    "summary": "# TorchElastic\n\n**IMPORTANT:** This repository is deprecated.\n1. TorchElastic has been upstreamed to PyTorch 1.9 under `torch.distributed.elastic`.\nPlease refer to the PyTorch documentation [here](https://pytorch.org/docs/stable/distributed.elastic.html).\n\n2. The TorchElastic Controller for Kubernetes is no longer being actively maintained in favor of [TorchX](https://pytorch.org/torchx).\n",
    "keywords": [
      "torchelastic",
      "**important:**",
      "repository",
      "deprecated",
      "upstreamed",
      "pytorch",
      "`torch.distributed.elastic`",
      "documentation",
      "here](https://pytorch.org/docs/stable/distributed.elastic.html",
      "controller",
      "kubernetes",
      "actively",
      "maintained",
      "torchx](https://pytorch.org/torchx"
    ]
  },
  {
    "owner": "pytorch",
    "name": "ELF",
    "url": "https://github.com/pytorch/ELF",
    "language": "C++",
    "stars": 3415,
    "forks": 572,
    "updated_at": "2026-01-29T13:14:32Z",
    "description": "ELF: a platform for game research with AlphaGoZero/AlphaZero reimplementation",
    "summary": null,
    "keywords": []
  },
  {
    "owner": "pytorch",
    "name": "examples",
    "url": "https://github.com/pytorch/examples",
    "language": "Python",
    "stars": 23733,
    "forks": 9793,
    "updated_at": "2026-02-09T09:37:52Z",
    "description": "A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.",
    "summary": "# PyTorch Examples\n\nhttps://pytorch.org/examples/\n\n`pytorch/examples` is a repository showcasing examples of using [PyTorch](https://github.com/pytorch/pytorch). The goal is to have curated, short, few/no dependencies _high quality_ examples that are substantially different from each other that can be emulated in your existing work.\n\n- For tutorials: https://github.com/pytorch/tutorials\n- For changes to pytorch.org: https://github.com/pytorch/pytorch.github.io\n- For a general model hub: https://pytorch.org/hub/ or https://huggingface.co/models\n- For recipes on how to run PyTorch in production: https://github.com/facebookresearch/recipes\n- For general Q&A and support: https://discuss.pytorch.org/\n\n## Available models\n\n- [Image classification (MNIST) using Convnets](./mnist/README.md)\n- [Wor",
    "keywords": [
      "pytorch",
      "examples",
      "https://pytorch.org/examples/",
      "`pytorch/examples`",
      "repository",
      "showcasing",
      "pytorch](https://github.com/pytorch/pytorch",
      "curated",
      "dependencies",
      "quality_",
      "substantially",
      "different",
      "emulated",
      "existing",
      "tutorials",
      "https://github.com/pytorch/tutorials",
      "changes",
      "pytorch.org",
      "https://github.com/pytorch/pytorch.github.io",
      "general"
    ]
  },
  {
    "owner": "pytorch",
    "name": "executorch",
    "url": "https://github.com/pytorch/executorch",
    "language": "Python",
    "stars": 4247,
    "forks": 830,
    "updated_at": "2026-02-09T16:41:30Z",
    "description": "On-device AI across mobile, embedded and edge for PyTorch",
    "summary": "<div align=\"center\">\n  <img src=\"docs/source/_static/img/et-logo.png\" alt=\"ExecuTorch logo mark\" width=\"200\">\n  <h1>ExecuTorch</h1>\n  <p><strong>On-device AI inference powered by PyTorch</strong></p>\n</div>\n\n<div align=\"center\">\n  <a href=\"https://pypi.org/project/executorch/\"><img src=\"https://img.shields.io/pypi/v/executorch?style=for-the-badge&color=blue\" alt=\"PyPI - Version\"></a>\n  <a href=\"https://github.com/pytorch/executorch/graphs/contributors\"><img src=\"https://img.shields.io/github/contributors/pytorch/executorch?style=for-the-badge&color=blue\" alt=\"GitHub - Contributors\"></a>\n  <a href=\"https://github.com/pytorch/executorch/stargazers\"><img src=\"https://img.shields.io/github/stars/pytorch/executorch?style=for-the-badge&color=blue\" alt=\"GitHub - Stars\"></a>\n  <a href=\"https://dis",
    "keywords": [
      "align=\"center\">",
      "src=\"docs/source/_static/img/et-logo.png",
      "alt=\"executorch",
      "width=\"200\">",
      "<h1>executorch</h1>",
      "<p><strong>on-device",
      "inference",
      "powered",
      "pytorch</strong></p>",
      "href=\"https://pypi.org/project/executorch/\"><img",
      "src=\"https://img.shields.io/pypi/v/executorch?style=for-the-badge&color=blue",
      "alt=\"pypi",
      "version\"></a>",
      "href=\"https://github.com/pytorch/executorch/graphs/contributors\"><img",
      "src=\"https://img.shields.io/github/contributors/pytorch/executorch?style=for-the-badge&color=blue",
      "alt=\"github",
      "contributors\"></a>",
      "href=\"https://github.com/pytorch/executorch/stargazers\"><img",
      "src=\"https://img.shields.io/github/stars/pytorch/executorch?style=for-the-badge&color=blue",
      "stars\"></a>"
    ]
  },
  {
    "owner": "pytorch",
    "name": "expecttest",
    "url": "https://github.com/pytorch/expecttest",
    "language": "Python",
    "stars": 86,
    "forks": 11,
    "updated_at": "2025-12-21T09:16:13Z",
    "description": null,
    "summary": "# expecttest [![PyPI version](https://badge.fury.io/py/expecttest.svg)](https://badge.fury.io/py/expecttest)\n\nThis library implements expect tests (also known as \"golden\" tests). Expect\ntests are a method of writing tests where instead of hard-coding the expected\noutput of a test, you run the test to get the output, and the test framework\nautomatically populates the expected output.  If the output of the test changes,\nyou can rerun the test with the environment variable `EXPECTTEST_ACCEPT=1` to\nautomatically update the expected output.\n\nSomewhat unusually, this library implements *inline* expect tests: that is to\nsay, the expected output isn't saved to an external file, it is saved directly\nin the Python file (and we modify your Python file when updating the expect\ntest.)\n\nThe general reci",
    "keywords": [
      "expecttest",
      "![pypi",
      "version](https://badge.fury.io/py/expecttest.svg)](https://badge.fury.io/py/expecttest",
      "library",
      "implements",
      "golden",
      "tests",
      "writing",
      "instead",
      "hard-coding",
      "expected",
      "output",
      "framework",
      "automatically",
      "populates",
      "changes",
      "environment",
      "variable",
      "`expecttest_accept=1`",
      "somewhat"
    ]
  },
  {
    "owner": "pytorch",
    "name": "extension-cpp",
    "url": "https://github.com/pytorch/extension-cpp",
    "language": "Python",
    "stars": 1184,
    "forks": 251,
    "updated_at": "2026-02-04T09:41:13Z",
    "description": "C++ extensions in PyTorch",
    "summary": "# C++/CUDA Extensions in PyTorch\n\nThis repository contains two example C++/CUDA extensions for PyTorch:\n\n1. **extension_cpp** - Uses the standard ATen/LibTorch API\n2. **extension_cpp_stable** - Uses the [LibTorch Stable ABI](https://pytorch.org/docs/main/notes/libtorch_stable_abi.html)\n\nBoth extensions demonstrate how to write an example `mymuladd` custom op that has both\ncustom CPU and CUDA kernels.\n\n## extension_cpp (Standard ATen API)\n\nUses the full ATen/LibTorch API. This is the traditional way of writing PyTorch extensions.\nSee [this tutorial](https://pytorch.org/tutorials/advanced/cpp_custom_ops.html) for more details.\n\n## extension_cpp_stable (Stable ABI)\n\nUses the LibTorch Stable ABI to ensure that the extension built can be run with any version\nof PyTorch >= 2.10.0, without needin",
    "keywords": [
      "c++/cuda",
      "extensions",
      "pytorch",
      "repository",
      "contains",
      "example",
      "**extension_cpp**",
      "standard",
      "aten/libtorch",
      "**extension_cpp_stable**",
      "libtorch",
      "abi](https://pytorch.org/docs/main/notes/libtorch_stable_abi.html",
      "demonstrate",
      "`mymuladd`",
      "kernels",
      "extension_cpp",
      "traditional",
      "writing",
      "tutorial](https://pytorch.org/tutorials/advanced/cpp_custom_ops.html",
      "details"
    ]
  },
  {
    "owner": "pytorch",
    "name": "extension-ffi",
    "url": "https://github.com/pytorch/extension-ffi",
    "language": "Python",
    "stars": 256,
    "forks": 70,
    "updated_at": "2025-12-03T01:55:46Z",
    "description": "Examples of C extensions for PyTorch",
    "summary": "# PyTorch C FFI examples\n\nIn this repository you can find examples showing how to extend PyTorch with\ncustom C code. To use the ffi you need to install the `cffi` package from pip.\n\nCurrently there are two examples:\n* `package` - a pip distributable package\n* `script` - compiles the code into a local module, that can be later imported\n    from other files\n\nYou may also want to look at [pytorch/audio](https://github.com/pytorch/audio/)\nfor an example of a type-generic C FFI extension.\n",
    "keywords": [
      "pytorch",
      "examples",
      "repository",
      "showing",
      "install",
      "package",
      "currently",
      "`package`",
      "distributable",
      "`script`",
      "compiles",
      "module",
      "imported",
      "pytorch/audio](https://github.com/pytorch/audio/",
      "example",
      "type-generic",
      "extension"
    ]
  },
  {
    "owner": "pytorch",
    "name": "extension-script",
    "url": "https://github.com/pytorch/extension-script",
    "language": "Python",
    "stars": 114,
    "forks": 22,
    "updated_at": "2025-03-31T09:24:15Z",
    "description": "Example repository for custom C++/CUDA operators for TorchScript",
    "summary": "# Custom TorchScript Operators Example\n\nThis repository contains examples for writing, compiling and using custom\nTorchScript operators. See\n[here](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html)\nfor the accompanying tutorial.\n\n## Contents\n\nThere a few monuments in this repository you can visit. They are described in\ncontext in the tutorial, which you are encouraged to read. These monuments are:\n\n- `example_app/warp_perspective/op.cpp`: The custom operator implementation,\n- `example_app/main.cpp`: An example application that loads and executes a serialized TorchScript model, which uses the custom operator, in C++,\n- `script.py`: Example of using the custom operator in a scripted model,\n- `trace.py`: Example of using the custom operator in a traced model,\n- `eager.py`: ",
    "keywords": [
      "torchscript",
      "operators",
      "example",
      "repository",
      "contains",
      "examples",
      "writing",
      "compiling",
      "here](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html",
      "accompanying",
      "tutorial",
      "contents",
      "monuments",
      "described",
      "context",
      "encouraged",
      "`example_app/warp_perspective/op.cpp`",
      "operator",
      "implementation",
      "`example_app/main.cpp`"
    ]
  },
  {
    "owner": "pytorch",
    "name": "FBGEMM",
    "url": "https://github.com/pytorch/FBGEMM",
    "language": "C++",
    "stars": 1529,
    "forks": 715,
    "updated_at": "2026-02-09T11:45:13Z",
    "description": "FB (Facebook) + GEMM (General Matrix-Matrix Multiplication) - https://code.fb.com/ml-applications/fbgemm/",
    "summary": "# The FBGEMM Project\n\nThe FBGEMM Project is a repository of highly-optimized kernels used across\ndeep learning applications.\n\nThe codebase is organized and published as three related packages: FBGEMM,\nFBGEMM-GPU, and FBGEMM-GenAI. Each package has its own set of features and\ndocumentation.\n\n### Project Overview\n\n* **FBGEMM**: A low-precision, high-performance matrix multiplication and\n  convolution library for server-side inference.  The documentation below\n  provides an overview of FBGEMM, including its features, documentation, and\n  community resources.\n\n* **FBGEMM_GPU**: A collection of PyTorch GPU operator libraries built on top of\n  FBGEMM for training and inference, with focus on recommendation systems\n  applications.  Please see [the documentation](fbgemm_gpu/README.md) for more\n  i",
    "keywords": [
      "project",
      "repository",
      "highly-optimized",
      "kernels",
      "learning",
      "applications",
      "codebase",
      "organized",
      "published",
      "related",
      "packages",
      "fbgemm",
      "fbgemm-gpu",
      "fbgemm-genai",
      "package",
      "features",
      "documentation",
      "overview",
      "**fbgemm**",
      "low-precision"
    ]
  },
  {
    "owner": "pytorch",
    "name": "functorch",
    "url": "https://github.com/pytorch/functorch",
    "language": "Jupyter Notebook",
    "stars": 1436,
    "forks": 108,
    "updated_at": "2026-01-29T07:52:54Z",
    "description": "functorch is JAX-like composable function transforms for PyTorch.",
    "summary": "# functorch\n\n[**Why functorch?**](#why-composable-function-transforms)\n| [**Install guide**](#install)\n| [**Transformations**](#what-are-the-transforms)\n| [**Documentation**](#documentation)\n| [**Future Plans**](#future-plans)\n\n**This library is currently under heavy development - if you have suggestions\non the API or use-cases you'd like to be covered, please open an github issue\nor reach out. We'd love to hear about how you're using the library.**\n\n`functorch` is [JAX-like](https://github.com/google/jax) composable function\ntransforms for PyTorch.\n\nIt aims to provide composable `vmap` and `grad` transforms that work with\nPyTorch modules and PyTorch autograd with good eager-mode performance.\n\nIn addition, there is experimental functionality to trace through these\ntransformations using FX ",
    "keywords": [
      "functorch",
      "functorch?**](#why-composable-function-transforms",
      "**install",
      "guide**](#install",
      "**transformations**](#what-are-the-transforms",
      "**documentation**](#documentation",
      "**future",
      "plans**](#future-plans",
      "library",
      "currently",
      "development",
      "suggestions",
      "use-cases",
      "covered",
      "library.**",
      "`functorch`",
      "jax-like](https://github.com/google/jax",
      "composable",
      "function",
      "transforms"
    ]
  },
  {
    "owner": "pytorch",
    "name": "gloo",
    "url": "https://github.com/pytorch/gloo",
    "language": "C++",
    "stars": 1396,
    "forks": 345,
    "updated_at": "2026-02-09T17:58:32Z",
    "description": "Collective communications library with various primitives for multi-machine training.",
    "summary": "<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"./media/gloo_100k_dark.svg\">\n    <img width=\"55%\" src=\"./media/gloo_100k_light.svg\" alt=\"Gloo\">\n  </picture>\n</p>\n\n<h3 align=\"center\">\nCollective communications library with various primitives for multi-machine training.\n</h3>\n\n<p align=\"center\">\n  | <a href=\"https://github.com/facebookincubator/gloo/tree/main/docs\"><b>Gloo Documentation</b></a>\n  | <a href=\"https://pytorch.org/docs/stable/distributed.html\"><b>PyTorch Distributed Documentation</b></a>\n  | <a href=\"https://docs.google.com/presentation/d/1BX4o0ggV0-1MLwlLYcFkZHgNThyZX1-IJM3qbm0cmaA/edit?usp=sharing\"><b>Introduction to Gloo Presentation</b></a>\n  |\n</p>\n<p align=\"center\">\n  <a href=\"https://opensource.fb.com/support-ukraine\"><img alt=\"Supp",
    "keywords": [
      "align=\"center\">",
      "<picture>",
      "<source",
      "media=\"(prefers-color-scheme",
      "srcset=\"./media/gloo_100k_dark.svg\">",
      "width=\"55%",
      "src=\"./media/gloo_100k_light.svg",
      "alt=\"gloo\">",
      "</picture>",
      "collective",
      "communications",
      "library",
      "various",
      "primitives",
      "multi-machine",
      "training",
      "href=\"https://github.com/facebookincubator/gloo/tree/main/docs\"><b>gloo",
      "documentation</b></a>",
      "href=\"https://pytorch.org/docs/stable/distributed.html\"><b>pytorch",
      "distributed"
    ]
  },
  {
    "owner": "pytorch",
    "name": "glow",
    "url": "https://github.com/pytorch/glow",
    "language": "C++",
    "stars": 3325,
    "forks": 702,
    "updated_at": "2026-02-06T07:16:29Z",
    "description": "Compiler for Neural Network hardware accelerators",
    "summary": "![Glow Logo](./docs/logo.svg)\n\n[![pytorch](https://circleci.com/gh/pytorch/glow.svg?style=shield)](https://circleci.com/gh/pytorch/glow)\n\n\nGlow is a machine learning compiler and execution engine for hardware\naccelerators.  It is designed to be used as a backend for high-level machine\nlearning frameworks.  The compiler is designed to allow state of the art\ncompiler optimizations and code generation of neural network graphs. This\nlibrary is in active development. The project plan is described in the Github\nissues section and in the\n[Roadmap](https://github.com/pytorch/glow/wiki/Glow-Roadmap) wiki page.\n\n## Partners\n\nContributions to Glow are welcomed and encouraged! Glow is developed in\ncollaboration with the following partners:\n\n\n<!---\nNote:\nList of partner logos sorted alphabetically colu",
    "keywords": [
      "logo](./docs/logo.svg",
      "![pytorch](https://circleci.com/gh/pytorch/glow.svg?style=shield)](https://circleci.com/gh/pytorch/glow",
      "machine",
      "learning",
      "compiler",
      "execution",
      "hardware",
      "accelerators",
      "designed",
      "backend",
      "high-level",
      "frameworks",
      "optimizations",
      "generation",
      "network",
      "graphs",
      "library",
      "development",
      "project",
      "described"
    ]
  },
  {
    "owner": "pytorch",
    "name": "helion",
    "url": "https://github.com/pytorch/helion",
    "language": "Python",
    "stars": 744,
    "forks": 103,
    "updated_at": "2026-02-09T08:35:16Z",
    "description": "A Python-embedded DSL that makes it easy to write fast, scalable ML kernels with minimal boilerplate.",
    "summary": "<div align=\"center\">\n  <img src=\"docs/_static/helion_nobackground.png\" alt=\"Helion Logo\" width=\"250\"/>\n</div>\n\n# Events\n\n- **June, 2026**: Helion Tutorial @ [PLDI 2026](https://pldi26.sigplan.org/) - Writing Performance-Portable Kernels Simplified with Helion\n\n# About\n\nüìö **[View Documentation](https://helionlang.com)** üìö |\nüé• **[Watch Talk](https://youtu.be/BW-Ht-5IxgM)** üé• |\nüöÄ **[Try In Colab](https://colab.research.google.com/github/pytorch/helion/blob/main/notebooks/softmax.ipynb)** üöÄ |\n**[Try In AMD DevCloud](https://amd-ai-academy.com/github/pytorch/helion/blob/main/notebooks/softmax.ipynb)**\n\n**Helion** is a Python-embedded domain-specific language (DSL) for\nauthoring machine learning kernels, designed to compile down to [Triton],\na performant backend for programming GPUs and other de",
    "keywords": [
      "align=\"center\">",
      "src=\"docs/_static/helion_nobackground.png",
      "alt=\"helion",
      "width=\"250\"/>",
      "**june",
      "2026**",
      "tutorial",
      "2026](https://pldi26.sigplan.org/",
      "writing",
      "performance-portable",
      "kernels",
      "simplified",
      "**[view",
      "documentation](https://helionlang.com)**",
      "**[watch",
      "talk](https://youtu.be/bw-ht-5ixgm)**",
      "colab](https://colab.research.google.com/github/pytorch/helion/blob/main/notebooks/softmax.ipynb)**",
      "devcloud](https://amd-ai-academy.com/github/pytorch/helion/blob/main/notebooks/softmax.ipynb)**",
      "**helion**",
      "python-embedded"
    ]
  },
  {
    "owner": "pytorch",
    "name": "hub",
    "url": "https://github.com/pytorch/hub",
    "language": "Python",
    "stars": 1432,
    "forks": 248,
    "updated_at": "2026-01-19T10:26:30Z",
    "description": "Submission to https://pytorch.org/hub/",
    "summary": "# PyTorch Hub\n\n[![CircleCI](https://circleci.com/gh/pytorch/hub.svg?style=svg)](https://circleci.com/gh/pytorch/hub)\n\n\n## Logistics\n\nWe accept submission to PyTorch hub through PR in `hub` repo. Once the PR is merged into master here, it will show up on the [PyTorch website](https://pytorch.org/hub) in 24 hrs.\n\n\n## Steps to submit to PyTorch hub\n\n1. Add a `hubconf.py` in your repo, following the instruction in [torch.hub doc](https://pytorch.org/docs/master/hub.html#publishing-models). Verify it's working correctly by running `torch.hub.load(...)` locally.\n2. Create a PR in `pytorch/hub` repo. For each new model you have, create a `<repo_owner>_<repo_name>_<title>.md` file using this [template](docs/template.md).\n\n### Notes\n- Currently we don't support hosting pretrained weights, users wit",
    "keywords": [
      "pytorch",
      "![circleci](https://circleci.com/gh/pytorch/hub.svg?style=svg)](https://circleci.com/gh/pytorch/hub",
      "logistics",
      "submission",
      "through",
      "website](https://pytorch.org/hub",
      "`hubconf.py`",
      "following",
      "instruction",
      "torch.hub",
      "doc](https://pytorch.org/docs/master/hub.html#publishing-models",
      "working",
      "correctly",
      "running",
      "`torch.hub.load(...)`",
      "locally",
      "`pytorch/hub`",
      "`<repo_owner>_<repo_name>_<title>.md`",
      "template](docs/template.md",
      "currently"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "alignment-handbook",
    "url": "https://github.com/EleutherAI/alignment-handbook",
    "language": null,
    "stars": 4,
    "forks": 1,
    "updated_at": "2024-11-25T02:18:52Z",
    "description": "Robust recipes for to align language models with human and AI preferences",
    "summary": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/huggingface/alignment-handbook/main/assets/handbook.png\">\n</p>\n\n<p align=\"center\">\n    ü§ó <a href=\"https://huggingface.co/collections/alignment-handbook/handbook-v01-models-and-datasets-654e424d22e6880da5ebc015\" target=\"_blank\">Models & Datasets</a> | üìÉ <a href=\"https://arxiv.org/abs/2310.16944\" target=\"_blank\">Technical Report</a>\n</p>\n\n# The Alignment Handbook\n\nRobust recipes to align language models with human and AI preferences.\n\n## What is this?\n\nJust one year ago, chatbots were out of fashion and most people hadn't heard about techniques like Reinforcement Learning from Human Feedback (RLHF) to align language models with human preferences. Then, OpenAI broke the internet with ChatGPT and Meta followed suit by releasing t",
    "keywords": [
      "align=\"center\">",
      "src=\"https://raw.githubusercontent.com/huggingface/alignment-handbook/main/assets/handbook.png\">",
      "href=\"https://huggingface.co/collections/alignment-handbook/handbook-v01-models-and-datasets-654e424d22e6880da5ebc015",
      "target=\"_blank\">models",
      "datasets</a>",
      "href=\"https://arxiv.org/abs/2310.16944",
      "target=\"_blank\">technical",
      "report</a>",
      "alignment",
      "handbook",
      "recipes",
      "language",
      "preferences",
      "chatbots",
      "fashion",
      "techniques",
      "reinforcement",
      "learning",
      "feedback",
      "internet"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "alignment-reader",
    "url": "https://github.com/EleutherAI/alignment-reader",
    "language": "JavaScript",
    "stars": 1,
    "forks": 0,
    "updated_at": "2022-05-30T19:15:25Z",
    "description": "Search and filter through alignment literature",
    "summary": "# Alignment Reader\n\nSearch for the AI alignment literature in one place.\n",
    "keywords": [
      "alignment",
      "literature"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "architecture-experiments",
    "url": "https://github.com/EleutherAI/architecture-experiments",
    "language": "Python",
    "stars": 5,
    "forks": 1,
    "updated_at": "2023-05-08T19:50:00Z",
    "description": "Repository to host architecture experiments and development using Paxml and Praxis",
    "summary": "# architecture-experiments\nRepository to host architecture experiments and development using Paxml and Praxis\n\n## Example Run\n\n```\nexport LOG_DIR=... # can be gcp bucket or local directory\n\npython -m paxml.main \\\n    --exp=experiments.pile.BaseTransformerPile \\\n    --job_log_dir=${LOG_DIR}\n```\n\n## ToDos\n\n- [ ] ST-Moe Implementation\n- [ ] muP Implementation\n",
    "keywords": [
      "architecture-experiments",
      "repository",
      "architecture",
      "experiments",
      "development",
      "example",
      "log_dir=",
      "directory",
      "paxml.main",
      "--exp=experiments.pile.basetransformerpile",
      "--job_log_dir=${log_dir",
      "implementation"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "architecture-objective",
    "url": "https://github.com/EleutherAI/architecture-objective",
    "language": "Python",
    "stars": 10,
    "forks": 2,
    "updated_at": "2023-06-20T14:43:36Z",
    "description": null,
    "summary": "# T5X\n\nT5X is a modular, composable, research-friendly framework for high-performance,\nconfigurable, self-service training, evaluation, and inference of sequence\nmodels (starting with language) at many scales.\n\nIt is essentially a new and improved implementation of the\n[T5 codebase](https://github.com/google-research/text-to-text-transfer-transformer)\n(based on [Mesh TensorFlow](https://github.com/tensorflow/mesh)) in [JAX](https://github.com/google/jax) and [Flax](https://github.com/google/flax).\n\nBelow is a quick start guide for training models with TPUs on Google Cloud. For\nadditional tutorials and background, see the [complete documentation](docs/index.md).\n\n## Quickstart (Recommended)\n\nT5X can be run with [XManager](https://github.com/deepmind/xmanager) on\n[Vertex AI](https://cloud.go",
    "keywords": [
      "modular",
      "composable",
      "research-friendly",
      "framework",
      "high-performance",
      "configurable",
      "self-service",
      "training",
      "evaluation",
      "inference",
      "sequence",
      "starting",
      "language",
      "scales",
      "essentially",
      "improved",
      "implementation",
      "codebase](https://github.com/google-research/text-to-text-transfer-transformer",
      "tensorflow](https://github.com/tensorflow/mesh",
      "jax](https://github.com/google/jax"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "aria",
    "url": "https://github.com/EleutherAI/aria",
    "language": "Python",
    "stars": 94,
    "forks": 14,
    "updated_at": "2026-02-02T07:38:27Z",
    "description": "Official repository for the paper: Scaling Self-Supervised Representation Learning for Symbolic Piano Performance (ISMIR 2025)",
    "summary": "# Aria\n\nThis repository contains training, inference, and evaluation code for the paper [*Scaling Self-Supervised Representation Learning for Symbolic Piano Performance (ISMIR 2025)*](https://arxiv.org/abs/2506.23869), as well as implementations of our real-time piano continuation demo. *Aria* is a pretrained autoregressive generative model for symbolic music, based on the LLaMA 3.2 (1B) architecture, which was trained on ~60k hours of MIDI transcriptions of expressive solo-piano recordings. Alongside the base model, we are releasing a checkpoint finetuned to improve generative quality, as well as a checkpoint finetuned to produce general-purpose piano MIDI embeddings using a SimCSE-style contrastive training objective.\n\nüìñ Read our [paper](https://arxiv.org/abs/2506.23869)  \nü§ó Access our m",
    "keywords": [
      "repository",
      "contains",
      "training",
      "inference",
      "evaluation",
      "*scaling",
      "self-supervised",
      "representation",
      "learning",
      "symbolic",
      "performance",
      "2025)*](https://arxiv.org/abs/2506.23869",
      "implementations",
      "real-time",
      "continuation",
      "pretrained",
      "autoregressive",
      "generative",
      "architecture",
      "trained"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "aria-amt",
    "url": "https://github.com/EleutherAI/aria-amt",
    "language": "Python",
    "stars": 64,
    "forks": 9,
    "updated_at": "2026-02-02T07:40:32Z",
    "description": "Efficient and robust implementation of seq-to-seq automatic piano transcription.",
    "summary": "# aria-amt\n\nEfficient and robust implementation of seq-to-seq automatic piano transcription.\n\n## Install \n\nRequires Python 3.11\n\n```\ngit clone https://github.com/EleutherAI/aria-amt.git\ncd aria-amt\npip install -e .\n```\n\nDownload the preliminary model weights:\n\nPiano (v1)\n\n```\nwget \"https://huggingface.co/datasets/loubb/aria-midi/resolve/main/piano-medium-double-1.0.safetensors?download=true\"\n```\n\n## Usage\n\nYou can download mp3s from youtube using [yt-dlp](https://github.com/yt-dlp/yt-dlp):\n\n```\nyt-dlp --audio-format mp3 --extract-audio --no-playlist --audio-quality 0 <youtube-link> -o <save-path>\n```\n\nYou can then transcribe using the cli: \n\n```\naria-amt transcribe \\\n    medium-double \\\n    <path-to-checkpoint> \\\n    -load_path <path-to-audio> \\\n    -save_dir <path-to-save-dir> \\\n    -bs 1",
    "keywords": [
      "aria-amt",
      "efficient",
      "implementation",
      "seq-to-seq",
      "automatic",
      "transcription",
      "install",
      "requires",
      "https://github.com/eleutherai/aria-amt.git",
      "download",
      "preliminary",
      "weights",
      "https://huggingface.co/datasets/loubb/aria-midi/resolve/main/piano-medium-double-1.0.safetensors?download=true",
      "youtube",
      "yt-dlp](https://github.com/yt-dlp/yt-dlp",
      "--audio-format",
      "--extract-audio",
      "--no-playlist",
      "--audio-quality",
      "<youtube-link>"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "aria-utils",
    "url": "https://github.com/EleutherAI/aria-utils",
    "language": "Python",
    "stars": 6,
    "forks": 3,
    "updated_at": "2025-12-26T08:15:30Z",
    "description": "MIDI tokenizers and pre-processing utils.",
    "summary": "<p align=\"center\">\n<pre>\n ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó\n‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó\n‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë\n‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë    ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë\n‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù    ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n</pre>\n</p>\n\nAn extremely lightweight and simple library for pre-processing and tokenizing MIDI files.\n\n\n",
    "keywords": [
      "align=\"center\">",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó",
      "‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó",
      "‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó",
      "‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë",
      "‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë",
      "‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë",
      "‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë",
      "‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù",
      "‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë",
      "‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù",
      "‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù",
      "extremely",
      "lightweight",
      "library",
      "pre-processing"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "aria.cpp",
    "url": "https://github.com/EleutherAI/aria.cpp",
    "language": "CMake",
    "stars": 1,
    "forks": 0,
    "updated_at": "2024-11-22T15:00:51Z",
    "description": "GGML implementation of https://github.com/EleutherAI/aria",
    "summary": null,
    "keywords": []
  },
  {
    "owner": "EleutherAI",
    "name": "attention-probes",
    "url": "https://github.com/EleutherAI/attention-probes",
    "language": "Python",
    "stars": 8,
    "forks": 1,
    "updated_at": "2026-01-27T21:27:32Z",
    "description": "Linear probes with attention weighting",
    "summary": "# Attention probes\n\nBased on https://github.com/shan23chen/MOSAIC (unlicensed). The original README can be found in `README_MOSAIC.md`.\n\n## Install\n\n```bash\n# install\nuv pip install git+https://github.com/EleutherAI/attention-probes.git\n\n# development\ngit clone https://github.com/EleutherAI/attention-probes && cd attention-probes\nuv sync && uv pip install -e .\n```\n\n## Experiments (from blog post)\n\n### Collect data\n\n```bash\n\n# caches to /output\nuv run scripts_mosaic/run_experiments.py --extract-only\n\n# for Neurons in a Haystack paper data:\n# step 1: download https://www.dropbox.com/scl/fo/14oxabm2eq47bkw2u0oxo/AKFTcikvAB8-GVdoBztQHxE?rlkey=u9qny1tsza6lqetzzua3jr8xn&e=1&dl=0\n# step 2: unzip into data/gurnee_data\n# step 3: uv run python scripts/gurnee_data.py\n# step 4 (caches to /output_hayst",
    "keywords": [
      "attention",
      "https://github.com/shan23chen/mosaic",
      "unlicensed",
      "original",
      "`readme_mosaic.md`",
      "install",
      "```bash",
      "git+https://github.com/eleutherai/attention-probes.git",
      "development",
      "https://github.com/eleutherai/attention-probes",
      "attention-probes",
      "experiments",
      "collect",
      "/output",
      "scripts_mosaic/run_experiments.py",
      "--extract-only",
      "neurons",
      "haystack",
      "download",
      "https://www.dropbox.com/scl/fo/14oxabm2eq47bkw2u0oxo/akftcikvab8-gvdobztqhxe?rlkey=u9qny1tsza6lqetzzua3jr8xn&e=1&dl=0"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "attribute",
    "url": "https://github.com/EleutherAI/attribute",
    "language": "Python",
    "stars": 15,
    "forks": 5,
    "updated_at": "2026-01-14T12:57:55Z",
    "description": null,
    "summary": "# attribute\n\nReproduction of Anthropic's [Attribution Graphs](https://transformer-circuits.pub/2025/attribution-graphs/methods.html).\n\n## Setup\n```\n# install uv if you don't have it\n# curl -LsSf https://astral.sh/uv/install.sh | sh\nuv venv --seed\nuv sync\n```\n\n## Cache activations\nThis step is not strictly necessary. Given the weights of a transcoder, we need to compute activations on a large dataset to be able to select maximum-activating examples.\n\n```\n# scripts/cache_activations\nMODEL=HuggingFaceTB/SmolLM2-135M\nTRANSCODER=\"nev/SmolLM2-CLT-135M-73k-k32\"\nDATASET=\"--dataset_repo EleutherAI/fineweb-edu-dedup-10b --dataset_split train --n_tokens 10_000_000\"\nNAME=\"smollm-v1\"\n\nuv run python cache.py $MODEL $TRANSCODER --num_gpus 1 $DATASET --hookpoints layers.0.mlp layers.1.mlp layers.2.mlp lay",
    "keywords": [
      "attribute",
      "reproduction",
      "anthropic's",
      "attribution",
      "graphs](https://transformer-circuits.pub/2025/attribution-graphs/methods.html",
      "install",
      "https://astral.sh/uv/install.sh",
      "activations",
      "strictly",
      "necessary",
      "weights",
      "transcoder",
      "compute",
      "dataset",
      "maximum-activating",
      "examples",
      "scripts/cache_activations",
      "model=huggingfacetb/smollm2-135m",
      "transcoder=\"nev/smollm2-clt-135m-73k-k32",
      "dataset=\"--dataset_repo"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "auto-circuit",
    "url": "https://github.com/EleutherAI/auto-circuit",
    "language": null,
    "stars": 2,
    "forks": 0,
    "updated_at": "2025-01-06T18:32:17Z",
    "description": "A library for efficient patching and automatic circuit discovery.",
    "summary": "[![PyPI - Version](https://img.shields.io/pypi/v/auto-circuit?logo=pypi&logoColor=white)](https://pypi.org/project/auto-circuit/)\n[![GitHub Release](https://img.shields.io/github/v/release/UFO-101/auto-circuit?logo=github&logoColor=white)](https://github.com/UFO-101/auto-circuit/releases)\n# AutoCircuit\n![](docs/assets/Edge_Patching_Rounded.png)\n\nA library for efficient patching and automatic circuit discovery.\n\n[![Static Badge](https://img.shields.io/badge/Read%20the%20Docs-414A82?style=for-the-badge&logo=Google%20Docs&logoColor=white&labelColor=CB5AD9)](https://UFO-101.github.io/auto-circuit)\n\n## Read the draft paper\n[Transformer Circuit Metrics are not Robust](Transformer%20Circuit%20Metrics%20are%20not%20Robust.pdf)\n\n## Getting Started\n\n```bash\npip install auto-circuit\n```\n\n### Easy and",
    "keywords": [
      "![pypi",
      "version](https://img.shields.io/pypi/v/auto-circuit?logo=pypi&logocolor=white)](https://pypi.org/project/auto-circuit/",
      "![github",
      "release](https://img.shields.io/github/v/release/ufo-101/auto-circuit?logo=github&logocolor=white)](https://github.com/ufo-101/auto-circuit/releases",
      "autocircuit",
      "![](docs/assets/edge_patching_rounded.png",
      "library",
      "efficient",
      "patching",
      "automatic",
      "circuit",
      "discovery",
      "![static",
      "badge](https://img.shields.io/badge/read%20the%20docs-414a82?style=for-the-badge&logo=google%20docs&logocolor=white&labelcolor=cb5ad9)](https://ufo-101.github.io/auto-circuit",
      "transformer",
      "metrics",
      "robust](transformer%20circuit%20metrics%20are%20not%20robust.pdf",
      "getting",
      "started",
      "```bash"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "bayesian-adam",
    "url": "https://github.com/EleutherAI/bayesian-adam",
    "language": "Python",
    "stars": 1,
    "forks": 0,
    "updated_at": "2024-05-24T05:26:51Z",
    "description": "Exactly what it says on the tin",
    "summary": "# bayesian-adam\nExactly what it says on the tin.\n\nThis repo provides an implementation of AdaSGHMC which combines [SGHMC](https://arxiv.org/abs/1402.4102) with [Adam](https://arxiv.org/abs/1412.6980). The algorithm samples correctly from the posterior distribution in the limit of alpha -> 0, beta2 -> 1, contains correction factors for uniform diagonal noise, and behaves exactly like adam when the magnitude of the loss -> infinity.\n\nTODO: \n- [ ] Add utilities for deriving sensible priors from transformers\n- [ ] Usage instructions\n- [ ] Use cases\n- [ ] Better test cases\n- [ ] Parallel tempering\n",
    "keywords": [
      "bayesian-adam",
      "exactly",
      "provides",
      "implementation",
      "adasghmc",
      "combines",
      "sghmc](https://arxiv.org/abs/1402.4102",
      "adam](https://arxiv.org/abs/1412.6980",
      "algorithm",
      "samples",
      "correctly",
      "posterior",
      "distribution",
      "contains",
      "correction",
      "factors",
      "uniform",
      "diagonal",
      "behaves",
      "magnitude"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "bergson",
    "url": "https://github.com/EleutherAI/bergson",
    "language": "Python",
    "stars": 39,
    "forks": 13,
    "updated_at": "2026-02-09T13:48:31Z",
    "description": "Mapping out the \"memory\" of neural nets with data attribution",
    "summary": "# Bergson\nThis library enables you to trace the memory of deep neural nets with gradient-based data attribution techniques. We currently focus on TrackStar, as described in [Scalable Influence and Fact Tracing for Large Language Model Pretraining](https://arxiv.org/abs/2410.17413v3) by Chang et al. (2024), and also include support for several alternative influence functions. We plan to add support for [Magic](https://arxiv.org/abs/2504.16430) soon.\n\nWe view attribution as a counterfactual question: **_If we \"unlearned\" this training sample, how would the model's behavior change?_** This formulation ties attribution to some notion of what it means to \"unlearn\" a training sample. Here we focus on a very simple notion of unlearning: taking a gradient _ascent_ step on the loss with respect to ",
    "keywords": [
      "bergson",
      "library",
      "enables",
      "gradient-based",
      "attribution",
      "techniques",
      "currently",
      "trackstar",
      "described",
      "scalable",
      "influence",
      "tracing",
      "language",
      "pretraining](https://arxiv.org/abs/2410.17413v3",
      "2024",
      "include",
      "support",
      "several",
      "alternative",
      "functions"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "best-download",
    "url": "https://github.com/EleutherAI/best-download",
    "language": "Python",
    "stars": 19,
    "forks": 6,
    "updated_at": "2024-04-22T16:14:50Z",
    "description": "URL downloader supporting checkpointing and continuous checksumming.",
    "summary": "# best-download\n![python badge](https://github.com/EleutherAI/best-download/actions/workflows/python-app.yml/badge.svg)  \nURL downloader supporting checkpointing and continuous checksumming.\n\nNOTE: When the local_file already exists we automatically overwrite unless there is a checkpoint file there. When the download successfully completes the checkpoint will be deleted and True returned. This avoids leaving rubbish in the file system or doing full checksum calculations for large files. You will need to manage existing files if your scripts are re-runnable, either maintain your own database/done files or do a manual checksum.\n\n## Recent Updates:\n1. Added multiple urls option for failover.\n2. Parameter changes to 'download_file'\n- *local_file* is now optional, and will be set to the url bas",
    "keywords": [
      "best-download",
      "![python",
      "badge](https://github.com/eleutherai/best-download/actions/workflows/python-app.yml/badge.svg",
      "downloader",
      "supporting",
      "checkpointing",
      "continuous",
      "checksumming",
      "local_file",
      "already",
      "automatically",
      "overwrite",
      "checkpoint",
      "download",
      "successfully",
      "completes",
      "deleted",
      "returned",
      "leaving",
      "rubbish"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "BIG-bench",
    "url": "https://github.com/EleutherAI/BIG-bench",
    "language": "Python",
    "stars": 23,
    "forks": 0,
    "updated_at": "2025-04-14T11:38:46Z",
    "description": "Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models",
    "summary": "# BIG-bench :chair:\n\nThe Beyond the Imitation Game Benchmark (BIG-bench) is a *collaborative*\nbenchmark intended to probe large language models and extrapolate their future\ncapabilities. \nThe *more than 200* tasks included in BIG-bench are summarized by keyword [here](bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table), and by task name [here](bigbench/benchmark_tasks/README.md). A paper introducing the benchmark, including evaluation results on large language models, is currently in preparation.\n\nNew task submissions are encouraged. Tasks will be [reviewed](docs/doc.md#submission-review-process) and merged into the BIG-bench repository on a rolling basis. New tasks are no longer eligible for inclusion in the initial BIG-bench release and paper. However, they will be included in f",
    "keywords": [
      "big-bench",
      "chair",
      "imitation",
      "benchmark",
      "*collaborative*",
      "intended",
      "language",
      "extrapolate",
      "capabilities",
      "included",
      "summarized",
      "keyword",
      "here](bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table",
      "here](bigbench/benchmark_tasks/readme.md",
      "introducing",
      "including",
      "evaluation",
      "results",
      "models",
      "currently"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "bucket-cleaner",
    "url": "https://github.com/EleutherAI/bucket-cleaner",
    "language": "Python",
    "stars": 3,
    "forks": 1,
    "updated_at": "2024-11-25T02:21:39Z",
    "description": "A small utility to clear out old model checkpoints in Google Cloud Buckets whilst keeping tensorboard event files",
    "summary": "# Bucket Cleaner\nA small utility to clear out old model checkpoints in Google Cloud Buckets whilst keeping tensorboard event files.\n\n## Usage\n```\npython bucket_cleaner.py [-h] [-y] [--move MOVE] dir [dir ...]\n\npositional arguments:\n  dir          path to cloud bucket directory\n\noptional arguments:\n  -h, --help   show this help message and exit\n  -y, --yes    whether to ignore user prompts\n  --move MOVE  whether to move the events files to a given folder (preserves\n               folder structure)\n```\n\ne.g `python bucket_cleaner.py --move gs://neo-models/archive gs://neo-models/gpt2 `\n\nwill move all the events files from `gs://neo-models/gpt2/` to `gs://neo-models/archive` and delete everything else\n",
    "keywords": [
      "cleaner",
      "utility",
      "checkpoints",
      "buckets",
      "keeping",
      "tensorboard",
      "bucket_cleaner.py",
      "--move",
      "positional",
      "arguments",
      "directory",
      "optional",
      "message",
      "whether",
      "prompts",
      "preserves",
      "structure",
      "`python",
      "gs://neo-models/archive",
      "gs://neo-models/gpt2"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "CAA",
    "url": "https://github.com/EleutherAI/CAA",
    "language": null,
    "stars": 0,
    "forks": 0,
    "updated_at": "2024-02-26T20:57:19Z",
    "description": "Steering Llama 2 with Contrastive Activation Addition",
    "summary": "# Steering Llama 2 with Contrastive Activation Addition\n\n## Setup\n\n```bash\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\nThen create a `.env` file with the following variables (see `.env.example`):\n\n```\nHF_TOKEN=huggingface_token_with_access_to_llama2\nOPEN_AI_KEY=openai_api_key_with_access_to_gpt4\n```\n\n## Datasets\n\nAll raw and processed data can be seen in `/datasets`. Original sources are listed below. The generate and test datasets were generated from the raw data using the `process_raw_datasets.py` script.\n\nWe use 50 of the contrast pairs for evaluation. The rest are used for generating steering vectors.\n\n### Coordination with other AIs (`coordinate-other-ais`)\n\nAnthropic human generated eval data.\n\n- https://huggingface.co/datasets/Anthropic/model-w",
    "keywords": [
      "steering",
      "contrastive",
      "activation",
      "addition",
      "```bash",
      "python3",
      "venv/bin/activate",
      "install",
      "requirements.txt",
      "following",
      "variables",
      "`.env.example`",
      "hf_token=huggingface_token_with_access_to_llama2",
      "open_ai_key=openai_api_key_with_access_to_gpt4",
      "datasets",
      "processed",
      "`/datasets`",
      "original",
      "sources",
      "generate"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "ccs",
    "url": "https://github.com/EleutherAI/ccs",
    "language": "Python",
    "stars": 6,
    "forks": 6,
    "updated_at": "2025-10-14T12:35:54Z",
    "description": null,
    "summary": "## Introduction\n\n**WIP: This codebase is under active development**\n\nBecause language models are trained to predict the next token in naturally occurring text, they often reproduce common\nhuman errors and misconceptions, even when they \"know better\" in some sense. More worryingly, when models are trained to\ngenerate text that's rated highly by humans, they may learn to output false statements that human evaluators can't\ndetect. We aim to circumvent this issue by directly [**eliciting latent knowledge\n**](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit) (ELK) inside the activations\nof a language model.\n\nSpecifically, we're building on the **Contrastive Representation Clustering** (CRC) method described in the\npaper [Discovering Latent Knowledge in Langua",
    "keywords": [
      "introduction",
      "codebase",
      "development**",
      "because",
      "language",
      "trained",
      "predict",
      "naturally",
      "occurring",
      "reproduce",
      "misconceptions",
      "better",
      "worryingly",
      "generate",
      "humans",
      "statements",
      "evaluators",
      "detect",
      "circumvent",
      "directly"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "cc_img_dl",
    "url": "https://github.com/EleutherAI/cc_img_dl",
    "language": "Python",
    "stars": 1,
    "forks": 0,
    "updated_at": "2021-09-16T14:22:38Z",
    "description": null,
    "summary": null,
    "keywords": []
  },
  {
    "owner": "EleutherAI",
    "name": "circuit-breakers-SFT",
    "url": "https://github.com/EleutherAI/circuit-breakers-SFT",
    "language": null,
    "stars": 0,
    "forks": 0,
    "updated_at": "2026-01-07T23:41:39Z",
    "description": "Improving Alignment and Robustness with Circuit Breakers",
    "summary": "# Circuit Breakers\n\n[[Paper](https://arxiv.org/abs/2406.04313)] | [[Website](http://circuit-breaker.ai/)] | [[Models](https://huggingface.co/collections/GraySwanAI/model-with-circuit-breakers-668ca12763d1bc005b8b2ac3)]\n\nWe present Circuit Breaking, a new approach inspired by [representation engineering](https://ai-transparency.org/), designed to prevent AI systems from generating harmful content by directly altering harmful model representations. The family of circuit-breaking (or short-circuiting as one might put it) methods provide an alternative to traditional methods like refusal and adversarial training, protecting both LLMs and multimodal models from strong, unseen adversarial attacks without compromising model capability. Our approach represents a significant step forward in the dev",
    "keywords": [
      "circuit",
      "breakers",
      "paper](https://arxiv.org/abs/2406.04313",
      "website](http://circuit-breaker.ai/",
      "models](https://huggingface.co/collections/grayswanai/model-with-circuit-breakers-668ca12763d1bc005b8b2ac3",
      "present",
      "breaking",
      "approach",
      "inspired",
      "representation",
      "engineering](https://ai-transparency.org/",
      "designed",
      "prevent",
      "systems",
      "generating",
      "harmful",
      "content",
      "directly",
      "altering",
      "representations"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "classifier-latent-diffusion",
    "url": "https://github.com/EleutherAI/classifier-latent-diffusion",
    "language": "Python",
    "stars": 1,
    "forks": 1,
    "updated_at": "2025-09-01T08:02:03Z",
    "description": null,
    "summary": "# classifier-latent-diffusion\n\nBasic idea here:\n- Take an off-the-shelf diffusion model, \n- \"unsample\" some images using reverse DDIM,  \n- train a classifier on those unsampled images, \n- look for adversarial examples in that space\n\nThe hunch is that the diffusion model will distort the space in such a way that it's easier to generate images that look like the actual adversarial target class than to add in weird high frequency noise that fools the classifier while leaving the image unchanged.\n",
    "keywords": [
      "classifier-latent-diffusion",
      "off-the-shelf",
      "diffusion",
      "unsample",
      "reverse",
      "classifier",
      "unsampled",
      "images",
      "adversarial",
      "examples",
      "distort",
      "generate",
      "frequency",
      "leaving",
      "unchanged"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "clearnets",
    "url": "https://github.com/EleutherAI/clearnets",
    "language": "Python",
    "stars": 5,
    "forks": 0,
    "updated_at": "2025-07-17T04:27:46Z",
    "description": null,
    "summary": "# Clearnets\n\nDevelopment of and interpretability using disentangled architectures.\n\n## Train\n\nContains code for training sparse feedforward transformers, regular transformers, SAEs, and Transcoders.\n\nTrain GptNeoX-style base transformers with either sparse or dense feedforwards:\n\n```\npython -m clearnets.train.train_transformer\npython -m clearnets.train.train_transformer --dense\npython -m clearnets.train.train_transformer --dense --dataset \"lennart-finke/SimpleStories\" --tokenizer \"EleutherAI/gpt-neo-125m\"\n```\n\n`sparse_gptneox` and `sparse_gptneox_config` are both modified from the GptNeoX implementation in HuggingFace transformers with a flag to enable training with sparse feedforwards.\n\nTrain a comparison SAE or transcoder on a dense feedforward transformer using:\n\n```\npython -m clearnets",
    "keywords": [
      "clearnets",
      "development",
      "interpretability",
      "disentangled",
      "architectures",
      "contains",
      "training",
      "feedforward",
      "transformers",
      "regular",
      "transcoders",
      "gptneox-style",
      "feedforwards",
      "clearnets.train.train_transformer",
      "--dense",
      "--dataset",
      "lennart-finke/simplestories",
      "--tokenizer",
      "eleutherai/gpt-neo-125m",
      "`sparse_gptneox`"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "clt-training",
    "url": "https://github.com/EleutherAI/clt-training",
    "language": "Python",
    "stars": 18,
    "forks": 5,
    "updated_at": "2025-12-09T12:52:03Z",
    "description": "Sparsify transformers with cross-layer transcoders",
    "summary": "## Introduction\nThis library trains _k_-sparse autoencoders (SAEs) and transcoders on the activations of HuggingFace language models, roughly following the recipe detailed in [Scaling and evaluating sparse autoencoders](https://arxiv.org/abs/2406.04093v1) (Gao et al. 2024).\n\nThis is a lean, simple library with few configuration options. Unlike most other SAE libraries (e.g. [SAELens](https://github.com/jbloomAus/SAELens)), it does not cache activations on disk, but rather computes them on-the-fly. This allows us to scale to very large models and datasets with zero storage overhead, but has the downside that trying different hyperparameters for the same model and dataset will be slower than if we cached activations (since activations will be re-computed). We may add caching as an option in ",
    "keywords": [
      "introduction",
      "library",
      "_k_-sparse",
      "autoencoders",
      "transcoders",
      "activations",
      "huggingface",
      "language",
      "models",
      "roughly",
      "following",
      "detailed",
      "scaling",
      "evaluating",
      "autoencoders](https://arxiv.org/abs/2406.04093v1",
      "configuration",
      "options",
      "libraries",
      "saelens](https://github.com/jbloomaus/saelens",
      "computes"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "CodeCARP",
    "url": "https://github.com/EleutherAI/CodeCARP",
    "language": null,
    "stars": 6,
    "forks": 2,
    "updated_at": "2023-05-20T04:15:47Z",
    "description": "Data collection pipeline for CodeCARP. Includes PyCharm plugins.",
    "summary": "# CodeCARP\nData collection pipeline for CodeCARP. Includes PyCharm plugins.\n",
    "keywords": [
      "codecarp",
      "collection",
      "pipeline",
      "includes",
      "pycharm",
      "plugins"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "common-llm-settings",
    "url": "https://github.com/EleutherAI/common-llm-settings",
    "language": "JavaScript",
    "stars": 1,
    "forks": 0,
    "updated_at": "2024-01-02T18:24:34Z",
    "description": "Common LLM Settings App",
    "summary": "# Common LLM Settings\n\nThis repo was developed to keep track of [the LLM setting information shared in this spreadsheet.](https://docs.google.com/spreadsheets/d/14vbBbuRMEHoqeuMHkTfw3uiZVmyXNuoSp8s-aHvfvZk/edit#gid=0) under version control. \n\nIt renders the yaml file in `public/config` into a table. To update or add a model: (make sure you have `npm` installed)\n\n1. Clone the repo.\n2. Update the `schema.yaml` file.\n3. Run `npm install` to load dependencies.\n4. Run `npm start` to view the results in your browser. \n\n\n## Working with React \n\nThis project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).\nIn the project directory, you can run:\n\n### `npm start`\n\nRuns the app in the development mode.\\\nOpen [http://localhost:3000](http://localhost:3000) to view",
    "keywords": [
      "settings",
      "developed",
      "setting",
      "information",
      "spreadsheet.](https://docs.google.com/spreadsheets/d/14vbbburmehoqeumhktfw3uizvmyxnuosp8s-ahvfvzk/edit#gid=0",
      "version",
      "control",
      "renders",
      "`public/config`",
      "installed",
      "`schema.yaml`",
      "install`",
      "dependencies",
      "results",
      "browser",
      "working",
      "project",
      "bootstrapped",
      "create",
      "app](https://github.com/facebook/create-react-app"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "CommonLoopUtils",
    "url": "https://github.com/EleutherAI/CommonLoopUtils",
    "language": "Jupyter Notebook",
    "stars": 0,
    "forks": 0,
    "updated_at": "2022-10-10T01:34:09Z",
    "description": "[WIP] a version of CLU with WandB logging added.",
    "summary": "# CLU - Common Loop Utils\n\nThis repository contains common functionality for writing ML training loops. The\ngoal is to make trainings loops short and readable (but moving common tasks to\nsmall libraries) without removing the flexibility required for research.\n\nTo get started, check out this Colab:\n\nhttps://colab.research.google.com/github/google/CommonLoopUtils/blob/master/clu_synopsis.ipynb\n\nIf you're looking for usage examples, see:\n\nhttps://github.com/google/flax/tree/master/examples\n\nYou can also find answers to common questions about CLU on Flax Github\ndiscussions page:\n\nhttps://github.com/google/flax/discussions\n\nNote: As this point we are not accepting contributions. Please fork the\nrepository if you want to extend the libraries for your use case.\n",
    "keywords": [
      "repository",
      "contains",
      "functionality",
      "writing",
      "training",
      "trainings",
      "readable",
      "libraries",
      "without",
      "removing",
      "flexibility",
      "required",
      "research",
      "started",
      "https://colab.research.google.com/github/google/commonlooputils/blob/master/clu_synopsis.ipynb",
      "looking",
      "examples",
      "https://github.com/google/flax/tree/master/examples",
      "answers",
      "questions"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "composer",
    "url": "https://github.com/EleutherAI/composer",
    "language": "Python",
    "stars": 3,
    "forks": 2,
    "updated_at": "2023-04-22T04:58:01Z",
    "description": "Train neural networks up to 7x faster",
    "summary": "<br />\n<p align=\"center\">\n    <a href=\"https://github.com/mosaicml/composer#gh-light-mode-only\" class=\"only-light\">\n      <img src=\"https://storage.googleapis.com/docs.mosaicml.com/images/header_light.svg\" width=\"50%\"/>\n    </a>\n    <!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_BEGIN -->\n    <a href=\"https://github.com/mosaicml/composer#gh-dark-mode-only\" class=\"only-dark\">\n      <img src=\"https://storage.googleapis.com/docs.mosaicml.com/images/header_dark.svg\" width=\"50%\"/>\n    </a>\n    <!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_END -->\n</p>\n\n<h2><p align=\"center\">A PyTorch Library for Efficient Neural Network Training</p></h2>\n<h3><p align=\"center\">Train Faster, Reduce Cost, Get Better Models</p></h3>\n\n<h4><p align='center'>\n<a href=\"https://www.mosaicml.com\">[Website]</a>\n- <a href=\"https://docs.mos",
    "keywords": [
      "align=\"center\">",
      "href=\"https://github.com/mosaicml/composer#gh-light-mode-only",
      "class=\"only-light\">",
      "src=\"https://storage.googleapis.com/docs.mosaicml.com/images/header_light.svg",
      "width=\"50%\"/>",
      "setuptools_long_description_hide_begin",
      "href=\"https://github.com/mosaicml/composer#gh-dark-mode-only",
      "class=\"only-dark\">",
      "src=\"https://storage.googleapis.com/docs.mosaicml.com/images/header_dark.svg",
      "setuptools_long_description_hide_end",
      "align=\"center\">a",
      "pytorch",
      "library",
      "efficient",
      "network",
      "training</p></h2>",
      "align=\"center\">train",
      "faster",
      "models</p></h3>",
      "align='center'>"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "concept-erasure",
    "url": "https://github.com/EleutherAI/concept-erasure",
    "language": "Python",
    "stars": 243,
    "forks": 15,
    "updated_at": "2026-01-31T19:41:44Z",
    "description": "Erasing concepts from neural representations with provable guarantees",
    "summary": "# Least-Squares Concept Erasure (LEACE)\nConcept erasure aims to remove specified features from a representation. It can be used to improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). This is the repo for **LEAst-squares Concept Erasure (LEACE)**, a closed-form method which provably prevents all linear classifiers from detecting a concept while inflicting the least possible damage to the representation. You can check out the paper [here](https://arxiv.org/abs/2306.03819).\n\n# Installation\n\nWe require Python 3.10 or later. You can install the package from PyPI:\n\n```bash\npip install concept-erasure\n```\n\n# Usage\n\nThe two main classes in this repo are `LeaceFitter` and `LeaceEraser`.\n\n- `Le",
    "keywords": [
      "least-squares",
      "concept",
      "erasure",
      "leace",
      "specified",
      "features",
      "representation",
      "improve",
      "fairness",
      "preventing",
      "classifier",
      "interpretability",
      "removing",
      "observe",
      "changes",
      "behavior",
      "**least-squares",
      "leace)**",
      "closed-form",
      "provably"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "conceptual-constraints",
    "url": "https://github.com/EleutherAI/conceptual-constraints",
    "language": "Jupyter Notebook",
    "stars": 1,
    "forks": 0,
    "updated_at": "2024-02-12T19:48:14Z",
    "description": "Applying LEACE to models during training",
    "summary": "# Online LEACE\nThis repo tests an online version of [LEACE](https://github.com/EleutherAI/concept-erasure/tree/main) to erase spurious heuristic concepts from [BERT](https://huggingface.co/docs/transformers/model_doc/bert) representations *during finetuning* on the [MNLI dataset](https://huggingface.co/datasets/multi_nli). All the models are evaluated on the [HANS dataset](https://github.com/tommccoy1/hans), which evaluates the robustness of a model with respect to these spurious heuristics.\n\n## Install requirements\nCreate a Python >= 3.10 environment and install the requirements:\n```bash\npip install -e .\n```\n\n## Usage\nOne can finetune a BERT model on the MNLI dataset and evaluate the resulting model on the HANS dataset with the command:\n```bash\npython cmd/train.py --concept-erasure ERASUR",
    "keywords": [
      "version",
      "leace](https://github.com/eleutherai/concept-erasure/tree/main",
      "spurious",
      "heuristic",
      "concepts",
      "bert](https://huggingface.co/docs/transformers/model_doc/bert",
      "representations",
      "*during",
      "finetuning*",
      "dataset](https://huggingface.co/datasets/multi_nli",
      "evaluated",
      "dataset](https://github.com/tommccoy1/hans",
      "evaluates",
      "robustness",
      "respect",
      "heuristics",
      "install",
      "requirements",
      "environment",
      "```bash"
    ]
  },
  {
    "owner": "EleutherAI",
    "name": "cookbook",
    "url": "https://github.com/EleutherAI/cookbook",
    "language": "Python",
    "stars": 829,
    "forks": 43,
    "updated_at": "2026-01-16T13:15:09Z",
    "description": "Deep learning for dummies. All the practical details and useful utilities that go into working with real models.",
    "summary": "# The Cookbook\nDeep learning for dummies, by Quentin Anthony, Jacob Hatef, Hailey Schoelkopf, and Stella Biderman\n\nAll the practical details and utilities that go into working with real models! If you're just getting started, we recommend jumping ahead to [Basics](#basics) for some introductory resources on transformers.\n\n## Table of Contents\n\n- [The Cookbook](#the-cookbook)\n  * [Utilities](#utilities)\n    + [Calculations](#calculations)\n    + [Benchmarks](#benchmarks)\n  * [Reading List](#reading-list)\n    + [Basics](#basics)\n    + [How to do LLM Calculations](#how-to-do-llm-calculations)\n    + [Distributed Deep Learning](#distributed-deep-learning)\n    + [Best Practices](#best-practices)\n    + [Data/Model Directories](#data-and-model-directories)\n  * [Minimal Repositories for Educational ",
    "keywords": [
      "cookbook",
      "learning",
      "dummies",
      "quentin",
      "anthony",
      "schoelkopf",
      "biderman",
      "practical",
      "details",
      "utilities",
      "working",
      "models!",
      "getting",
      "started",
      "recommend",
      "jumping",
      "basics](#basics",
      "introductory",
      "resources",
      "transformers"
    ]
  }
]